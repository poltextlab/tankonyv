# NLP és névelemfelismerés {#nlp_ch}

## Fogalmi alapok

A természetes-nyelv feldolgozása (*Natural Language Processing -- NLP*) a nyelvészet és a mesterséges intelligencia közös területe, amely a számítógépes módszerek segítségével elemzi az emberek által használt (természetes) nyelveket. Azaz képes feldolgozni különböző szöveges dokumentumok tartalmát, kinyerni a bennük található információkat, kategorizálni és rendszerezni azokat. A névelem-felismerés többféle módon is megoldható, így például felügyelt tanulással, szótár alapú módszerekkel vagy kollokációk elemzésével. A névelem-felismerés körében két alapvető módszer alkalmazására van lehetőség. A szabályalapú módszer alkalmazása során előre megadott adatok alapján kerül kinyerésre az információ (ilyen szabály például a mondatközi nagybetű mint a tulajdonnév kezdete). A másik módszer a statisztikai tanulás, amikor a gép alkot szabályokat a kutató előzetes mintakódolása alapján. A névelemfelismerés során nehézséget okozhat a különböző névelemosztályok közötti gyakori átfedés, így például ha egy adott szó településnév és vezetéknév is lehet. Fontos különbséget tenni a névelem-felismerés és a tulajdonnév-felismerés között. A névelem-felismerésbe beletartozik minden olyan kifejezés, amely a világ valamely entitására egyedi módon (unikálisan) referál. Ezzel szemben a tulajdonnév-felismerés, kizárólag a tulajdonnevekre koncentrál.[@uvegesNamedEntityRecognition2019; @vinczeBeszedEsNyelvelemzo2019]

A fejezetben részletesen foglalkozunk a lemmatizálással, ami a magyar nyelvű szövegek szövegelőkészítésnek fontos eleme (erről lásd bővebben a *Korpuszépítés és szövegelőkészítés* fejezetet), és a névelem-felismeréssel (*Named Entity Recognition -- NER*). 
Névelemnek azokat a tokensorozatokat nevezzük, amelyek valamely entitást egyedi módon jelölnek. A névelem-felismerés az infomációkinyerés részterülete, melynek lényege, hogy automatikusan felismerjük a strukturálátlan szövegben szereplő tulajdonneveket, majd azokat kigyűjtsük, és típusonként (például személynév, földrajzi név, márkanév, stb.) csoportosítsuk. Bár a tulajdonnevek mellett névelemnek tekinthetők még például a telefonszámok vagy az e-mail címek is, a névelem-felismerés leginkább mégis a tulajdonnevek felismerésére irányul. A névelem-felismerés a számítógépes nyelvészetben a korai 1990-es évektől kezdve fontos feladatnak és megoldandó problémának számít.

A magyar nyelvű szövegekben a tulajdonnevek automatikus felismerésére jelen kötetben a `huSpaCy` [^11_nlp-1] elemző használatát mutatjuk be, amely képes mondatok teljes nyelvi elemzésére (szótő, szófajok, stb.) illetve névelemek (például személynevek, helységek) azonosítására is folyó szövegben, emellett alkalmas a magyar nyelvű szövegek lemmatizálására, ami amint azt korábban bemutattuk, a magyar nyelvű szövegek előfeldolgozásának fontos lépése. Bár magyar nyelven más elemzők [^11_nlp-2] is képesek a nyers szövegek mondatra és szavakra bontására és szófaji elemzésére, azaz POS-taggelésére (*Part of Speech-tagging*) továbbá a mondatok függőségi elemzésére, két okból döntöttünk a `huSpaCy` használata mellett. Egyrészt ez a illeszkedik a nemzetközi akadémia és ipari szférában legszélesebb körben használt `SpaCy` [^11_nlp-3] keretrendszerbe, másrészt a `reticulate` [^11_nlp-4] csomag segítségével viszonylag egyszerűen használható `R` környezetben is, és nagyon jól együttműködik a kötetben rendszeresen használt `quanteda` csomaggal.

[^11_nlp-1] <https://github.com/huspacy/huspacy>

[^11_nlp-2]: Például az UDPipe: <http://lindat.mff.cuni.cz/services/udpipe>, a magyarlanc: <https://rgai.inf.u-szeged.hu/magyarlanc> és az e-magyar: <https://e-magyar.hu/hu/>. Magyar nyelvű szövegek NLP elemzésére használható eszközök részletes listája: <https://github.com/oroszgy/awesome-hungarian-nlp>

[^11_nlp-3]: Részletes leírása: <https://spacy.io/>

[^11_nlp-4]: Részletes leírása: <https://cran.r-project.org/web/packages/reticulate/index.html>



## A `spacyr` használata

```{r}
library(reticulate)
library(spacyr)
library(dplyr)
library(stringr)
library(quanteda)
library(quanteda.textplots)
library(HunMineR)
```

A `spaCy` használatához `Python` környezet szükséges, az első használat előtt telepíteni kell a számítógépünkre egy `Anaconda` alkalmazást: <https://www.anaconda.com/>. Majd az `RStudio/Tools/Global Options` menüjében be kell állítanunk a `Pyton` interpretert, azaz meg kell adnunk, hogy a gépünkön hol található a feltelepített `Anaconda`. Ezt csak az első használat előtt kell megtennünk, a későbbiekben innen folytathatjuk a modell betöltését.


Ezt követően a már megszokott módon installálnunk kell a `reticulate` és a `spacyr` [^11_nlp-5] csomagot és telepítenünk a magyar nyelvi modellt. A Pythonban készült `spacy`-t a `spacyr::spacy_install()` paranccsal kell telepíteni. A következő lépésben létre kell hoznunk egy `conda` környezetet, és a `huggingface`-ről be kell töltenünk a magyar modellt.

[^11_nlp-5]: Részletes leírása: <https://spacyr.quanteda.io/articles/using_spacyr.html>

```{r, eval=FALSE}
conda_install(envname = "spacyr", "https://huggingface.co/huspacy/hu_core_news_lg/resolve/v3.5.2/hu_core_news_lg-any-py3-none-any.whl" , pip = TRUE)

```

```{r}
spacy_initialize(model = "hu_core_news_lg", condaenv="spacyr")
```


### Lemmatizálás, tokenizálás, szófaji egyértelműsítés

Ezután a `spacy_parse()` függvény segítségével lehetőségünk van a szövegek tokenizálására, szótári alakra hozására (lemmatizálására) és szófaji egyértelműsítésére.

```{r, warning=FALSE}
txt <- c(d1 = "Budapesten süt a nap.",
         d2 = "Tájékoztatom önöket, hogy az ülés vezetésében Hegedűs Lorántné és Szűcs Lajos jegyzők lesznek segítségemre.")

parsedtxt <- spacy_parse(txt)

print(parsedtxt)

```

Láthatjuk, hogy az eredmény egy olyan tábla, amely soronként tartalmazza a lemmákat. Mivel az elemzések során legtöbbször arra van szükségünk, hogy egy teljes szöveg lemmáit egy egységként kezeljük, a kapott táblán el kell végeznünk néhány átlakítást. Mivel nekünk a lemmákra van szükségünk, először is töröljük az összes oszlopot a `doc_id` és a `lemma` kivételével.

```{r}
parsedtxt$sentence_id <- NULL
parsedtxt$token_id <- NULL
parsedtxt$token <- NULL
parsedtxt$pos <- NULL
parsedtxt$entity <- NULL

parsedtxt
```

Majd a `doc_id` segítségével összakapcsoljuk azokat a lemmákat, amelyek egy dokumentumhoz tartoznak és az egyes lemmákat `;` segítsével elválasztjuk elmástól.

```{r}
parsedtxt_2<- parsedtxt %>% 
  group_by(doc_id) %>% 
  mutate(text = str_c(lemma, collapse = ";"))

parsedtxt_2

```

Mivel az eredeti táblában minden lemma az eredeti az azt tartalmazó dokumentum id-jét kapta meg, az így létrehozott táblánkban a szövegek annyiszor ismétlődnek, ahány lemmából álltak. Ezért egy következő lépésben ki kell törtölnünk a feleslegesen ismétlődő sorokat. Ehhez először töröljük a `lemma` oszlopot, hogy a sorok tökéletesen egyezzenek.

```{r}
parsedtxt_2$lemma <- NULL
parsedtxt_2
```

Majd a következő lépésben a `dplyr` csomag `distinct` függvénye segítségével - amely mindig csak egy-egy egyedi sort tart meg az adattáblában - kitöröljük a felesleges sorokat.

```{r}

parsedtxt_3 <-distinct(parsedtxt_2)

parsedtxt_3
```

Az így létrejött adattáblában a `text` mezőben már nem az eredeti szöveg, hanem annak lemmái szerepelnek. Ha az adattáblát elmentjük, a lemmákon végezhetjük tovább az elemzéseket.


### Saját `.txt` vagy `.csv` fájlokban elmentett szövegek lemmatizálása

Saját `.txt` vagy `.csv` fájlok lemmatizálásához a fájlokat a kötetben bemutatott módon olvassuk be egy adattáblába. (ehhez lásd a *Függelék, Munka saját adatokkal* alfejezetét) Az alábbi példában egy, a `HunMineR` csomagban lévő kisebb korpuszon mutatjuk be az ilyen fájlok lemmatizálását. Fontos kiemelni, hogy a nagyobb fájlok feldolgozása elég sok időt (akár több órát) is igénybe vehet.

```{r}
df <- HunMineR::data_parlspeakers_small
```

A beolvasott szövegeket először `quanteda` korpusszá alakítjuk. Majd a `spacy_parsed()` függvény segítségével a fentebb bemutatottak szerint elvégezzük a lemmatizálást.

```{r, warning=FALSE}
df_corpus <- corpus(df)

parsed_df <- spacy_parse(df_corpus)

head(parsed_df, 5)
```

```{r}
parsed_df$sentence_id <- NULL
parsed_df$token_id <- NULL
parsed_df$token <- NULL
parsed_df$pos <- NULL
parsed_df$entity <- NULL
```

```{r, warning=FALSE}
parsed_df_2<- parsed_df %>% 
  group_by(doc_id) %>% 
  mutate(
        text = str_c(lemma, collapse = " "))

parsed_df_2$lemma <- NULL

parsed_df_3 <-distinct(parsed_df_2)

head(parsed_df_3, 5)

```

A nagyobb fájlok lemmatizálásának eredményét célszerű elmenteni a kötetben ismert módok egyikén például `RDS` vagy `.csv` fájlba.

### Névelemfelismerés és eredményeinek vizualizálása

A szövegekből történő névelemfelismeréshez ugyancsak egy adattáblára és egy belőle kialakított `quanteda` korpuszra van szükségünk. A következő példában mi az előzőleg léterhozott lemmatizált adattáblával dolgozunk, de a névelemfelismerés működik nyers szövegeken is.

A léterhozott korpuszon a `spacy_parse()` függvény argumentumában kell jeleznünk, hogy az entitások felismerését szeretnénk elvégezni (`entity = TRUE`). Az eredménytáblában láthatjuk, hogy egy új oszlopban minden névelem mellett megkaptuk annak típusát (`PER` = személynév, `LOC` = helynév, `ORG` = szervezet, `MISC` = egyéb).

A `corpus()` függvény egyedi dokumentum neveket vár, ezért átnevezzük először a `doc_id` értékeit.

```{r, warning=FALSE}
parsed_df_3 <- parsed_df_3 %>% 
  mutate(doc_id = paste(doc_id, row_number(), sep = "-"))

lemma_corpus <- corpus(parsed_df_3)

parsedtxt <- spacy_parse(lemma_corpus, entity = TRUE)

entity_extract(parsedtxt, type = "all")
```

A következőkben a névelemfelismerés eredményeinek vizualizálásra mutatunk be egy példát, amihez az előzőekben elkészített lemmákat tartalmazó adattáblát használjuk fel, úgy hogy első lépésként korpuszt készítünk belőle.

```{r}
df <- parsed_df_3

lemma_corpus <-corpus(df)
```

Ezután a `spacy_extract_entity()` függvénye segítségévek elvégezzük a névelemfelismerést. Az elemzés eredményét itt nem adattáblában, hanem listában kérjük vissza.

A névelemek tokenjeit ezután a jobb áttekinthetőség érdekében megritkítottuk, és csak azokat hagytuk meg, amelyek legalább három alkalommal szerepeltek a korpuszban.

```{r}
lemma_ner <- spacy_extract_entity(
  lemma_corpus,
  output = c("list"),
    multithread = TRUE)

ner_tokens <- tokens(lemma_ner)

features <- dfm(ner_tokens) %>%
  dfm_trim(min_termfreq = 3) %>%
  featnames()

ner_tokens <- tokens_select(ner_tokens, features, padding = TRUE)
```

Ezután a különböző alakban előforduló, de ugyanarra az entitásra vonatkozó névelemeket összevontuk.

```{r}
soros <- c("Soros", "Soros György")
lemma <- rep("Soros György", length(soros))
ner_tokens <- tokens_replace(ner_tokens, soros, lemma, valuetype = "fixed")

ogy <- c("Országgyűlés", "Ház")
lemma <- rep("Országgyűlés", length(ogy))
ner_tokens <- tokens_replace(ner_tokens, ogy, lemma, valuetype = "fixed")
```

Majd elkészítettük a szóbeágyazás fejezetben már megismert fcm-et, végezetül pedig egy együttes előfordulási mátrixot készítettünk a kinyert entitásokból és a `ggplot()` segítségével ábrázoltuk (ld \@ref(fig:OGYnevelem). ábra).[^11_nlp-6] Az így kapott ábránk láthatóvá teszi, hogy mely szavak fordulnak elő jellemzően együtt, valamint a vonalvastagsággal azt is egmutatja, hogy ez relatív értelemben milyen gyakran történik.

[^11_nlp-6]: Részletes leírását lásd: <https://tutorials.quanteda.io/basic-operations/fcm/fcm/>

```{r OGYnevelem, fig.cap="Az országgyúlési beszédek névelemeinek együttelőfordulási mátrixa"}
ner_fcm <- fcm(ner_tokens, context = "window", count = "weighted", weights = 1 / (1:5), tri = TRUE)

feat <- names(topfeatures(ner_fcm, 80))
ner_fcm_select <- fcm_select(ner_fcm, pattern = feat, selection = "keep")
dim(ner_fcm_select)

size <- log(colSums(dfm_select(ner_fcm, feat, selection = "keep")))

set.seed(144)
    
textplot_network(ner_fcm_select, min_freq = 0.7, vertex_size = size / max(size) * 3)
```
