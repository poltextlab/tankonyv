[["index.html", "Szövegbányászat és mesterséges intelligencia R-ben Üdvözöljük! Online második kiadás", " Szövegbányászat és mesterséges intelligencia R-ben Sebk Miklós, Ring Orsolya, Máté Ákos 2021 Üdvözöljük! Könyvünk bevezeti az érdekldket a szövegbányászat és a mesterséges intelligencia társadalomtudományi alkalmazásának speciális problémáiba. Támaszkodva a Sebk Miklós által szerkesztett Kvantitatív szövegelemzés és szövegbányászat a politikatudományban (LHarmattan, 2016) cím kötet elméleti bevezetésére, ezúttal a társadalomtudományi elemzések során használható kvantitatív szövegelemzés legfontosabb gyakorlati feladatait vesszük sorra. A szövegek adatként való értelmezése (text as data) és kvantitatív elemzése, avagy a szövegbányászat (text mining) a nemzetközi társadalomtudományi kutatások egyik leggyorsabban fejld irányzata. A szövegbányászat emellett a társadalomtudósok számára az egyik legnyilvánvalóbb belépési pont a mesterséges intelligenciát, ezen belül is gépi tanulást alkalmazó kutatások területére. A magyar tankönyvpiacon elsként ismertetünk lépésrl-lépésre a nemzetközi társadalomtudományban használatos olyan kvantitatív szövegelemzési eljárásokat, mint a névelemfelismerés, a véleményelemzés, a topikmodellezés, illetve a szövegek felügyelt tanulásra épül osztályozása. A módszereink bemutatására szolgáló elemzéseket az egyik leggyakrabban használt programnyelv, az R segítségével végeztük el. A kötet anyaga akár minimális programozási ismerettel is elsajátítható, így teljesen kezdk számára is ajánlott. A hazai olvasók érdekldését szem eltt tartva példáink dönt többsége új, magyar nyelv korpuszokra épül, melyek alapján megismerhetk a magyar nyelv kvantitatív szövegelemzés módozatai. A könyv megrendelhet a Typotex kiadó honlapján! Online második kiadás A nyomtatott kiadáshoz képest a jelenlegi online verzója a könyvnek számos újítást, javítást tartalmaz. Az ábrák többsége interaktív módón felfedezhet, a bevezetésben egy rövid bemutató erejéig megvizsgáljuk Ady és Petfi verseit, a 12. fejezetet pedig a Naiv Bayes felügyelt osztályozási módszerrel bvítettük. Javasolt hivatkozás: Sebk Miklós, Ring Orsolya, és Máté Ákos. 2021. Szövegbányászat és Mesterséges Intelligencia R-ben. Budapest: Typotex. Bib formátumban: @book{sebokringmate2021szovegbanyaszat, address = {Budapest}, title = {Szövegbányászat és mesterséges intelligencia {R}-ben}, publisher = {Typotex}, author = {Sebk, Miklós and Ring, Orsolya and Máté, Ákos}, year = {2021} } A kötet alapjául szolgáló kutatást, amelyet a Társadalomtudományi Kutatóközpont valósított meg, az Innovációs és Technológiai Minisztérium és a Nemzeti Kutatási, Fejlesztési és Innovációs Hivatal támogatta a Mesterséges Intelligencia Nemzeti Laboratórium keretében. A kötet megjelenését az MTA Könyvkiadási Alapja, a Társadalomtudományi Kutatóközpont Könyvtámogatási Alapja, a Nemzeti Kutatási, Fejlesztési és Innovációs Hivatal (NKFIH FK 123907, NKFIH FK 129018), valamint az MTA Bolyai János Kutatási Ösztöndíja támogatta. "],["intro.html", "1 Bevezetés 1.1 A kötet témái 1.2 Használati utasítás 1.3 A HunMineR használata 1.4 Egy gyakorlati példa az R programmal való szövegbányászatra 1.5 Köszönetnyilvánítás", " 1 Bevezetés 1.1 A kötet témái A szövegek adatként való értelmezése (text as data) és kvantitatív elemzése (quantitative text analysis), avagy a szövegbányászat (text mining) a nemzetközi társadalomtudományi kutatások egyik leggyorsabban fejld irányzata. A szövegbányászat egy olyan folyamat, amely során nyers, strukturálatlan szövegeket (pl.: beszédek, felszólalások, újságcikkek) egy strukturált formátumba helyezünk, hogy így új, korábban ismeretlen információkhoz tudjunk hozzájutni, mint trendekhez, mintázatokhoz, összefüggésekhez stb. A szövegek és más kvalitatív adatok (filmek, képek) elemzése annyiban különbözik a mennyiségi (kvantitatív) adatokétól, hogy nyers formájukban még nem alkalmasak statisztikai, illetve ökonometriai elemzésre. Ezért van szükség az ezzel összefügg módszertani problémák speciális tárgyalására. Jelen kötet bevezeti az érdekldket a szövegbányászat és a mesterséges intelligencia társadalomtudományi alkalmazásának ilyen speciális problémáiba, valamint ezek gyakorlati megoldásába. Közvetlen elzménynek tekinthet a témában a Sebk Miklós által szerkesztett Kvantitatív szövegelemzés és szövegbányászat a politikatudományban címmel megjelent könyv, amely a magyar tudományos diskurzusban kevésbé bevett alapfogalmakat és eljárásokat mutatta be (Sebk 2016). A hangsúly az elméleten volt, bár számos fejezet foglalkozott konkrét kódrészletek elemzésével. Míg az elz kötet az egyes kódolási eljárásokat, illetve ezek kutatásmódszertani elnyeit és hátrányait ismertette, ezúttal a társadalomtudományi elemzések során használható kvantitatív szövegelemzés legfontosabb gyakorlati feladatait vesszük sorra, ezek egymással való logikai kapcsolatátt illusztrálja a lenti ábra. A könyvben bemutatott kódok elssorban a quanteda csomagot illetve egyes a quanteda-ra épül csomagokat használnak (Benoit et al. 2018). A szövegbányászat könyvünkben bemutatott egyes módszerei A kötetünkben tárgyalt egyes módszereket három kategóriába sorolhatjuk, a mködési elvük alapján. A szövegek statisztikai leírása egyszerre lehet két szöveg összehasonlítása például a Koszinusz vagy Jaccard hasonlóság alapján, melyekrl a Szövegösszehasonlítás fejzetben írunk, valamint egy adott szöveg különböz statisztikai jellemzinek leírása, az erre voantkozó módszerekkel Leíró statisztika cím fejezet foglalkozik. A szótáralapú módszerekrl és ezek érzelemelemzésre való használatát a Szótárak és érzelemelemzés cím fejezet tárgyalja. Az ideológiai skálázás során politikai szereplk egymáshoz viszonyított realtív pozícióit azonosítjuk általában beszédek, illetve felszólalások alapján az erre vonatkozó módszertant a Szövegskálázás cím fejezet részletezi. A szövegbányászat egyik legalapvetbb problémája a dokumentumok egyes kategóriákba való csoportosítása. Az erre vonatkozó módszerek két alkategóriája közül is többet mutatunk be. Amikor ismert kategóriákba történ osztályzásról beszélünk, akkor a kutatónak kell meghatároznia elre a kategóriákat a modell számára, mieltt az replikálja a besorolásokat. Mind a Naïve Bayes és a Support Vector Machine ezt a feladatot látja el ezek mködésérl és alkalmazásáról az Osztályozás és felügyelt tanulás cím fejezetben írunk. Ezzel szemben ismeretlen kategóriákba való csoportosítás esetén a kutató nem ad elzleges utatsítást az algoritmus számára, a modell a csoportosítást a dokumentumok szövegében meglév látens mintázatok alapján végzi el. Az ismeretlen kategóriákba történ csoportosítást a könyvben bemutatott több módszerrel is elvégezhetjók, mint a K-közép klaszterezés, Latent Dirichlet Allocation, és Struktúrális topikmodellek, amelyeket a Felügyelet nélküli tanulás - Topikmodellezés cím fejezetben tárgyaljuk. A Végezetül pedig a szóbeágyazások ellentétben a korábbiakban említett szózsák logikát követ módszerekkel képesek egy dokumentum szóhasználatának bels összefüggéseit azonosítani, ezzel a kifejezetten rugalmas kutatási módszerrel, illetve alkalmazásával Szóbeágyazások fejezetben foglalkozunk. Könyvünk a magyar tankönyvpiacon elsként ismerteti lépésrl-lépésre a nemzetközi társadalomtudományban használatos kvantitatív szövegelemzési eljárásokat. A módszereink bemutatására szolgáló elemzéseket az R programnyelv segítségével végeztük el, mely a nemzetközi társadalomtudományi vizsgálatok során egyik leggyakrabban használt környezet a Python mellett. A kötetben igyekeztünk magyar szakkifejezéseket használni, de mivel a szövegbányászat nyelve az angol, mindig megadtuk, azok angol megfeleljét is. Kivételt képeznek azok az esetek, ahol nincs használatban megfelel magyar terminológia, ezeknél megtartottuk az angol kifejezéseket, de magyarázattal láttuk el azokat. Az olvasó a két kötet együttes használatával olyan ismeretek birtokába jut, melyek révén képes lesz alkalmazni a kvantitatív szövegelemzés és szövegbányászat legalapvetbb eljárásait saját kutatásaiban. Deduktív vagy induktív felfedez logikája szerint dönthet az adatelemzés módjáról, és a felkínált menübl kiválaszthatja a kutatási tervéhez legjobban illeszked megoldásokat. A bemutatott konkrét példák segítségével pedig akár reprodukálhatja is ezen eljárásokat saját kutatásában. Mindezt a kötet fejezeteiben bséggel tárgyalt R-scriptek (kódok) részletes leírása is segíti. Ennek alapján a kötet két f célcsoportja a társadalomtudományi kutatói és felsoktatási hallgatói-oktatói közösség. Az oktatási alkalmazást segítheti a fontosabb fogalmak magyar és angol nyelv tárgymutatója, valamint több helyen a további olvasásra ajánlott szakirodalom felsorolása. A kötet honlapján (https://tankonyv.poltextlab.com) közvetlenül is elérhetek a felhasznált adatbázisok és kódok. Kötetünk négy logikai egységbl épül fel. Az els négy fejezet bemutatja azokat a fogalmakat és eljárásokat, amelyek elengedhetetlenek egy szövegbányászati kutatás során, valamint itt kerül sor a szöveges adatforrásokkal való munkafolyamat ismertetésére, a szövegelkészítés és a korpuszépítés technikáinak bemutatására. A második blokkban az egyszerbb elemzési módszereket tárgyaljuk, így a leíró statisztikák készítését, a szótár alapú elemzést, valamint érzelemelemzést. A kötet harmadik blokkját a mesterséges intelligencia alapú megközelítéseknek szenteljük, melynek során az olvasó a felügyelt és felügyelet nélküli tanulás fogalmával ismerkedhet meg. A felügyelet nélküli módszerek közül a topik-modellezést, szóbeágyazást és a szövegskálázás wordfish módszerét mutatjuk be, a felügyelt elemzések közül pedig az osztályozással foglalkozunk részletesebben. Végezetül kötetünket egy függelék zárja, melyben a kezd RStudió felhasználóknak adunk gyakorlati iránymutatást a programfelülettel való megismerkedéshez, használatának elsajátításához. 1.2 Használati utasítás A könyv célja, hogy keresztmetszeti képet adjon a szövegbányászat R programnyelven használatos eszközeirl. A fejezetekben ezért a magyarázó szövegben maga az R kód is megtalálható, illetve láthatóak a lefuttatott kód eredményei. Az alábbi példában a sötét háttér az R környezetet jelöli, ahol az R kód bettípusa is eltér a fszövegétl. A kód eredményét pedig a #&gt; kezdet sorokba szedtük, ezzel szimulálva az R console ablakát. # példa R kód 1 + 1 #&gt; [1] 2 Az egyes fejezetekben szerepl kódrészleteket egymás utáni sorrendben bemásolva és lefuttatva a saját R környezetünkben tudjuk reprodukálni a könyvben szerepl technikákat. A Függelékben részletesebben is foglalkozunk az R és az RStudio beállításaival, használatával. Az ajánlott R minimum verzió a 4.0.0, illetve az ajánlott minimum RStudio verzió az 1.4.0000.1 A könyvhöz tartozik egy HunMineR nev R csomag is, amely tartalmazza az egyes fejezetekben használt összes adatbázist, így az adatbeviteli problémákat elkerülve lehet gyakorolni a szövegbányászatot. A könyv megjelenésekor a csomag még nem került be a központi R CRAN csomag repozitóriumába, hanem a poltextLAB GitHub repozitóriumából tölthet le. A könyvben szerepl ábrák nagy része a ggplot2 csomaggal készült a theme_set(theme_light()) opció beállításával a háttérben. Ez azt jelenti, hogy az ábrákat elállító kódok a theme_light() sort nem tartalmazzák, de a tényleges ábrán már megjelennek a tematikus elemek. Az egyes fejezetekben használt R csomagok listája és verziószáma a lenti táblázatban található. Fontos tudni, hogy a használt R csomagokat folyamatosan fejlesztik, ezért elképzelhet hogy eltér verziószámú változatok esetén változhat a kód szintaxis. A könyvben használt R csomagok Csomagnév Verziószám dplyr 1.0.5 e1071 1.7.6 factoextra 1.0.7 gapminder 0.3.0 GGally 2.1.1 ggdendro 0.1.22 ggplot2 3.3.3 ggrepel 0.9.1 HunMineR 0.0.0.9000 igraph 1.2.6 kableExtra 1.3.4 knitr 1.33 lubridate 1.7.10 purrr 0.3.4 quanteda 3.0.0 quanteda.textmodels 0.9.4 quanteda.textplots 0.94 quanteda.textstats 0.94 readr 1.4.0 readtext 0.80 readxl 1.3.1 rvest 1.0.0 spacyr 1.2.1 stm 1.3.6 stringr 1.4.0 text2vec 0.6 tibble 3.1.1 tidyr 1.1.3 tidytext 0.3.1 topicmodels 0.2.12 1.3 A HunMineR használata A Windows rendszert használóknak elször az installr csomagot kell telepíteni, majd annak segítségével letölteni az Rtools nev programot (az OS X és Linux rendszerek esetében erre a lépésre nincs szükség). A lenti kód futtatásával ezek a lépések automatikusan megtörténnek. # az installr csomag letöltése és installálása install.packages(&quot;installr&quot;) # az Rtools.exe fájl letöltése és installálása installr::install.Rtools() Ezt követen a devtools csomagban található install_github paranccsal tudjuk telepíteni a HunMineR csomagot, a lenti kód lefuttatásával. # A devtools csomag letöltése és installálása install.packages(&quot;devtools&quot;) # A HunMineR csomag letöltése és installálása devtools::install_github(&quot;poltextlab/HunMineR&quot;) Ebben a fázisban a data függvénnyel tudjuk megnézni, hogy pontosan milyen adatbázisok szerepelnek a csomagban, illetve ugyanitt megtalálható az egyes adatbázisok részletes leírása. Ha egy adatbázisról szeretnénk többet megtudni, akkor a kiegészít információkat ?adatbazis_neve megoldással tudjuk megnézni.2 # A HunMineR csomag betöltése library(HunMineR) # csomagban lév adatok listázása data(package = &quot;HunMineR&quot;) # A miniszterelnöki beszédek minta adatbázisának részletei ?data_miniszterelnokok 1.4 Egy gyakorlati példa az R programmal való szövegbányászatra A következkben egy példát mutatunk be a szövegbányászat gyakorlati alkalmazására vonatkozóan, amely során Ady Endre és Petfi Sándor összes verseinek szóhasználatát hasonlítjuk össze, az eredményekhez pedig szemléletes ábrázolást is készítünk. A jelen példában alkalmazott eljárásokat a késbbi fejezetekben részletesen is kifejtjük itt csupán szemléltetni kívánjuk velük, hogy milyen jelleg elemzéseket sajátíthat majd el az olvasó a könyv segítségével. Az R számos kiegészít csomaggal rendelkezik, amelyek hasznos funkcióit úgy érhetjük el, ha telepítjük az ket tartalmazó csomagot. Ezt az install.packages paranccsal tehetjük meg, ha már korábban egyszer elvégeztük, akkor nem szükséges egy csomaggal megismételni. A lenti kódsorral a példánkhoz szükséges csomagokat telepíthetjük, de a könyvben használt többi csomagot is ezzel a módszerrel telepíthetjük. install.packages(&quot;readtext&quot;) install.packages(&quot;quanteda&quot;) install.packages(&quot;quanteda.textstats&quot;) install.packages(&quot;quanteda.textplots&quot;) install.packages(&quot;ggplot2&quot;) A library() paranccsal pedig betölteni tudjuk a már telepített csomagjainkat, ezt minden alkalommal el kell, hogy végezzük, hogyha ezek egy funkcióját kívánjuk használni. Itt láthatjuk, hogy a két csomag után a HunMineR betöltése is szükséges, viszont annak ellenére, hogy ugyanazzal a funkcióval hívjuk el a HunMiner nem egy csomag, hanem egy repository (adattár) így a telepítése egy eltér eljárással zajlik az elz alfejezetben leírtaknak megfelelen. library(readtext) library(readr) library(quanteda) library(quanteda.textstats) library(quanteda.textplots) library(ggplot2) library(HunMineR) library(dplyr) Ezt követen betöltjük a szükséges adatokat, jelen esetben a HunMiner adattárából. ady &lt;- HunMineR::data_ady petofi &lt;- HunMineR::data_petofi Ahhoz, hogy adatokat használni tudjunk azokon elször mindig úgynevezett tisztítási folyamatokat kell elvégeznünk, valamint át kell alakítanunk az adataink formátumát is. Az úgynevezett szövegtisztítási folyamatok a szövegeink elkészítését jelentik, ha kihagyjuk ket vagy nem végezzük el ket kell alapossággal, akkor az eredményeink félrevezetek lesznek. Az R-rel való kódolás során többféle formátumban is tárolhatjuk az adatainkat, illetve az adataink egyik formátumból másikba való átalakítására is van lehetségünk. Az egyes formátumok az adatokat különböz elrendezésben tárolják, azért van szükségünk gyakran az adataink átalakítására, mivel az R-n belüli funkciókat, amelyekre szükségünk lesz csak specifikus formátumokon hajthatjuk végre. Els lépésként korpusz formátumba helyezzük az adatainkat, erre csupán azért van szükség mert a késbbiekben ebbl a formátumból tudjuk majd ket, token formátumba helyezni. ady_corpus &lt;- corpus(ady) petofi_corpus &lt;- corpus(petofi) Ezt követen nem csak token formátumba helyezzük az adatokat a lenti kódsorokkal, hanem több szövegtisztító lépést is elvégzünk velük. A remove_punct opció segítségével eltávolítjuk a mondatvégi írásjeleket, veszket. A tokens_tolower() parancs segítségével minden szavunk betjét kisbetvé alakítjuk így a kódunk nem tesz különbséget ugyanazon szó két megjelenése között csupán azért, mert az egyik a mondat elején helyezkedik el. A tokens_remove() parancs segítségével egyes szavakat eltávolíthatunk két kódsorban is megjelenik ez a parancs, elször a quanteda csomag magyar stopszó listájának elemeit távolítjuk el, majd másodjára általunk felsorolt kifejezések eltávolítására használjuk fel. Végül pedig a tokens_wordstem() parancs segítségével úgynevezett szótövezést hajtunk végre. Ez azt jelenti, hogy a szótövekrl levágjuk a ragokat így például a házban és a házhoz kifejezések egységesen ház kifejezésként fognak megjelenni a vizsgálatunkban. ady_tok &lt;- tokens(ady_corpus, remove_punct = TRUE) %&gt;% tokens_tolower() %&gt;% tokens_remove(stopwords(&quot;hungarian&quot;)) %&gt;% tokens_remove (c(&quot;is&quot;, &quot;ha&quot;, &quot;te&quot;, &quot;föl&quot;, &quot;kis&quot;, &quot;eltt&quot;, &quot;oli&quot;, &quot;&quot;, &quot;a&quot;, &quot;txt&quot;, &quot;se&quot;)) petofi_tok &lt;- tokens(petofi_corpus, remove_punct = TRUE) %&gt;% tokens_tolower() %&gt;% tokens_remove(stopwords(&quot;hungarian&quot;)) %&gt;% tokens_remove (c(&quot;is&quot;, &quot;ha&quot;, &quot;te&quot;, &quot;a&quot;, &quot;txt&quot;, &quot;se&quot;)) Ezt követen a token formátumban lév adatainkat dfm formátumba helyezzük. ady_dfm &lt;- dfm(ady_tok) petofi_dfm &lt;- dfm(petofi_tok) Majd megnyírbáljuk a szavaink listáját: a dfm_trim() parancs segítségével beállíthatjuk, hogy milyen gyakorisággal megjelen szavakat hagyjunk az adataink között. Vannak jó okok mind a túlságosan gyakran megjelen és a túlságosan kevésszer megejelen szavak eltávolítására is. Ha egy szó nagyon gyakran jelenik meg, akkor feltételezhet, hogy az nem segít a számunkra értelmezni az egyes dokumentumaink közötti különbséget, mivel nem különbözteti meg azokat. Ha pedig egy szó túlságosan kevésszer jelenik meg, akkor az lehet, hogy egy egyedülálló eset, amely nem adhat a dokumentumainkra általánosítható információt a számunkra. Jelen kód elírja, hogy ahhoz, hogy egy szó bekerüljün az összehasonlításunkba legalább 25-ször meg kell jelenie a szövegben. ady_dfm &lt;- dfm_trim(ady_dfm, min_termfreq = 25) petofi_dfm &lt;- dfm_trim(petofi_dfm, min_termfreq = 25) Végül pedig az R programon belül ábrázolhatjuk is az eredményeinket. Jelen esetben nem végeztünk el számítást, hanem a megtisztított szövegeink szavaiból generáltunk szófelhket. A Petfi versekbl készült szófelh: textplot_wordcloud(petofi_dfm, min_count = 50, color = &quot;red&quot;) Ábra 1.1: Petofi szófelho Az Ady versekbl készült szófelh: textplot_wordcloud(ady_dfm, min_count = 50, color = &quot;orange&quot;) Ábra 1.2: Ady szófelho Az így kapott ábrákon az Ady és Petfi versek szavait láthatjuk egy úgynevezett szófelhben összegyjtve. Annál nagyobb méretben jelenít meg az ábránk egy kifejezést, minél gyakrabban fordul az el a szövegünkben, amely jelen esetben a két költ összes verse. Ugyan a szófelh nem egy statisztikai számítás, amely alapján következtetéseket vonhatnánk le a vizsgált szövegekrl, de egy hasznos és látványos ábrázolási módszer, illetve jelen esetben jól példázza az R egy jelents elnyét, hogy a segítségével az eredményeinket gyorsan és egyszeren tudjuk ábrázolni. 1.5 Köszönetnyilvánítás Jelen kötet az ELKH Társadalomtudományi Kutatóközpont poltextLAB szövegbányászati kutatócsoportja (http://poltextlab.com/) mhelyében készült. A kötet fejezetei Sebk Miklós, Ring Orsolya és Máté Ákos közös munkájának eredményei. Az Alapfogalmak, illetve a Szövegösszehasonlítás fejezetekben társszerz volt Székely Anna. A Bevezetésben, a Függelékben, és az Adatkezelés R-ben, az Osztályozás és felügyelt tanulás cím fejezetekben Gelányi Péter hajtott végre nagyobb frissítéseket, valamint alakította ki a könyv interaktív ábráit. A kézirat a szerzk többéves oktatási gyakorlatára, a hallgatóktól kapott visszajelzésekre építve készült el. Köszönjük a Bibó Szakkollégiumban (2021), a Rajk Szakkollégiumban (20192021), valamint a Széchenyi Szakkollégiumban (2019) tartott féléves, valamint a Corvinus Egyetemen és a Társadalomtudományi Kutatóközpontban tartott rövidebb képzési alkalmak résztvevinek visszajelzéseit. Köszönjük a projekt gyakornokainak, Czene-Joó Máténak, Kaló Eszternek, Meleg Andrásnak, Lovász Dorottyának, Nagy Orsolyának, valamint kutatás asszisztenseinek, Balázs Gergnek, Gelányi Péternek és Lancsár Eszternek a kézirat végleges formába öntése sorn nyújtott segítséget. Külön köszönet illeti a Társadalomtudományi Kutatóközpont Comparative Agendas Project (https://cap.tk.hu/hu) kutatócsoportjának tagjait, kiemelten Boda Zsoltot, Molnár Csabát és Pokornyi Zsanettet a kötetben használt korpuszok sokéves elkészítéséért. Köszönettel tartozunk az egyes fejezetek alapjául szolgáló elemzések és publikációk társszerzinek, Barczikay Tamásnak, Berki Tamásnak, Kacsuk Zoltánnak, Kubik Bálintnak, Molnár Csabának és Szabó Martina Katalinnak. Köszönjük Ballabás Dániel szakmai lektor hasznos megjegyzéseit, Fedinec Csilla nyelvi lektor alapos munkáját, valamint a Typotex Kiadó rugalmasságát és színvonalas közremködését a könyv kiadásában! Végül, de nem utolsósorban hálásak vagyunk a kötet megvalósulásához támogatást nyújtó szervezeteknek és ösztöndíjaknak: az MTA Könyvkiadási Alapjának, a Társadalomtudományi Kutatóközpont Könyvtámogatási Alapjának, a Nemzeti Kutatási, Fejlesztési és Innovációs Hivatalnak (NKFIH FK 123907, NKFIH FK 129018), az MTA Bolyai János Kutatási Ösztöndíjának. A kötet alapjául szolgáló kutatást, amelyet a Társadalomtudományi Kutatóközpont valósított meg, az Innovációs és Technológiai Minisztérium és a Nemzeti Kutatási, Fejlesztési és Innovációs Hivatal támogatta a Mesterséges Intelligencia Nemzeti Laboratórium keretében. Az R Windows, OS X és Linux változatai itt érhetek el: https://cloud.r-project.org/. Az RStudio pedig innen érhet el: https://www.rstudio.com/products/rstudio/download/. Többek között az adat forrása, a változók részletes leírása, illetve az adatbázis mérete is megtalálható így. "],["alapfogalmak.html", "2 Kulcsfogalmak 2.1 Big Data és társadalomtudomány 2.2 Fogalmi alapok 2.3 A szövegbányászat alapelvei", " 2 Kulcsfogalmak 2.1 Big Data és társadalomtudomány A szövegek géppel való feldolgozása és elemzése módszertanának számos megnevezése létezik. A szövegelemzés, kvantitatív szövegelemzés, szövegbányászat, természetes nyelvfeldolgozás, automatizált szövegelemzés, automatizált tartalomelemzés és hasonló fogalmak között nincs éles tartalami különbség. Ezek a kifejezések jellemzen ugyanarra az általánosabb kutatási irányra reflektálnak, csupán hangsúlybeli eltolódások vannak köztük, így gyakran szinonimaként is használják ket. A szövegek gépi feldolgozásával foglalkozó tudományág a Big Data forradalom részeként kezdett kialakulni, melyet az adatok egyre nagyobb és diverzebb tömegének elérhet és összegyjthet jellege hívott életre. Ennek megfelelen az adattudomány számos különböz adatforrás, így képek, videók, hanganyagok, internetes keresési adatok, telefonok lokációs adatai és megannyi különböz információ feldolgozásával foglalkozik. A szöveg is egy az adatbányászat érdekldési körébe tartozó számos adattípus közül, melynek elemzésére külön kutatási irány alakult ki. Mivel napjainkban minden másodpercben óriási mennyiség szöveg keletkezik és válik hozzáférhetvé az interneten, egyre nagyobb az igény az ilyen jelleg források és az emberi nyelv automatizált feldolgozására. Ebbl adódóan az elemzési eszköztár is egyre szélesebb kör és egyre szofisztikáltabb, így a tartalomelemzési és szövegbányászati ismeretekkel bíró elemzk számára rengeteg értékes információ kinyerhet. Ezért a szövegbányászat nemcsak a társadalomtudósok számára izgalmas kutatási irány, hanem gyakran hasznosítják üzleti célokra is. Gondoljunk például az online sajtótermékekre, az ezekhez kapcsolódó kommentekre vagy a politikusok beszédéire. Ezek mind-mind hatalmas mennyiségben rendelkezésre állnak, hasznosításukhoz azonban képesnek kell lenni ezeket a szövegeket összegyjteni, megfelel módon feldolgozni és kiértékelni. A könyv ebben is segítséget nyújt az Olvasónak. Mieltt azonban az adatkezelés és az elemzés részleteire rátérnénk, érdemes végigvenni néhány elvi megfontolást, melyek nélkülözhetetlenek a leend elemz számára az etikus, érvényes és eredményes szövegbányászati kutatások kivitelezéséhez. A nagy mennyiségben rendelkezésre álló szöveges források kiváló kutatási terepet kínálnak a társadalomtudósok számára megannyi vizsgálati kérdéshez, azonban fontos tisztában lenni azzal, hogy a mindenki által elérhet adatokat is meglehetsen körültekinten, etikai szempontok figyelembevételével kell használni. Egy másik szempont, amelyet érdemes szem eltt tartani, mieltt az ember fejest ugrana az adatok végtelenjébe, a 3V elve: volume, velocity, variety vagyis az adatok mérete, a keletkezésük sebessége és azok változatossága (Brady 2019). Ezek mind olyan tulajdonságok, amelyek az adatelemzt munkája során más (és sok esetben több vagy nagyobb) kihívások elé állítják, mint egy hagyományos statisztikai elemzés esetében. A szövegbányászati módszerek abban is eltérnek a hagyományos társadalomtudományi elemzésektl, hogy  az adattudományokba visszanyúló gyökerei miatt  jelents teret nyit az induktív (empiricista) kutatások számára a deduktív szemlélettel szemben. A deduktív kutatásmódszertani megközelítés esetén a kutató elre meghatározza az alkalmazandó fogalomrendszert, és azokat az elvárásokat, amelyek teljesülése esetén sikeresnek tekinti az elemzést. Az adattudományban az ilyen megközelítés a felügyelt tanulási feladatokat jellemzi, vagyis azokat a feladatokat, ahol ismert az elvárt eredmény. Ilyen például egy osztályozási feladat, amikor újságcikkeket szeretnénk különböz témakörökbe besorolni. Ebben az esetben az adatok egy részét általában kézzel kategorizáljuk, és a gépi eljárás sikerességét ehhez viszonyítjuk. Mivel az ideális eredmény (osztálycímke) ismert, a gépi teljesítmény könnyen mérhet (például a pontosság, a gép által sikeresen kategorizált cikkek százalékában kifejezve). Az induktív megoldás esetében kevésbé egyértelm a gépi eljárás teljesítményének mérése, hiszen a rejtett mintázatok feltárását várjuk az algoritmustól, emiatt nincsenek elre meghatározott eredmények sem, amelyekhez viszonyíthatjuk a teljesítményt. Az adattudományban az ilyen feladatokat hívják felügyelet nélküli tanulásnak. Ide tartozik a klaszterelemzés, vagy a topic modellezés, melynek esetén a kutató csak azt határozza meg, hány klasztert, hány témát szeretne kinyerni, a gép pedig létrehozza az egymáshoz leghasonlóbb csoportokat. Értelemszeren itt a kutatói validálás jóval nagyobb hangsúlyt kap, mint a deduktív megközelítés esetében. Egy harmadik, középutas megoldás a megalapozott elmélet megközelítése, mely ötvözi az induktív és a deduktív módszer elnyeit. Ennek során a kutató kidolgoz egy laza elméleti keretet, melynek alapján elvégzi az elemzést, majd az eredményeket figyelembe véve finomít a fogalmi keretén, és újabb elemzést futtat, addig folytatva ezt az iterációt, amíg a kutatás eredményeit kielégítnek nem találja. A szövegbányászati elemzéseket kategorizálhatjuk továbbá a gépi hozzájárulás mértéke szerint. Ennek megfelelen megkülönböztethetünk kézi, géppel támogatott és gépi eljárásokat. Mindhárom megközelítésnek megvan a maga elnye. A kézi megoldások esetén valószínbb, hogy azt mérjük a szövegünkben, amit mérni szeretnénk (például bizonyos szakpolitikai tartalmat), ugyanakkor ez id- és költségigényes. A gépi eljárások ezzel szemben költséghatékonyak és gyorsak, de fennáll a veszélye, hogy nem azt mérjük, amit eredetileg mérni szerettünk volna (ennek megállapításában ismét a validálás kap kulcsszerepet). Továbbá lehetséges kézzel támogatott gépi megoldások alkalmazása, ahol a humán és a gépi elemzés ideális arányának megtalálása jelenti a f kihívást. 2.2 Fogalmi alapok Miután áttekintettük a szövegbányászatban használatos elméleti megközelítéseket, érdemes tisztázni a fogalmi alapokat is. A szövegbányászat szempontjából a szöveg is egy adat. Az elemzéshez használatos strukturált adathalmazt korpusznak nevezzük. A korpusz az összes szövegünket jelöli, ennek részegységei a dokumentumok. Ha például a Magyar Nemzet cikkeit kívánjuk elemezni, a kiválasztott idszak összes cikke lesz a teljes korpuszunk, az egyes cikkek pedig a dokumentumaink. Az elemzés mindig egy meghatározott tématerületre (domain-re) koncentrál. E tématerület utalhat a nyelvre, amelyen a szövegek íródtak, vagy a specifikus tartalomra, amelyet vizsgálunk, de mindenképpen meghatározza a szöveg szókészletével kapcsolatos várakozásainkat. Más lesz tehát a szóhasználat egy bulvárlap cikkeiben, mint egy tudományos szaklap cikkeiben, aminek elssorban akkor van jelentsége, ha szótár alapú elemzéseket készítünk. A szótár alapú elemzések során olyan szószedeteket hozunk létre, amelyek segíthetnek a kutatásunk szempontjából érdekes témák vagy tartalmak azonosításában. Így például létrehozhatunk pozitív és negatív szótárakat, vagy a gazdasági és a külpolitikai témákhoz kapcsolódó szótárakat, melyek segíthetnek azonosítani, hogy adott dokumentum inkább gazdasági vagy inkább külpolitikai témákat tárgyal. Léteznek elre elkészített szótárak  angol nyelven például a Bing Liu által fejlesztett szótár egy jól ismert és széles körben alkalmazható példa (Liu 2010) , azonban fontos fejben tartani, hogy a vizsgált téma specifikus nyelvezete jellemzen meghatározza azt, hogy egy-egy szótárba milyen kifejezéseknek kellene kerülniük. Már említettük, hogy egy szövegbányászati elemzés során a szöveg is adatként kezelend. Tehát hasonló módon gondolhatunk az elemzend szövegeinkre, mint egy statisztikai elemzésre szánt adatbázisra, annak csupán, reprezentációja tér el az utóbbitól. Tehát míg egy statisztikai elemzésre szánt táblázatban elssorban számokat és adott esetben kategorikus változókat reprezentáló karakterláncokat (stringeket)  például férfi/n, falu/város  találunk, addig a szöveges adatokban els ránézésre nem tnik ki gépileg értelmezhet struktúra. Ahhoz, hogy a szövegeink a gépi elemzés számára feldolgozhatóvá váljanak, annak reprezentációját kell megváltoztatni, vagyis strukturálatlan adathalmazból strukturált adathalmazt kell létrehozni, melyet jellemzen a szövegek mátrixszá alakításával teszünk meg. A mátrixszá alakítás els hallásra bonyolult eljárás benyomását keltheti, azonban a gyakorlatban egy meglehetsen egyszer transzformációról van szó, melynek eredményeként a szavakat számokkal reprezentáljuk. A könnyebb megértés érdekében vegyük az alábbi példát: tekintsük a három példamondatot a három elemzend dokumentumnak, ezek összességét pedig a korpuszunknak. 1. Az Európai Unió 27 tagországának egyike Magyarország. 2. Magyarország 2004-ben csatlakozott az Európai Unióhoz. 3. Szlovákia, akárcsak Magyarország, 2004-ben lett ez Európai Unió tagja. A példamondatok dokumentum-kifejezés mátrixsza az alábbi táblázat szerint fog kinézni. Vegyük észre azt is, hogy több olyan kifejezés van, melyek csak ragozásukban térnek el: Unió, Unióhoz; tagja, tagjának. Ezeket a kifejezéseket a kutatói szándék függvényében azonos alakúra hozhatjuk, hogy egy egységként jelenjenek meg. Az elemzések többségében a szövegelkészítés egyik kiinduló lépése a szótövesítés vagy a lemmatizálás, elbbi a szavak toldalékainak levágását jelöli, utóbbi a szavak szótári alakra való visszaalakítását. A ragozás eltávolítását illeten elöljáróban annyit érdemes megjegyezni, hogy az agglutináló, vagyis ragasztó nyelvek esetén, mint amilyen a magyar is, a toldalékok eltávolítása gyakran igen komoly kihívást jelent. Nem csak a toldalékok formája lehet igen sokféle, de az is elfordulhat, hogy a tszó nem egyezik meg a toldalék levágásával keletkez szótvel. Ilyen például a vödröt kifejezés, melynek szótöve a vödr, de a nyelvtanilag helyes tszó a vödör. Hasonlóan a majmok kifejezés esetén a szót a majm lesz, míg a nyelvtanilag helyes tszó a majom. Emiatt a toldalékok levágását a magyar nyelv szövegek esetén megfelel körültekintéssel kell végezni. Táblázat 2.1: Dokumentum-kifejezés mátrix a példamondatokból Dokumentum 27 2004-ben akárcsak az csatlakozott egyike Európai lett Magyarország Szlovákia tagja tagjának Unióhoz 1 1 0 0 1 0 1 1 0 1 0 0 1 0 2 0 1 0 1 1 0 1 0 1 0 0 0 1 3 0 1 1 1 0 0 1 1 1 1 1 0 0 A dokumentum-kifejezés mátrixban minden dokumentumot egy vektor (értsd: egy sor) reprezentál, az eltér kifejezések pedig külön oszlopokat kapnak. Tehát a fenti példában minden dokumentumunk egy 14 elem vektorként jelenik meg, amelynek elemei azt jelölik, hogy milyen gyakran szerepel az adott kifejezés a dokumentumban. A dokumentum-kifejezés mátrixok egy jellemz tulajdonsága, hogy igen nagy dimenziókkal rendelkezhetnek (értsd: sok sorral és sok oszloppal), hiszen minden kifejezést külön oszlopként reprezentálnak. Egy sok dokumentumból álló vagy egy témák tekintetében változatos korpusz esetében a kifejezés mátrix elemeinek jelents része 0 lesz, hiszen számos olyan kifejezés fordul el az egyes dokumentumokban, amelyek más dokumentumban nem szerepelnek. A sok nullát tartalmazó mátrixot hívjuk ritka mátrixnak. Az adatok jobb kezelhetsége érdekben a ritka mátrixot valamilyen dimenzióredukciós eljárással sr mátrixszá lehet alakítani (például a nagyon ritka kifejezések eltávolításával vagy valamilyen súlyozáson alapuló eljárással). 2.3 A szövegbányászat alapelvei A módszertani fogalmak tisztázásást követen néhány elméleti megfontolást osztanánk meg a Grimmer és Stewart (2013) által megfogalmazott alapelvek nyomán, melyek hasznos útravalóul szolgálhatnak a szövegbányászattal ismerked kutatók számára. 1. A szövegbányászat rossz, de hasznos Az emberi nyelv egy meglehetsen bonyolult rendszer, így egy szöveg jelentésének, érzelmi telítettségének különböz olvasók általi értelmezése meglehetsen eltér lehet, így nem meglep, hogy egy gép sok esetben csak korlátozott eredményeket képes felmutatni ezen feladatok teljesítésében. Ettl függetlenül nem elvitatható a szövegbányászati modellek hasznossága, hiszen olyan mennyiség szöveg válik feldolgozhatóvá, ami gépi támogatás nélkül elképzelhetetlen lenne, mindemellett azonban nem lehet megfeledkezni a módszertan korlátairól sem. 2. A kvantitatív modellek kiegészítik az embert, nem helyettesítik azt A kvantitatív eszközökkel történ elemzés nem szünteti meg a szövegek elolvasásának szükségességét, hiszen egészen más információk kinyerését teszi lehetvé, mint egy kvalitatív megközelítés. Emiatt a kvantitatív szövegelemzés talán legfontosabb kihívása, hogy a kutató megtalálja a gépi és a humán erforrások együttes hasznosításának legjobb módját. 3. Nincs legjobb modell Minden kutatáshoz meg kell találni a leginkább alkalmas modellt a kutatási kérdés, a rendelkezésre álló adatok és a kutatói szándék alapján. Gyakran különböz eljárások kombinálása vezethet egy specifikus probléma legjobb megoldásához. Azonban minden esetben az eredmények értékelésére kell támaszkodni, hogy megállapíthassuk egy modell teljesítményét adott problémára és szövegkorpuszra nézve. 4. Validálás, validálás, validálás! Mivel az automatizált szövegelemzés számos esetben jelentsen lecsökkenti az elemzéshez szükséges idt és energiát, csábító lehet a gondolat, hogy ezekhez a módszerekhez forduljon a kutató, ugyanakkor nem szabad elfelejteni, hogy az elemzés csupán a kezdeti lépés, hiszen a kutatónak validálnia kell az eredményeket ahhoz, hogy valóban megbízható következtetésekre jussunk. Az érvényesség-vizsgálat (validálás) lényege egy felügyelet nélküli modell esetén  ahol az elvárt eredmények nem ismertek, így a teljesítmény nem tesztelhet , hogy meggyzdjünk: egy felügyelt modellel (olyan modellel, ahol az elvárt eredmény ismert, így ellenrizhet) egyenérték eredményeket hozzon. Ennek az elvárásnak a teljesítése gyakran nem egyszer, azonban az eljárások alapos kiértékelést nélkülöz alkalmazása meglehetsen kétesélyes eredményekhez vezethet, emiatt érdemes megfelel alapossággal eljárni az érvényesség-vizsgálat során. "],["adatkezeles.html", "3 Adatkezelés R-ben 3.1 Az adatok importálása 3.2 Az adatok exportálása 3.3 A pipe operátor 3.4 Mveletek adattáblákkal 3.5 Munka karakter vektorokkal6", " 3 Adatkezelés R-ben 3.1 Az adatok importálása Az adatok importálására az R alapfüggvénye mellett több csomag is megoldást kínál. Ezek közül a könyv írásakor a legnépszerbbek a readr és a rio csomagok. A szövegek különböz karakterkódolásának problémáját tapasztalataink szerint a legjobban a readr csomag read_csv() függvénye kezeli, ezért legtöbbször ezt fogjuk használni a .csv állományok beolvasására. Amennyiben kihasználjuk az RStudio projekt opcióját (lásd a Függelékben) akkor elegend csak az elérni kívánt adatok relatív elérési útját megadni (relative path). Ideális esetben az adataink egy csv fájlban vannak, ahol az egyes értékeket vesszk (vagy egyéb speciális karakterek) választják el. Ez esetben a read_delim() függvényt is használhatjuk. A beolvasásnál egybl el is tároljuk az adatokat egy objektumban. A sep = opcióval tudjuk a szeparátor karaktert beállítani, mert elfordulhat, hogy vessz helyett pontosvessz tagolja az adatainkat. library(readr) library(dplyr) library(gapminder) library(stringr) library(readtext) df &lt;- readr::read_csv(&quot;data/adatfile.csv&quot;) Az R képes linkrl letölteni fájlokat, elég megadnunk egy mköd elérési útvonalat. df_online &lt;- read_csv(&quot;https://www.qta.tk.mta.hu/adatok/adatfile.csv&quot;) Az R csomag ökoszisztémája kellen változatos ahhoz, hogy gyakorlatilag bármilyen inputtal meg tudjon birkózni. Az Excel fájlokat a readxl csomagot használva tudjuk betölteni a read_excel() függvény használatával.lásd ehhez a Függeléket A leggyakoribb statisztikai programok formátumait pedig a haven csomag tudja kezelni (például Stata, Spss, SAS). A szintaxis itt is hasonló: read_stata(), read_spss(), read_sas(). A nagy mennyiség szöveges dokumentum (a legyakrabban elforduló kiterjesztések: .txt, .doc, .pdf, .json, .csv, .xml, .rtf, .odt) betöltésére a legalkalmasabb a readtext csomag. Az alábbi példa azt mutatja be, hogyan tudjuk beolvasni egy adott mappából az összes .txt kiterjesztés fájlt anélkül, hogy egyenként kellene megadnunk a fájlok neveit. A kódsorban szerepl * karakter ebben a környezetben azt jelenti, hogy bármilyen fájl az adott mappában, ami .txt-re végzdik. Amennyiben a fájlok nevei tartalmaznak valamilyen metaadatot, akkor ezt is be tudjuk olvasni a betöltés során. Ilyen metaadat lehet például egy parlamenti felszólalásnál a felszólaló neve, a beszéd ideje, a felszólaló párttagsága (például: kovacsjanos_1994_fkgp.txt). df_text &lt;- readtext::readtext( &quot;data/*.txt&quot;, docvarsfrom = &quot;filenames&quot;, dvsep = &quot;_&quot;, docvarnames = c(&quot;nev&quot;, &quot;ev&quot;, &quot;part&quot;) ) 3.2 Az adatok exportálása Az adatainkat R-bl a write.csv()-vel exportálhatjuk a kívánt helyre, .csv formátumba. Az openxlsx csomaggal .xls és .xlsx Excel formátumokba is tudunk exportálni. Az R rendelkezik saját, .Rds és .Rda kiterjesztés, tömörített fájlformátummal. Mivel ezeket csak az R-ben nyithatjuk meg, érdemes a köztes, hosszadalmas számítást igényl lépések elmentésére használni, a saveRDS() és a save() parancsokkal. 3.3 A pipe operátor Az úgynevezett pipe operátor alapjaiban határozta meg a modern R fejldését és a népszer csomag ökoszisztéma, a tidyverse, egyik alapköve. Úgy gondoljuk, hogy a tidyverse és a pipe egyszerbbé teszi az R használatának elsajátítását, ezért mi is erre helyezzük a hangsúlyt.3 Vizuálisan a pipe operátor így néz ki: %&gt;%, és arra szolgál, hogy a kódban több egymáshoz kapcsolódó mveletet egybefzzünk.4 Technikailag a pipe a bal oldali elemet adja meg a jobb oldali függvény els argumentumának. A lenti példa ugyanazt a folyamatot írja le az alap R (base R), illetve a pipe használatával.5 Miközben a kódot olvassuk, érdemes a pipe-ot és aztán-nak fordítani. reggeli(oltozkodes(felkeles(ebredes(en, idopont = &quot;8:00&quot;), oldal = &quot;jobb&quot;), nadrag = TRUE, ing = TRUE)) en %&gt;% ebredes(idopont = &quot;8:00&quot;) %&gt;% felkeles(oldal = &quot;jobb&quot;) %&gt;% oltozkodes(nadrag = TRUE, ing = TRUE) %&gt;% reggeli() A fenti példa is jól mutatja, hogy a pipe a bal oldali elemet fogja a jobb oldali függvény els elemének berakni. A fejezet további részeiben még bven fogunk gyakorlati példát találni a pipe használatára. Mivel az itt bemutatott példák az alkalmazásoknak csak egy relatíve szk körét mutatják be, érdemes átolvasni a csomagokhoz tartozó dokumentációt, illetve ha van, akkor tanulmányozni a mködést demonstráló bemutató oldalakat is. 3.4 Mveletek adattáblákkal Az adattábla (data frame) az egyik leghasznosabb és leggyakrabban használt adattárolási mód az R-ben (a részletesebb leírás a Függelékben található). Ebben az alfejezetben azt mutatjuk be a dplyr és gapminder csomagok segítségével, hogyan lehet vele hatékonyan dolgozni. A dplyr az egyik legnépszerbb R csomag, a tidyverse része. A gapminder csomag pedig a példa adatbázisunkat tartalmazza, amiben a világ országainak különböz gazdasági és társadalmi mutatói találhatók. A sorok (megfigyelések) szréséhez a dplyr csomag filter() parancsát használva lehetségünk van arra, hogy egy vagy több kritérium alapján szkítsük az adatbázisunkat. A lenti példában azokat a megfigyeléseket tartjuk meg, ahol az év 1962 és a várható élettartam több mint 72 év. gapminder %&gt;% dplyr::filter(year == 1962, lifeExp &gt; 72) #&gt; # A tibble: 5 x 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Denmark Europe 1962 72.4 4646899 13583. #&gt; 2 Iceland Europe 1962 73.7 182053 10350. #&gt; 3 Netherlands Europe 1962 73.2 11805689 12791. #&gt; 4 Norway Europe 1962 73.5 3638919 13450. #&gt; 5 Sweden Europe 1962 73.4 7561588 12329. Ugyanígy leválogathatjuk az adattáblából az adatokat akkor is, ha egy karakter változó alapján szeretnénk szrni. gapminder %&gt;% filter(country == &quot;Sweden&quot;, year &gt; 1990) #&gt; # A tibble: 4 x 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Sweden Europe 1992 78.2 8718867 23880. #&gt; 2 Sweden Europe 1997 79.4 8897619 25267. #&gt; 3 Sweden Europe 2002 80.0 8954175 29342. #&gt; 4 Sweden Europe 2007 80.9 9031088 33860. Itt tehát az adattábla azon sorait szeretnénk látni, ahol az ország megegyezik a Sweden karakterlánccal, az év pedig 1990 utáni. A select() függvény segítségével válogathatunk oszlopokat a data frame-bl. A változók kiválasztására több megoldás is van. A dplyr csomag tartalmaz apróbb kisegít függvényeket, amik megkönnyítik a nagy adatbázisok esetén a változók kiválogatását a nevük alapján. Ezek a függvények a contains(), starts_with(), ends_with(), matches(), és beszédesen arra szolgálnak, hogy bizonyos nev változókat ne kelljen egyenként felsorolni. A select()-en belüli változó sorrend egyben az eredmény data frame változójának sorrendjét is megadja. A negatív kiválasztás is lehetséges, ebben az esetben egy - jelet kell tennünk a nem kívánt változó(k) elé (pl.: select(df, year, country, -continent). gapminder %&gt;% dplyr::select(dplyr::contains(&quot;ea&quot;), dplyr::starts_with(&quot;co&quot;), pop) #&gt; # A tibble: 1,704 x 4 #&gt; year country continent pop #&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 1952 Afghanistan Asia 8425333 #&gt; 2 1957 Afghanistan Asia 9240934 #&gt; 3 1962 Afghanistan Asia 10267083 #&gt; 4 1967 Afghanistan Asia 11537966 #&gt; 5 1972 Afghanistan Asia 13079460 #&gt; 6 1977 Afghanistan Asia 14880372 #&gt; # ... with 1,698 more rows Az így kiválogatott változókból létrehozhatunk és objektumként eltárolhatunk egy új adattáblát az objektumok részletesebb leírása a függelékben található Függelékben, amivel azután tovább dolgozhatunk, vagy kiírathatjuk például .csv fájlba, vagy elmenthetjük a saveRDS segítségével. gapminder_select &lt;- gapminder %&gt;% select(contains(&quot;ea&quot;), starts_with(&quot;co&quot;), pop) readr::write_csv(gapminder_select, &quot;gapminder_select.csv&quot;) saveRDS(gapminder_select, &quot;gapminder_select.Rds&quot;) A saveRDS segítségével elmentett fájlt késbb a readRDS() függvénnyel olvashatjuk be, majd onnan folytathatjuk a munkát, ahol korábban abbahagytuk. readRDS(&quot;gapminder_select.Rds&quot;) Az elemzési munkafolyamat elkerülhetetlen része, hogy új változókat hozzunk létre, vagy a meglévket módosítsuk. Ezt a mutate()-el tehetjük meg, ahol a szintaxis a következ: mutate(data frame, uj valtozo = ertekek). Példaként kiszámoljuk a svéd GDP-t (milliárd dollárban) 1992-tl kezdve. A mutate() alkalmazását részletesebben is bemutatjuk a szövegek elkészítésével foglalkozó fejezetben. gapminder %&gt;% filter(country == &quot;Sweden&quot;, year &gt;= 1992) %&gt;% dplyr::mutate(gdp = (gdpPercap * pop) / 10^9) #&gt; # A tibble: 4 x 7 #&gt; country continent year lifeExp pop gdpPercap gdp #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Sweden Europe 1992 78.2 8718867 23880. 208. #&gt; 2 Sweden Europe 1997 79.4 8897619 25267. 225. #&gt; 3 Sweden Europe 2002 80.0 8954175 29342. 263. #&gt; 4 Sweden Europe 2007 80.9 9031088 33860. 306. Az adataink részletesebb és alaposabb megismerésében segítenek a különböz szint leíró statisztikai adatok. A szintek megadására a group_by() használható, a csoportokon belüli számításokhoz pedig a summarize(). A lenti példa azt illusztrálja, hogy ha kontinensenként csoportosítjuk a gapminder adattáblát, akkor a summarise() használatával megkaphatjuk a megfigyelések számát, illetve az átlagos per capita GDP-t. A summarise() a mutate() közeli rokona, hasonló szintaxissal és logikával használható. Ezt a függvénypárost fogjuk majd használni a szöveges adataink leíró statisztikáinál is az 5. fejezetben. gapminder %&gt;% dplyr::group_by(continent) %&gt;% dplyr::summarise(megfigyelesek = n(), atlag_gdp = mean(gdpPercap)) #&gt; # A tibble: 5 x 3 #&gt; continent megfigyelesek atlag_gdp #&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Africa 624 2194. #&gt; 2 Americas 300 7136. #&gt; 3 Asia 396 7902. #&gt; 4 Europe 360 14469. #&gt; 5 Oceania 24 18622. 3.5 Munka karakter vektorokkal6 A szöveges adatokkal (karakter stringekkel) való munka elkerülhetetlen velejárója, a vektorokról, köztük a karakter vektorokról részletesebben a Függelékben írunk. A felesleges szövegelemeket, karaktereket el kell távolítanunk, hogy javuljon az elemzésünk hatásfoka. Erre a célra a stringr csomagot fogjuk használni, kombinálva a korábban bemutatott mutate()-el. A stringr függvények az str_ eltaggal kezddnek és eléggé beszédes nevekkel rendelkeznek. Egy gyakran elforduló probléma, hogy extra szóközök maradnak a szövegben, vagy bizonyos szavakról, karakterkombinációkról tudjuk, hogy nem kellenek az elemzésünkhöz. Ebben az esetben egy vagy több reguláris kifejezés (regular expression, regex) használatával tudjuk pontosan kijelölni, hogy a karakter sornak melyik részét akarjuk módosítani.7 A legegyszerbb formája a regexeknek, ha pontosan tudjuk milyen szöveget akarunk megtalálni. A kísérletezésre az str_view()-t használjuk, ami megjeleníti, hogy a megadott regex mintánk pontosan mit jelöl. A függvény match = TRUE paramétere lehetvé teszi, hogy csak a releváns találatokat kapjuk vissza. szoveg &lt;- c(&quot;gitar&quot;, &quot;ukulele&quot;, &quot;nagybogo&quot;) stringr::str_view(szoveg, pattern = &quot;ar&quot;) Reguláris kifejezésekkel rákereshetünk nem csak egyes elemekre (pl.: szavak, szótagok, betk) hanem olyan konkrét esetekre is, amikor ezek egymás után fordulnak el. Ilyenkor úgynevezett ngrammokat használunk, amelyek egy karakterláncban szerepl n-számú elem szekvenciája. A lenti példban ezeke mködését egy úgynevezett bigrammal tehát egy n=2 érték ngrammal mutatjuk be. Az ngrammok segítenek kezelni olyan eseteket, amikor két egymást követ elem eltér jelentéssel bír, egymás mellett, mint külön-külön. szoveg &lt;- c(&quot;a fehér ház fehérebb mint bármely más ház&quot;) stringr::str_view(szoveg, pattern = &quot;fehér ház&quot;) Az ún. horgonyokkal, (anchor) azt lehet megadni, hogy a karakter string elején vagy végén szeretnénk-e egyezést találni. A string eleji anchor a ^, a string végi pedig a $. str_view(&quot;Dr. Doktor Dr.&quot;, pattern = &quot;^Dr.&quot;) str_view(&quot;Dr. Doktor Dr.&quot;, pattern = &quot;Dr.$&quot;) Továbbá azt is meghatározhatjuk, hogy egy adott karakter, vagy karakter kombinációt, valamint a mellett elhelyezked karaktereket is szeretnénk kijelölni. str_view(&quot;Dr. Doktor Dr.&quot;, pattern = &quot;.k.&quot;) Egy másik jellemz probléma, hogy olyan speciális karaktert akarunk leírni a regex kifejezésünkkel, ami amúgy a regex szintaxisban használt. Ilyen eset például a ., ami mint írásjel sokszor csak zaj, ám a regex kontextusban a bármilyen karakter megfelelje. Ahhoz, hogy magát az írásjelet jelöljük, a \\\\ -t kell elé rakni. str_view(&quot;Dr. Doktor Dr.&quot;, pattern = &quot;\\\\.&quot;) Néhány hasznos regex kifejezés: [:digit:] - számok (123) [:alpha:] - betk (abc ABC) [:lower:] - kisbetk (abc) [:upper:] - nagybetk (ABC) [:alnum:] - betk és számok (123 abc ABC) [:punct:] - központozás (.!?\\(){}) [:graph:] - betk, számok és központozás (123 abc ABC .!?\\(){}) [:space:] - szóköz ( ) [:blank:] - szóköz és tabulálás [:cntrl:] - kontrol karakterek (\\n, \\r, stb.) * - bármi A tidyverse megközelítés miatt a kötetben szerepl R kód követi a The tidyverse style guide dokumentációt (https://style.tidyverse.org/) Az RStudio-ban a pipe operátor billenty kombinációja a Ctrl + Shift + M Köszönjük Andrew Heissnek a kitn példát. A könyv terjedelme miatt ezt a témát itt csak bemutatni tudjuk, de minden részletre kiterjeden nem tudunk elmélyülni benne. A témában nagyon jól használható online anyagok találhatóak az RStudio GitHub tárhelyén (https://github.com/rstudio/cheatsheets/raw/master/strings.pdf), illetve Wickham and Grolemund (2016) 14. fejezetében. A reguláris kifejezés egy olyan, meghatározott szintaktikai szabályok szerint leírt karakterlánc (string), amivel meghatározható stringek egy adott halmaza. Az ilyen kifejezés valamilyen minta szerinti szöveg keresésére, cseréjére, illetve a szöveges adatok ellenrzésére használható. További információ: http://www.regular-expressions.info/ "],["corpus_ch.html", "4 Korpuszépítés és szövegelkészítés 4.1 Szövegbeszerzés 4.2 Szövegelkészítés", " 4 Korpuszépítés és szövegelkészítés 4.1 Szövegbeszerzés A szövegbányászati elemzések egyik els lépése az elemzés alapjául szolgáló korpusz megépítése. A korpuszt alkotó szövegek beszerzésének egyik módja a webscarping, melynek során weboldalakról történik az információ kinyerése. A scrapelést végezhetjük R-ben az rvest csomag segítségével. Fejezetünkben a scrapelésnek csupán néhány alaplépését mutatjuk meg.8 library(rvest) library(readr) library(dplyr) library(lubridate) library(stringr) library(quanteda) library(quanteda.textmodels) library(quanteda.textstats) library(HunMineR) A szükséges csomagok9 beolvasása után a read_html() függvény segítségével az adott weboldal adatait kérjük le a szerverrl. A read_html() függvény argumentuma az adott weblap URL-je. Ha például a poltextLAB projekt honlapjáról szeretnénk adatokat gyjteni, azt az alábbi módon tehetjük meg: r &lt;- rvest::read_html(&quot;https://poltextlab.tk.hu/hu&quot;) r #&gt; {html_document} #&gt; &lt;html lang=&quot;hu&quot; class=&quot;no-js&quot;&gt; #&gt; [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; cha ... #&gt; [2] &lt;body class=&quot;index&quot;&gt;\\n\\n\\t&lt;script&gt;\\n\\t (function(i,s,o,g,r,a,m ... Ezután a html_nodes() függvény argumentumaként meg kell adnunk azt a HTML címkét vagy CSS azonosítót, ami a legyjteni kívánt elemeket azonosítja a weboldalon. [^websraping] Ezeket az azonosítókat az adott weboldal forráskódjának megtekintésével tudhatjuk meg, amire a különböz böngészk különböz lehetségeket kínálnak. Majd a html_text() függvény segítségével megkapjuk azokat a szövegeket, amelyek az adott weblapon az adott azonosítóval rendelkeznek. Példánkban a https://poltextlab.tk.hu/hu weboldalról azokat az információkat szeretnénk kigyjteni, amelyek az &lt;title&gt; címke alatt szerepelnek. title &lt;- read_html(&quot;https://poltextlab.tk.hu/hu&quot;) %&gt;% rvest::html_nodes(&quot;title&quot;) %&gt;% rvest::html_text() title #&gt; [1] &quot;MTA TK Political and Legal Text Mining and Artificial Intelligence Laboratory (poltextLAB)&quot; Ezután a kigyjtött információkat kiírhatjuk egy csv fájlba. write_csv(title, &quot;title.csv&quot;) A webscraping során az egyik nehézség, ha a weboldal letiltja az automatikus letöltést, ezt kivédhetjük például különböz böngészbvítmények segítségével, illetve a fejléc (header) vagy a hálózati kliens (user agent) megváltoztatásával. De segíthet véletlenszer kiszolgáló (proxy) vagy VPN szolgáltatás10 használata is, valamint ha az egyes kérések között idt hagyunk. Mieltt egy weboldal tartalmának scrapelését elkezdenénk, fontos tájékozódni a hatályos szerzi jogi szabályozásokról. A weboldalakon legtöbbször a legyjtött szövegekhez tartozó különböz metaadatok is szerepelnek (például egy parlamenti beszéd dátuma, az azt elmondó képvisel neve), melyeket érdemes a scarpelés során szintén összegyjteni. A scrapelés során fontos figyelnünk arra, hogy késbb jól használható formában mentsük el az adatokat, például .csv,.json vagy .txt kiterjesztésekkel. A karakterkódolási problémák elkerülése érdekében érdemes UTF-8 vagy UTF-16-os kódolást alkalmazni, mivel ezek tartalmazzák a magyar nyelv ékezetes karaktereit is.11 Arra is van lehetség, hogy az elemezni kívánt korpuszt papíron keletkezett, majd szkennelt és szükség szerint optikai karakterfelismerés (Optical Character Recognition  OCR) segítségével feldolgozott szövegekbl építsük fel. Mivel azonban ezeket a feladatokat nem R-ben végezzük, ezekrl itt nem szólunk bvebben. Az így beszerzett és .txt vagy .csv fájllá alakított szövegekbl való korpuszépítés a következ lépésekben megegyezik a weboldalakról gyjtött szövegekével. 4.2 Szövegelkészítés Az elemzéshez vezet következ lépés a szövegelkészítés, amit a szöveg tisztításával kell kezdenünk. A szövegtisztításnál mindig járjunk el körültekinten és az egyes lépéseket a kutatási kérdésünknek megfelelen tervezzük meg, a folyamat során pedig idnként végezzünk ellenrzést, ezzel elkerülhetjük a kutatásunkhoz szükséges információk elvesztését. Miután az elemezni kívánt szövegeinket beszereztük, majd a Az adatok importálása alfejezetben leírtak szerint importáltuk, következhetnek az alapvet elfeldolgozási lépések, ezek közé tartozik például a scrapelés során a korpuszba került html címkék, számok és egyéb zajok (például a speciális karakterek, írásjelek) eltávolítása, valamint a kisbetsítés, a tokenizálás, a szótövezés és a tiltólistás szavak eltávolítása, azaz stopszavazás. A stringr csomag segítségével elször eltávolíthatjuk a felesleges html címkéket a korpuszból.12 Ehhez elször létrehozzuk a text1 nev objektumot, ami egy karaktervektorból áll. text1 &lt;- c(&quot;MTA TK&quot;, &quot;&lt;font size=&#39;6&#39;&gt; Political and Legal Text Mining and Artificial Intelligence Laboratory (poltextLAB)&quot;) text1 #&gt; [1] &quot;MTA TK&quot; #&gt; [2] &quot;&lt;font size=&#39;6&#39;&gt; Political and Legal Text Mining and Artificial Intelligence Laboratory (poltextLAB)&quot; Majd a str_replace_all()függvény segítségével eltávolítjuk két html címke közötti szövegrészt. Ehhez a függvény argumentumában létrehozunk egy regex kifejezést, aminek segítségével a függvény minden &lt; &gt; közötti szövegrészt üres karakterekre cserél. Ezután a str_to_lower()mindent kisbetvé konvertál, majd a str_trim() eltávolítja a szóközöket a karakterláncok elejérl és végérl. text1 %&gt;% stringr::str_replace_all(pattern = &quot;&lt;.*?&gt;&quot;, replacement = &quot;&quot;) %&gt;% stringr::str_to_lower() %&gt;% stringr::str_trim() #&gt; [1] &quot;mta tk&quot; #&gt; [2] &quot;political and legal text mining and artificial intelligence laboratory (poltextlab)&quot; 4.2.1 Tokenizálás, szótövezés, kisbetsítés és a tiltólistás szavak eltávolítása Az elkészítés következ lépésében tokenizáljuk, azaz egységeire bontjuk az elemezni kívánt szöveget, így a tokenek az egyes szavakat vagy kifejezéseket fogják jelölni. Ennek eredményeként kapjuk meg az n-gramokat, amik a vizsgált egységek (számok, betk, szavak, kifejezések) n-elem sorozatát alkotják. A következkben a Példa az elkészítésre mondatot bontjuk elször tokenekre a tokens() függvénnyel, majd a tokeneket a tokens_tolower() segítségével kisbetsítjük, a tokens_wordstem() függvénnyel pedig szótövezzük. Végezetül a quanteda csomagban található magyar nyelv stopszótár segítségével, elvégezzük a tiltólistás szavak eltávolítását. Ehhez elször létrehozzuk az sw elnevezés karaktervektort a magyar stopszavakból. A head() függvény segítségével belenézhetünk a szótárba, és a console-ra kiírathatjuk a szótár els hat szavát. Végül a tokens_remove()segítségével eltávolítjuk a stopszavakat. text &lt;- &quot;Példa az elokészítésre&quot; toks &lt;- quanteda::tokens(text) toks &lt;- quanteda::tokens_tolower(toks) toks &lt;- quanteda::tokens_wordstem(toks) toks #&gt; Tokens consisting of 1 document. #&gt; text1 : #&gt; [1] &quot;példa&quot; &quot;az&quot; &quot;elokészítésr&quot; sw &lt;- quanteda::stopwords(&quot;hungarian&quot;) head(sw) #&gt; [1] &quot;a&quot; &quot;ahogy&quot; &quot;ahol&quot; &quot;aki&quot; &quot;akik&quot; &quot;akkor&quot; quanteda::tokens_remove(toks, sw) #&gt; Tokens consisting of 1 document. #&gt; text1 : #&gt; [1] &quot;példa&quot; &quot;elokészítésr&quot; Ezt követi a szótövezés (stemmelés) lépése, melynek során az alkalmazott szótövez algoritmus egyszeren levágja a szavak összes toldalékát, a képzket, a jelzket és a ragokat. Szótövezés helyett alkalmazhatunk szótári alakra hozást is (lemmatizálás). A két eljárás közötti különbség abban rejlik, hogy a szótövezés során csupán eltávolítjuk a szavak toldalékként azonosított végzdéseit, hogy ugyanannak a szónak különböz megjelenési formáit közös törzsre redukáljuk, míg a lemmatizálás esetében rögtön az értelmes, szótári formát kapjuk vissza. A két módszer közötti választás a kutatási kérdés alapján meghozott kutatói döntésen alapul (Grimmer and Stewart 2013). Az alábbi példában egyetlen szó különböz alakjainak szótári alakra hozásával szemléltetjük a lemmatizálás mködését. Ehhez elször a text1 nev objektumban tároljuk a szótári alakra hozni kívánt szöveget, majd tokenizáljuk és eltávolítjuk a központozást. Ezután definiáljuk a megfelel szótövet és azt, hogy mely szavak alakjait szeretnénk erre a szótre egységesíteni, majd a rep() függvény segítségével a korábban zárójelben megadott kifejeztéseket az elokeszites lemmával helyettesítjük, azaz a korábban definiált szólakokat az általunk megadott szótári alakkal helyettesítjük. Hosszabb szövegek lemmatizálásához elre létrehozott szótárakat használhatunk, ilyen például a WordNet, ami magyar nyelven is elérhet.13 text1 &lt;- &quot;Példa az elkészítésre. Az elkészítést a szövetisztítással kell megkezdenünk. Az elkészített korpuszon elemzést végzünk&quot; toks1 &lt;- tokens(text1, remove_punct = TRUE) elokeszites &lt;- c(&quot;elkészítésre&quot;, &quot;elkészítést&quot;, &quot;elkészített&quot;) lemma &lt;- rep(&quot;elkészítés&quot;, length(elokeszites)) toks1 &lt;- quanteda::tokens_replace(toks1, elokeszites, lemma, valuetype = &quot;fixed&quot;) toks1 #&gt; Tokens consisting of 1 document. #&gt; text1 : #&gt; [1] &quot;Példa&quot; &quot;az&quot; &quot;elokészítés&quot; #&gt; [4] &quot;Az&quot; &quot;elokészítés&quot; &quot;a&quot; #&gt; [7] &quot;szövetisztítással&quot; &quot;kell&quot; &quot;megkezdenünk&quot; #&gt; [10] &quot;Az&quot; &quot;elokészítés&quot; &quot;korpuszon&quot; #&gt; [ ... and 2 more ] A fenti text1 objektumban tárolt szöveg szótövezését az alábbiak szerint tudjuk elvégezni. Megvizsgálva az elkészítés különböz alakjainak lemmatizált és stemmelt változatát jól láthatjuk a két módszer közötti különbséget. text1 &lt;- &quot;Példa az elkészítésre. Az elkészítést a szövetisztítással kell megkezdenünk. Az elkészített korpuszon elemzést végzünk&quot; toks2 &lt;- tokens(text1, remove_punct = TRUE) toks2 &lt;- tokens_wordstem(toks2) toks2 #&gt; Tokens consisting of 1 document. #&gt; text1 : #&gt; [1] &quot;Példa&quot; &quot;az&quot; &quot;elokészítésr&quot; #&gt; [4] &quot;Az&quot; &quot;elokészítést&quot; &quot;a&quot; #&gt; [7] &quot;szövetisztításs&quot; &quot;kell&quot; &quot;megkezdenünk&quot; #&gt; [10] &quot;Az&quot; &quot;elokészített&quot; &quot;korpuszon&quot; #&gt; [ ... and 2 more ] 4.2.2 Dokumentum kifejezés mátrix (dtm, dfm) A szövegbányászati elemzések nagy részéhez szükségünk van arra, hogy a szövegeinkbl dokumentum kifejezés mátrix-ot (Document Term Matrix  dtm vagy Document Feature Matrix  dfm) hozzunk létre.14 Ezzel a lépéssel alakítjuk a szövegeinket számokká, ami lehetvé teszi, hogy utána különböz statisztikai mveleteket végezzünk velük. A dokumentum kifejezés mátrix minden sora egy dokumentum, minden oszlopa egy kifejezés, az oszlopokban szerepl változók pedig megmutatják az egyes kifejezések számát az egyes dokumentumokban. A legtöbb dokumentum kifejezés mátrix ritka mátrix, mivel a legtöbb dokumentum és kifejezés párosítása nem történik meg: a kifejezések nagy része csak néhány dokumentumban szerepel, ezek értéke nulla lesz. Az alábbi példában három, egy-egy mondatos dokumentumon szemléltetjük a fentieket. A korábban megismert módon elkészítjük, azaz kisbetsítjük, szótövezzük a dokumentumokat, eltávolítjuk a tiltólistás szavakat, majd létrehozzuk bellük a dokumentum kifejezés mátrixot.15 text &lt;- c( d1 = &quot;Ez egy példa az elofeldolgozásra&quot;, d2 = &quot;Egy másik lehetséges példa&quot;, d3 = &quot;Ez pedig egy harmadik példa&quot; ) dfm &lt;- text %&gt;% tokens %&gt;% tokens_remove(pattern = stopwords(&quot;hungarian&quot;)) %&gt;% tokens_tolower() %&gt;% tokens_wordstem(language = &quot;hungarian&quot;) %&gt;% dfm() dfm #&gt; Document-feature matrix of: 3 documents, 4 features (50.00% sparse) and 0 docvars. #&gt; features #&gt; docs péld elofeldolgozás lehetséges harmad #&gt; d1 1 1 0 0 #&gt; d2 1 0 1 0 #&gt; d3 1 0 0 1 4.2.3 Súlyozás A dokumentum kifejezés mátrix lehet egy egyszer bináris mátrix, ami csak azt az információt tartalmazza, hogy egy adott szó elfordul-e egy adott dokumentumban. Míg az egyszer bináris mátrixban ugyanakkora súlya van egy szónak ha egyszer és ha tízszer szerepel, készíthetünk olyan mátrixot is, ahol egy szónak annál nagyobb a súlya egy dokumentumban, minél többször fordul el. A szógyakoriság (term frequency  TF) szerint súlyozott mátrixnál azt is figyelembe vesszük, hogy az adott szó hány dokumentumban szerepel. Minél több dokumentumban szerepel egy szó, annál kisebb a jelentsége. Ilyen szavak például a névelk, amelyek sok dokumentumban elfordulnak ugyan, de nem sok tartalmi jelentséggel bírnak. Két szó közül általában az a fontosabb, amelyik koncentráltan, kevés dokumentumban, de azokon belül nagy gyakorisággal fordul el. A dokumentum gyakorisági érték (document frequency  DF) egy szó gyakoriságát jellemzi egy korpuszon belül. A súlyozási sémákban általában a dokumentum gyakorisági érték inverzével számolnak (inverse document frequency - IDF), ez a leggyakrabban használt TF-IDF súlyozás (term frequency &amp; inverse document frequency - TF-IDF). Az így súlyozott TF mátrix egy-egy cellájában található érték azt mutatja, hogy egy adott szónak mekkora a jelentsége egy adott dokumentumban. A TF-IDF súlyozás értéke tehát magas azon szavak esetén, amelyek az adott dokumentumban gyakran fordulnak el, míg a teljes korpuszban ritkán; alacsonyabb azon szavak esetén, amelyek az adott dokumentumban ritkábban, vagy a korpuszban gyakrabban fordulnak el; és kicsi azon szavaknál, amelyek a korpusz lényegében összes dokumentumában elfordulnak (Tikk 2007, 3337 o.) Az alábbiakban az 1999-es törvényszövegeken szemléltetjük, hogy egy 125 dokumentumból létrehozott mátrix segítségével milyen alapvet statisztikai mveleteket végezhetünk.16 A HunMineR csomagból tudjuk importálni a törvényeket. lawtext_df &lt;- HunMineR::data_lawtext_1999 Majd az importált adatokból létrehozzuk a korpuszt lawtext_corpus néven. Ezt követi a dokumentum kifejezés mátrix kialakítása (mivel a quanteda csomaggal dolgozunk, dfm mátrixot hozunk létre), és ezzel egy lépésben elvégezzük az alapvet szövegtisztító lépéseket is. lawtext_corpus &lt;- quanteda::corpus(lawtext_df) lawtext_dfm &lt;- lawtext_corpus %&gt;% tokens( remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE ) %&gt;% tokens_tolower() %&gt;% tokens_remove(pattern = stopwords(&quot;hungarian&quot;)) %&gt;% tokens_wordstem(language = &quot;hungarian&quot;) %&gt;% dfm() A topfeatures() függvény segítségével megnézhetjük a mátrix leggyakoribb szavait, a függvény argumentumában megadva a dokumentum kifejezés mátrix nevét és a kívánt kifejezésszámot. quanteda::topfeatures(lawtext_dfm, 15) #&gt; the bekezdés of áll szerzodo #&gt; 7942 5877 5666 4267 3620 #&gt; rendelkezés törvény to hely an #&gt; 3403 3312 3290 3114 3068 #&gt; személy év kiadás b ha #&gt; 3048 2975 2884 2831 2794 Mivel látható, hogy a szövegekben sok angol kifejezés is volt egy következ lépcsben az angol stopszavakat is eltávolítjuk. lawtext_dfm_2 &lt;- quanteda::dfm_remove(lawtext_dfm, pattern = stopwords(&quot;english&quot;)) Ezután megnézzük a leggyakoribb 15 kifejezést. topfeatures(lawtext_dfm_2, 15) #&gt; bekezdés áll szerzodo rendelkezés törvény #&gt; 5877 4267 3620 3403 3312 #&gt; hely személy év kiadás b #&gt; 3114 3048 2975 2884 2831 #&gt; ha következo költségvetés muködés eset #&gt; 2794 2398 2376 2325 2070 A következ lépés, hogy TF-IDF súlyozású statisztikát készítünk, a dokumentum kifejezés mátrix alapján. Ehhez elször létrehozzuk a lawtext_tfidf nev objektumot, majd a textstat_frequency() függvény segítségével kilistázzuk annak els 10 elemét. lawtext_tfidf &lt;- quanteda::dfm_tfidf(lawtext_dfm_2) quanteda.textstats::textstat_frequency(lawtext_tfidf, force = TRUE, n = 10) #&gt; feature frequency rank docfreq group #&gt; 1 felhalmozás 1452 1 7 all #&gt; 2 szerzodo 1349 2 53 all #&gt; 3 shall 1291 3 14 all #&gt; 4 kiadás 1280 4 45 all #&gt; 5 költségvetés 1101 5 43 all #&gt; 6 támogatás 1035 6 38 all #&gt; 7 beruházás 942 7 22 all #&gt; 8 contracting 894 8 7 all #&gt; 9 articl 884 9 14 all #&gt; 10 muködés 741 10 60 all A folyamatról bvebb információ található például az alábbi oldalakon: https://cran.r-project.org/web/packages/rvest/rvest.pdf, https://rvest.tidyverse.org. A quanteda csomagnak több kiegészít csomagja van, amelyeknek a tartalmára a nevük utal. A könyvben a quanteda.textmodels, quanteda.textplots, quanteda.textstats csomagokat használjuk még, amelyek statisztikai és vizualizaciós függvényeket tartalmaznak. A VPN (Virtual Private Network, azaz a virtuális magánhálózat) azt teszi lehetvé, hogy a felhasználók egy megosztott vagy nyilvános hálózaton keresztül úgy küldjenek és fogadjanak adatokat, mintha számítógépeik közvetlenül kapcsolódnának a helyi hálózathoz. A karakterkódolással kapcsolatosan további hasznos információk találhatóak az alábbi oldalon: http://www.cs.bme.hu/~egmont/utf8. A stringr csomag jól használható eszköztárat kínál a különböz karakterláncokkezeléséhez. Részletesebb leírása megtalálható: https://stringr.tidyverse.org/. A karakterláncokról bvebben: https://r4ds.had.co.nz/strings.html WordNet: https://github.com/mmihaltz/huwn. A magyar nyelv szövegek lemmatizálását elvégezhetjük a szövegek R-be való beolvasása eltt is a magyarlanc nyelvi elemz segítségével, melyrl a Természetes-nyelv feldolgozás (NLP) és névelemfelismerés cím fejezetben szólunk részletesebben. A két mátrix csak a nevében különbözik, tartalmilag nem. A használni kívánt csomag leírásában mindig megtalálható, hogy dtm vagy dfm mátrix segítségével dolgozik-e. Mivel a könyvben általunk használt quanteda csomag dfm mátrixot használ, mi is ezt használjuk. Bár a lenti mátrixok, amelyekben csak 0 és 1 szerepel, a bináris mátrix látszatát keltik, de valójában nem azok, ha egy-egy kifejezésbl több is szerepelne a mondatban, a mátrixban 2, 3, 4 stb. szám lenne. Az itt használt kódok az alábbiakon alapulnak: https://rdrr.io/cran/quanteda/man/dfm_weight.html, https://rdrr.io/cran/quanteda/man/dfm_tfidf.html. A példaként használt korpusz a Hungarian Comparative Agendas Project keretében készült adatbázis része: https://cap.tk.hu/torveny "],["leiro_stat.html", "5 Leíró statisztika 5.1 Szövegek a vektortérben 5.2 Leíró statisztika 5.3 A szövegek lexikai diverzitása 5.4 Összehasonlítás21 5.5 A kulcsszavak kontextusa", " 5 Leíró statisztika 5.1 Szövegek a vektortérben A szövegbányászati feladatok két altípusa a keresés és a kinyerés. A keresés során olyan szövegeket keresünk, amelyekben egy adott kifejezés elfordul. A webes keresprogramok egyik jellemz tevékenysége, az információ-visszakeresés (information retrieval) során például az a cél, hogy a korpuszból visszakeressük a keres információigénye szempontjából releváns információkat, mely keresés alapulhat metaadatokon vagy teljes szöveges indexelésen (Russel and Norvig 2005, 742.o; Tikk 2007). Az információkinyerés (information extraction) esetén a cél, hogy a strukturálatlan szövegekbl strukturált adatokat állítsunk el. Azaz az információkinyerés során nem a felhasználó által keresett információt keressük meg és lokalizáljuk, hanem az adott kérdés szempontjából releváns információkat gyjtjük ki a dokumentumokból. Az információkinyerés alternatív megoldása segítségével már képesek lehetünk a kifejezések közötti kapcsolatok elemzésére, tendenciák és minták felismerésére és az információk összekapcsolása révén új információk létrehozására, azaz a segítségével strukturálatlan szövegekbl is elállíthatunk strukturált információkat (Kwartler 2017; Schütze, Manning, and Raghavan 2008; Tikk 2007, 6381.o). A szövegbányászati vizsgálatok során folyó szövegek, azaz strukturálatlan vagy részben strukturált dokumentumok elemzésére kerül sor. Ezekbl a kutatási kérdéseink szempontjából releváns, látens összefüggéseket nyerünk ki, amelyek már strukturált szerkezetek. A dokumentumok reprezentálásának három legelterjedtebb módja a halmazelmélet alapú, az algebrai és a valószínségi modell. A halmazelméleti modellek a dokumentumok hasonlóságát halmazelmélet, a valószínségi modellek pedig feltételes valószínségi becslés alapján határozzák meg. Az algebrai modellek a dokumentumokat vektorként vagy mátrixként ábrázolják és algebrai mveletek segítségével hasonlítják össze. A vektortérmodell sokdimenziós vektortérben ábrázolja a dokumentumokat, úgy, hogy a dokumentumokat vektorokkal reprezentálja, a vektortér dimenziói pedig a dokumentumok összességében elforduló egyedi szavak. A modell alkalmazása során azok a dokumentumok hasonlítanak egymásra, amelyeknek a szókészlete átfedi egymást, és a hasonlóság mértéke az átfedéssel arányos. A vektortérmodellben a dokumentumgyjteményt a dokumentum-kifejezés mátrixszal reprezentáljuk, a mátrixban a sorok száma megegyezik az egyedi szavak számával, az oszlopokat pedig a dokumentumvektorok alkotják. Az egyedi szavak összességét szótárnak nevezzük. Mivel mátrixban az egyedi szavak száma általában igen nagy, ezért a mátrix hatékony kezeléséhez annak mérete különböz eljárásokkal csökkenthet. Fontos tudni, hogy a dokumentumok vektortér reprezentációjában a szavak szövegen belüli sorrendjére és pozíciójára vonatkozó információ nem található meg (Russel and Norvig 2005, 74244 o.; Kwartler 2017; Welbers, Van Atteveldt, and Benoit 2017). A vektortérmodellt szózsák (bag of words) modellnek is nevezzük, melynek segítségével a fent leírtak szerint az egyes szavak gyakoriságát vizsgálhatjuk meg egy adott korpuszon belül. 5.2 Leíró statisztika Fejezetünkben nyolc véletlenszeren kiválasztott magyar miniszterelnöki beszéd vizsgálatát végezzük el,17 amihez az alábbi csomagokat használjuk: library(HunMineR) library(readtext) library(dplyr) library(lubridate) library(stringr) library(ggplot2) library(quanteda) library(quanteda.textstats) library(quanteda.textplots) library(GGally) library(ggdendro) library(tidytext) library(plotly) Els lépésben a Bevezetben már ismertetett módon a HunMineR csomagból betöltjük a beszédeket. A glimpse() függvénnyel egy gyors pilltast vethetünk a betöltött adatokra. texts &lt;- HunMineR::data_miniszterelnokok_raw dplyr::glimpse(texts) #&gt; Rows: 7 #&gt; Columns: 4 #&gt; $ doc_id &lt;chr&gt; &quot;antall_jozsef_1990&quot;, &quot;bajnai_gordon_2009&quot;, &quot;gyurcsán~ #&gt; $ text &lt;chr&gt; &quot;Elnök Úr! Tisztelt Országgyulés! Hölgyeim és Uraim! ~ #&gt; $ year &lt;dbl&gt; 1990, 2009, 2005, 1994, 2002, 1995, 2018 #&gt; $ pm &lt;chr&gt; &quot;antall_jozsef&quot;, &quot;bajnai_gordon&quot;, &quot;gyurcsány_ferenc&quot;,~ A glimpse funkció segítségével nem csak a sorok és oszlopok számát tekinthetjük meg, hanem az egyes oszlopok neveit is, amelyek alapján megállapíthatjuk, hogy milyen információkat tartalmaz ez az objektum. Az egyes beszédek dokumentum azonosítóját, azok szövegét, az évüket és végül a miniszterelnök nevét, aki elmondta az adott beszédet. Ezt követen az Adatkezelés R-ben cím fejezetben ismertetett mutate() függvény használatával két csoportra osztjuk a beszédeket. Ehhez elször a string_extract() függvény segítségével meghatározzuk, hogy a kettéosztáshoz használni kívánt új változó a doc_id legyen, a [^\\\\.]* regex segítségével leválasztva arról a .txt kiterjesztést, majd a str_sub() függvénnyel megmondjuk, hogy a miniszterelnökök neve a doc_id hátulról számított hatodik karakteréig tart. Ezután kialakítjuk a két csoportot, azaz az if_else() segítségével meghatározzuk, hogy ha antall_jozsef, boross_peter, orban_viktor beszédeirl van szó azokat a jobb csoportba tegye, a maradékot pedig a bal csoportba. Majd azt is meghatározzuk, hogy melyik beszédnek mi a dátuma. Ehhez szintén a str_sub() függvényt használjuk, majd a lubridate segítségével alakítjuk ki a kívánt dátumformátumot.18 Ezután a glimpse() függvény segítségével megtekintjük, hogy milyen változtatásokat végeztünk az adattáblánkon. Láthatjuk, hogy míg korábban 8 dokumentumunk és 2 változónk volt, az átalakítás eredményeként a 8 dokumentum mellett már 5 változót találunk. Ezzel a lépéssel tehát kialakítottuk azokat a változókat, amelyekre az elemzés során szükségünk lesz. miniszterelnokok &lt;- c(&quot;antall_jozsef&quot;, &quot;boross_peter&quot;, &quot;orban_viktor&quot;) texts &lt;- texts %&gt;% mutate( doc_id = stringr::str_extract(doc_id, &quot;[^\\\\.]*&quot;), mineln = stringr::str_sub(doc_id, end = -6), partoldal = dplyr::if_else(mineln %in% miniszterelnokok, &quot;jobb&quot;, &quot;bal&quot;) ) texts$year &lt;- str_sub(texts$doc_id, start = -2) %&gt;% stringr::str_c(&quot;-01-01&quot;) %&gt;% lubridate::ymd() %&gt;% lubridate::year() glimpse(texts) #&gt; Rows: 7 #&gt; Columns: 6 #&gt; $ doc_id &lt;chr&gt; &quot;antall_jozsef_1990&quot;, &quot;bajnai_gordon_2009&quot;, &quot;gyurc~ #&gt; $ text &lt;chr&gt; &quot;Elnök Úr! Tisztelt Országgyulés! Hölgyeim és Urai~ #&gt; $ year &lt;dbl&gt; 1990, 2009, 2005, 1994, 2002, 1998, 2018 #&gt; $ pm &lt;chr&gt; &quot;antall_jozsef&quot;, &quot;bajnai_gordon&quot;, &quot;gyurcsány_feren~ #&gt; $ mineln &lt;chr&gt; &quot;antall_jozsef&quot;, &quot;bajnai_gordon&quot;, &quot;gyurcsány_feren~ #&gt; $ partoldal &lt;chr&gt; &quot;jobb&quot;, &quot;bal&quot;, &quot;bal&quot;, &quot;bal&quot;, &quot;bal&quot;, &quot;jobb&quot;, &quot;jobb&quot; Ezt követen a további lépések elvégzéséhez létrehozzuk a quanteda korpuszt, majd a summary() függvény segítségével megtekinthetjük a korpusz alapvet statisztikai jellemzit. Láthatjuk például, hogy az egyes dokumentumok hány tokenbl vagy mondatból állnak. corpus_mineln &lt;- corpus(texts) summary(corpus_mineln) #&gt; Corpus consisting of 7 documents, showing 7 documents: #&gt; #&gt; Text Types Tokens Sentences year pm #&gt; antall_jozsef_1990 3745 9408 431 1990 antall_jozsef #&gt; bajnai_gordon_2009 1391 3277 201 2009 bajnai_gordon #&gt; gyurcsány_ferenc_2005 2963 10267 454 2005 gyurcsány_ferenc #&gt; horn_gyula_1994 1704 4372 226 1994 horn_gyula #&gt; medgyessy_peter_2002 1021 2362 82 2002 medgyessy_peter #&gt; orban_viktor_1998 1810 4287 212 1998 orban_viktor #&gt; orban_viktor_2018 933 1976 126 2018 orban_viktor #&gt; mineln partoldal #&gt; antall_jozsef jobb #&gt; bajnai_gordon bal #&gt; gyurcsány_ferenc bal #&gt; horn_gyula bal #&gt; medgyessy_peter bal #&gt; orban_viktor jobb #&gt; orban_viktor jobb Mivel az elemzés során a korpuszon belül két csoportra osztva szeretnénk összehasonlításokat tenni, az alábbiakban két alkorpuszt alakítunk ki. mineln_jobb &lt;- quanteda::corpus_subset(corpus_mineln, mineln %in% c(&quot;antall_jozsef&quot;, &quot;boross_peter&quot;, &quot;orban_viktor&quot;)) mineln_bal &lt;- quanteda::corpus_subset(corpus_mineln, mineln %in% c(&quot;horn_gyula&quot;, &quot;gyurcsany_ferenc&quot;, &quot;medgyessy_peter&quot;, &quot;bajnai_gordon&quot;)) summary(mineln_jobb) #&gt; Corpus consisting of 3 documents, showing 3 documents: #&gt; #&gt; Text Types Tokens Sentences year pm #&gt; antall_jozsef_1990 3745 9408 431 1990 antall_jozsef #&gt; orban_viktor_1998 1810 4287 212 1998 orban_viktor #&gt; orban_viktor_2018 933 1976 126 2018 orban_viktor #&gt; mineln partoldal #&gt; antall_jozsef jobb #&gt; orban_viktor jobb #&gt; orban_viktor jobb summary(mineln_bal) #&gt; Corpus consisting of 3 documents, showing 3 documents: #&gt; #&gt; Text Types Tokens Sentences year pm #&gt; bajnai_gordon_2009 1391 3277 201 2009 bajnai_gordon #&gt; horn_gyula_1994 1704 4372 226 1994 horn_gyula #&gt; medgyessy_peter_2002 1021 2362 82 2002 medgyessy_peter #&gt; mineln partoldal #&gt; bajnai_gordon bal #&gt; horn_gyula bal #&gt; medgyessy_peter bal A korábban létrehozott jobb és bal változó segítségével nem csak az egyes dokumentumokat, hanem a két csoportba sorolt beszédeket is összehasonlíthatjuk egymással. summary(corpus_mineln) %&gt;% group_by(partoldal) %&gt;% summarise( mean_wordcount = mean(Tokens), std_dev = sd(Tokens), min_wordc = min(Tokens), max_wordc = max(Tokens) ) #&gt; # A tibble: 2 x 5 #&gt; partoldal mean_wordcount std_dev min_wordc max_wordc #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 bal 5070. 3561. 2362 10267 #&gt; 2 jobb 4710. 3271. 1976 9408 A textstat_collocations() függvény segítségével szókapcsolatokat kereshetünk. A függvény argumentumai közül a size a szókapcsolatok hossza, a min_count pedig a minimális elfordulásuk száma. Miután a szókapcsolatokat megkerestük, közülük a korábban már megismert head() függvény segítségével tetszleges számút megnézhetünk.19 corpus_mineln %&gt;% quanteda.textstats::textstat_collocations( size = 3, min_count = 6 ) %&gt;% head(n = 10) #&gt; collocation count count_nested length lambda z #&gt; 1 a kormány a 21 0 3 1.510 2.996 #&gt; 2 az új kormány 10 0 3 4.092 2.571 #&gt; 3 az a politika 6 0 3 3.849 2.418 #&gt; 4 a száz lépés 9 0 3 3.245 1.438 #&gt; 5 a magyar gazdaság 12 0 3 2.199 1.374 #&gt; 6 tisztelt hölgyeim és 31 0 3 3.290 1.266 #&gt; 7 ez a program 8 0 3 1.683 1.107 #&gt; 8 hogy ez a 10 0 3 0.530 1.024 #&gt; 9 hogy a magyar 18 0 3 0.756 0.855 #&gt; 10 az ellenzéki pártok 6 0 3 1.434 0.784 A szókapcsolatok listázásánál is láthattuk, hogy a korpuszunk még minden szót tartalmaz, ezért találtunk például hogy ez a összetételt. A következkben eltávolítjuk az ilyen funkció nélküli stopszavakat a korpuszból, amihez saját stopszólistát használunk. Elször a HunMineR csomagból beolvassuk és egy custom_stopwords nev objektumban tároljuk a stopszavakat, majd a tokens() függvény segítségével tokenizáljuk a korpuszt és a tokens_select() használatával eltávolítjuk a stopszavakat. Ha ezután újra megnézzük a kollokációkat, jól látható a stopszavak eltávolításának eredménye: custom_stopwords &lt;- HunMineR::data_stopwords_extra corpus_mineln %&gt;% tokens() %&gt;% tokens_select(pattern = custom_stopwords, selection = &quot;remove&quot;) %&gt;% textstat_collocations( size = 3, min_count = 6 ) %&gt;% head(n = 10) #&gt; collocation count count_nested length lambda z #&gt; 1 taps MSZP soraiból 7 0 3 -1.72 -0.932 #&gt; 2 taps kormánypártok soraiban 13 0 3 -1.75 -1.025 #&gt; 3 tisztelt hölgyeim uraim 31 0 3 -3.14 -1.062 #&gt; 4 közbeszólás fidesz soraiból 12 0 3 -4.24 -1.891 #&gt; 5 taps MSZP soraiban 9 0 3 -4.58 -2.702 A korpusz további elemzése eltt fontos, hogy ne csak a stopszavakat távolítsuk el, hanem az egyéb alapvet szövegtisztító lépéseket is elvégezzük. Azaz a tokens_select() segítségével eltávolítsuk a számokat, a központozást, az elválasztó karaktereket, mint például a szóközöket, tabulátorokat, sortöréseket. Ezután a tokens_ngrams() segítségével n-gramokat (n elem karakterláncokat) hozunk létre a tokenekbl, majd kialakítjuk a dokumentum kifejezés mátrixot (dfm) és elvégezzük a tf-idf szerinti súlyozást. A dfm_tfidf() függvény kiszámolja a dokumentum gyakoriság inverz súlyozását. A függvény alapértelmezés szerint a normalizált kifejezések gyakoriságát használja a dokumentumon belüli relatív kifejezés gyakoriság helyett, ezt írjuk felül a schem_tf = \"prop\" használatával. Végül a textstat_frequency() segítségével gyakorisági statisztikát készíthetünk a korábban meghatározott (példánkban két és három tagú) n-gramokról. corpus_mineln %&gt;% tokens( remove_numbers = TRUE, remove_punct = TRUE, remove_separators = TRUE ) %&gt;% tokens_select(pattern = custom_stopwords, selection = &quot;remove&quot;) %&gt;% quanteda::tokens_ngrams(n = 2:3) %&gt;% dfm() %&gt;% dfm_tfidf(scheme_tf = &quot;prop&quot;) %&gt;% quanteda.textstats::textstat_frequency(n = 10, force = TRUE) #&gt; feature frequency rank docfreq group #&gt; 1 fordítsanak_hátat 0.00228 1 1 all #&gt; 2 tisztelt_hölgyeim 0.00225 2 4 all #&gt; 3 tisztelt_hölgyeim_uraim 0.00225 2 4 all #&gt; 4 fidesz_soraiból 0.00201 4 1 all #&gt; 5 taps_mszp 0.00159 5 2 all #&gt; 6 magyarország_európa 0.00136 6 1 all #&gt; 7 tisztelt_képviselotársaim 0.00130 7 2 all #&gt; 8 kormánypártok_soraiban 0.00129 8 2 all #&gt; 9 taps_kormánypártok 0.00117 9 2 all #&gt; 10 taps_kormánypártok_soraiban 0.00117 9 2 all 5.3 A szövegek lexikai diverzitása Az alábbiakban a korpuszunkat alkotó szövegek lexikai diverzitását vizsgáljuk. Ehhez a quanteda csomag textstat_lexdiv() függvényét használjuk. Elször a corpus_mineln nev korpuszunkból létrehozzuk a mineln_dfm nev dokumentum-kifejezés mátrixot, amelyen elvégezzük a korábban már megismert alapvet tisztító lépéseket. A textstat_lexdiv() függvény eredménye szintén egy dfm, így azt arrange() parancs argumentumában a desc megadásával csökken sorba is rendezhetjük. Atextstat_lexdiv() különböz indexek segítségével számítja ki a szövegek lexikai különbözségét.20 mineln_dfm &lt;- corpus_mineln %&gt;% tokens( remove_punct = TRUE, remove_separators = TRUE, split_hyphens = TRUE ) %&gt;% dfm() %&gt;% quanteda::dfm_remove(pattern = custom_stopwords) mineln_dfm %&gt;% quanteda.textstats::textstat_lexdiv(measure = &quot;CTTR&quot;) %&gt;% dplyr::arrange(dplyr::desc(CTTR)) #&gt; document CTTR #&gt; 1 antall_jozsef_1990 33.0 #&gt; 2 gyurcsány_ferenc_2005 26.1 #&gt; 3 orban_viktor_1998 23.4 #&gt; 4 horn_gyula_1994 22.3 #&gt; 5 bajnai_gordon_2009 19.9 #&gt; 6 medgyessy_peter_2002 16.8 #&gt; 7 orban_viktor_2018 16.2 A megkapott értékeket hozzáadhatjuk a dfm-hez is. A lenti kód egy dfm_lexdiv nev adattáblát hoz létre, amely tartalmazza a mineln_dfm adattábla sorait, valamint a lexikai diverzitás értékeket. dfm_lexdiv &lt;- mineln_dfm cttr_score &lt;- unlist(textstat_lexdiv(dfm_lexdiv, measure = &quot;CTTR&quot;)[, 2]) quanteda::docvars(dfm_lexdiv, &quot;cttr&quot;) &lt;- cttr_score docvars(dfm_lexdiv) #&gt; year pm mineln partoldal cttr #&gt; 1 1990 antall_jozsef antall_jozsef jobb 33.0 #&gt; 2 2009 bajnai_gordon bajnai_gordon bal 19.9 #&gt; 3 2005 gyurcsány_ferenc gyurcsány_ferenc bal 26.1 #&gt; 4 1994 horn_gyula horn_gyula bal 22.3 #&gt; 5 2002 medgyessy_peter medgyessy_peter bal 16.8 #&gt; 6 1998 orban_viktor orban_viktor jobb 23.4 #&gt; 7 2018 orban_viktor orban_viktor jobb 16.2 A fenti elemzést elvégezhetjük úgy is, hogy valamennyi indexálást egyben megkapjuk. Ehhez a textstat_lexdiv() függvény argumentumába a measure = \"all\" kifejezést kell megadnunk. mineln_dfm %&gt;% textstat_lexdiv(measure = &quot;all&quot;) #&gt; document TTR C R CTTR U S K I #&gt; 1 antall_jozsef_1990 0.647 0.949 46.7 33.0 72.9 0.960 11.2 419 #&gt; 2 bajnai_gordon_2009 0.728 0.957 28.2 19.9 73.2 0.962 16.4 459 #&gt; 3 gyurcsány_ferenc_2005 0.556 0.930 37.0 26.1 52.2 0.944 11.9 292 #&gt; 4 horn_gyula_1994 0.714 0.956 31.5 22.3 74.0 0.962 12.6 572 #&gt; 5 medgyessy_peter_2002 0.711 0.951 23.8 16.8 62.8 0.955 26.5 251 #&gt; 6 orban_viktor_1998 0.722 0.957 33.0 23.4 78.1 0.964 16.5 400 #&gt; 7 orban_viktor_2018 0.753 0.958 23.0 16.2 71.5 0.961 26.2 313 #&gt; D Vm Maas lgV0 lgeV0 #&gt; 1 0.000930 0.0287 0.117 11.19 25.8 #&gt; 2 0.000974 0.0269 0.117 10.43 24.0 #&gt; 3 0.000960 0.0279 0.138 9.23 21.3 #&gt; 4 0.000745 0.0232 0.116 10.66 24.5 #&gt; 5 0.001755 0.0373 0.126 9.42 21.7 #&gt; 6 0.001172 0.0314 0.113 11.02 25.4 #&gt; 7 0.001545 0.0345 0.118 9.98 23.0 Ha pedig arra vagyunk kíváncsiak, hogy a kapott értékek hogyan korrelálnak egymással, azt a cor() függvény segítésével számolhatjuk ki. div_df &lt;- mineln_dfm %&gt;% textstat_lexdiv(measure = &quot;all&quot;) cor(div_df[, 2:13]) #&gt; TTR C R CTTR U S K I #&gt; TTR 1.000 0.970 -0.6532 -0.6532 0.7504 0.8470 0.5974 0.257 #&gt; C 0.970 1.000 -0.4521 -0.4521 0.8838 0.9498 0.4250 0.402 #&gt; R -0.653 -0.452 1.0000 1.0000 -0.0157 -0.1587 -0.8337 0.260 #&gt; CTTR -0.653 -0.452 1.0000 1.0000 -0.0157 -0.1587 -0.8337 0.260 #&gt; U 0.750 0.884 -0.0157 -0.0157 1.0000 0.9835 -0.0050 0.636 #&gt; S 0.847 0.950 -0.1587 -0.1587 0.9835 1.0000 0.1496 0.571 #&gt; K 0.597 0.425 -0.8337 -0.8337 -0.0050 0.1496 1.0000 -0.585 #&gt; I 0.257 0.402 0.2602 0.2602 0.6361 0.5714 -0.5846 1.000 #&gt; D 0.384 0.226 -0.6556 -0.6556 -0.1554 -0.0194 0.9458 -0.772 #&gt; Vm 0.277 0.154 -0.4803 -0.4803 -0.1495 -0.0415 0.8568 -0.826 #&gt; Maas -0.781 -0.908 0.0505 0.0505 -0.9969 -0.9932 -0.0453 -0.617 #&gt; lgV0 0.332 0.549 0.4784 0.4784 0.8692 0.7822 -0.4354 0.709 #&gt; D Vm Maas lgV0 #&gt; TTR 0.3839 0.2773 -0.7812 0.332 #&gt; C 0.2261 0.1545 -0.9076 0.549 #&gt; R -0.6556 -0.4803 0.0505 0.478 #&gt; CTTR -0.6556 -0.4803 0.0505 0.478 #&gt; U -0.1554 -0.1495 -0.9969 0.869 #&gt; S -0.0194 -0.0415 -0.9932 0.782 #&gt; K 0.9458 0.8568 -0.0453 -0.435 #&gt; I -0.7722 -0.8265 -0.6170 0.709 #&gt; D 1.0000 0.9721 0.1097 -0.482 #&gt; Vm 0.9721 1.0000 0.1122 -0.393 #&gt; Maas 0.1097 0.1122 1.0000 -0.848 #&gt; lgV0 -0.4815 -0.3931 -0.8479 1.000 A kapott értékeket a ggcorr() függvény segítségével ábrázolhatjuk is. Ha a függvény argumentumában a label = TRUE szerepel, a kapott ábrán a kiszámított értékek is láthatók (ld. 5.1. ábra). GGally::ggcorr(div_df[, 2:13], label = TRUE) Ábra 5.1: Korrelációs hotérkép Az így kapott ábránk egy korrelációs htérkép, az oszlopok tetején elhelyezked rövidítések az egyes mérszámokat jelentik, amelyekkel a beszédeket vizsgáltuk ezek képlete megtalálható a textstat lexdiv funkció oldalán. Ezek keresztmetszetében a számok az ábrázolják, hogy az egyes mérszámok eredményei milyen kapcsolatban állnak egymással. Ahogy az ábra melletti skála is jelzi a piros négyzetben lév számok pozitív korrelációt jeleznek, a kékben lévk pedig negatívat, minél halványabb egy adott négyzet színezése a korreláció mértéke annál kisebb. Ezt követen azt is megvizsgálhatjuk, hogy a korpusz szövegei mennyire könnyen olvashatóak. Ehhez a Flesch.Kincaid pontszámot használjuk, ami a szavak és a mondatok hossza alapján határozza meg a szöveg olvashatóságát. Ehhez a textstat_readability() függvényt használjuk, mely a korpuszunkat elemzi. quanteda.textstats::textstat_readability(x = corpus_mineln, measure = &quot;Flesch.Kincaid&quot;) #&gt; document Flesch.Kincaid #&gt; 1 antall_jozsef_1990 16.5 #&gt; 2 bajnai_gordon_2009 10.9 #&gt; 3 gyurcsány_ferenc_2005 13.6 #&gt; 4 horn_gyula_1994 13.8 #&gt; 5 medgyessy_peter_2002 15.8 #&gt; 6 orban_viktor_1998 13.0 #&gt; 7 orban_viktor_2018 11.4 Ezután a kiszámított értékkel kiegészítjük a korpuszt. docvars(corpus_mineln, &quot;f_k&quot;) &lt;- textstat_readability(corpus_mineln, measure = &quot;Flesch.Kincaid&quot;)[, 2] docvars(corpus_mineln) #&gt; year pm mineln partoldal f_k #&gt; 1 1990 antall_jozsef antall_jozsef jobb 16.5 #&gt; 2 2009 bajnai_gordon bajnai_gordon bal 10.9 #&gt; 3 2005 gyurcsány_ferenc gyurcsány_ferenc bal 13.6 #&gt; 4 1994 horn_gyula horn_gyula bal 13.8 #&gt; 5 2002 medgyessy_peter medgyessy_peter bal 15.8 #&gt; 6 1998 orban_viktor orban_viktor jobb 13.0 #&gt; 7 2018 orban_viktor orban_viktor jobb 11.4 Majd a ggplot2 segítségével vizualizálhatjuk az eredményt (ld. ??. ábra). Ehhez az olvashatósági pontszámmal kiegészített korpuszból egy adattáblát alakítunk ki, majd beállítjuk az ábrázolás paramétereit. Az ábra két tengelyén az év, illetve az olvashatósági pontszám szerepel, a jobb- és a baloldalt a vonal típusa különbözteti meg, az egyes dokumentumokat ponttal jelöljük, az ábrára pedig felíratjuk a miniszterelnökök neveit, valamint azt is beállítjuk, hogy az x tengely beosztása az egyes beszédek dátumához igazodjon. A theme_minimal() függvénnyel pedig azt határozzuk meg, hogy mindez fehér hátteret kapjon. Az így létrehozott ábránkat a ggplotly parancs segítségével pedig interaktívvá is tehetjük. corpus_df &lt;- docvars(corpus_mineln) mineln_df &lt;- ggplot(corpus_df, aes(year, f_k)) + geom_point(size = 2) + geom_line(aes(linetype = partoldal), size = 1) + geom_text(aes(label = mineln), color = &quot;black&quot;, nudge_y = 0.15) + scale_x_continuous(limits = c(1988, 2020)) + labs( x = NULL, y = &quot;Flesch-Kincaid index&quot;, color = NULL, linetype = NULL ) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) ggplotly(mineln_df) Ábra 5.2: Az olvashatósági index alakulása 5.4 Összehasonlítás21 A fentiekben láthattuk az eltéréseket a jobb és a bal oldali beszédeken belül, st ugyanahhoz a miniszterelnökhöz tartozó két beszéd között is. A következkben textstat_dist() és textstat_simil() függvények segítségével megvizsgáljuk, valójában mennyire hasonlítanak vagy különböznek ezek a beszédek. Mindkét függvény bemenete dmf, melybl elször egy súlyozott dfm-et készítünk, majd elvégezzük az összehasonlítást elször a jaccard-féle hasonlóság alapján. mineln_dfm %&gt;% dfm_weight(&quot;prop&quot;) %&gt;% quanteda.textstats::textstat_simil(margin = &quot;documents&quot;, method = &quot;jaccard&quot;) #&gt; textstat_simil object; method = &quot;jaccard&quot; #&gt; antall_jozsef_1990 bajnai_gordon_2009 #&gt; antall_jozsef_1990 1.0000 0.0559 #&gt; bajnai_gordon_2009 0.0559 1.0000 #&gt; gyurcsány_ferenc_2005 0.0798 0.0850 #&gt; horn_gyula_1994 0.0694 0.0592 #&gt; medgyessy_peter_2002 0.0404 0.0690 #&gt; orban_viktor_1998 0.0778 0.0626 #&gt; orban_viktor_2018 0.0362 0.0617 #&gt; gyurcsány_ferenc_2005 horn_gyula_1994 #&gt; antall_jozsef_1990 0.0798 0.0694 #&gt; bajnai_gordon_2009 0.0850 0.0592 #&gt; gyurcsány_ferenc_2005 1.0000 0.0683 #&gt; horn_gyula_1994 0.0683 1.0000 #&gt; medgyessy_peter_2002 0.0684 0.0587 #&gt; orban_viktor_1998 0.0734 0.0621 #&gt; orban_viktor_2018 0.0503 0.0494 #&gt; medgyessy_peter_2002 orban_viktor_1998 #&gt; antall_jozsef_1990 0.0404 0.0778 #&gt; bajnai_gordon_2009 0.0690 0.0626 #&gt; gyurcsány_ferenc_2005 0.0684 0.0734 #&gt; horn_gyula_1994 0.0587 0.0621 #&gt; medgyessy_peter_2002 1.0000 0.0650 #&gt; orban_viktor_1998 0.0650 1.0000 #&gt; orban_viktor_2018 0.0504 0.0583 #&gt; orban_viktor_2018 #&gt; antall_jozsef_1990 0.0362 #&gt; bajnai_gordon_2009 0.0617 #&gt; gyurcsány_ferenc_2005 0.0503 #&gt; horn_gyula_1994 0.0494 #&gt; medgyessy_peter_2002 0.0504 #&gt; orban_viktor_1998 0.0583 #&gt; orban_viktor_2018 1.0000 Majd a textstat_dist() függvény segítségével kiszámoljuk a dokumentumok egymástól való különbözségét. mineln_dfm %&gt;% quanteda.textstats::textstat_dist(margin = &quot;documents&quot;, method = &quot;euclidean&quot;) #&gt; textstat_dist object; method = &quot;euclidean&quot; #&gt; antall_jozsef_1990 bajnai_gordon_2009 #&gt; antall_jozsef_1990 0 162.8 #&gt; bajnai_gordon_2009 163 0 #&gt; gyurcsány_ferenc_2005 186 137.8 #&gt; horn_gyula_1994 164 80.1 #&gt; medgyessy_peter_2002 160 68.1 #&gt; orban_viktor_1998 139 84.7 #&gt; orban_viktor_2018 167 67.3 #&gt; gyurcsány_ferenc_2005 horn_gyula_1994 #&gt; antall_jozsef_1990 186 163.6 #&gt; bajnai_gordon_2009 138 80.1 #&gt; gyurcsány_ferenc_2005 0 147.0 #&gt; horn_gyula_1994 147 0 #&gt; medgyessy_peter_2002 143 75.9 #&gt; orban_viktor_1998 147 89.6 #&gt; orban_viktor_2018 148 74.8 #&gt; medgyessy_peter_2002 orban_viktor_1998 #&gt; antall_jozsef_1990 160.2 139.2 #&gt; bajnai_gordon_2009 68.1 84.7 #&gt; gyurcsány_ferenc_2005 142.6 146.9 #&gt; horn_gyula_1994 75.9 89.6 #&gt; medgyessy_peter_2002 0 77.5 #&gt; orban_viktor_1998 77.5 0 #&gt; orban_viktor_2018 60.7 83.6 #&gt; orban_viktor_2018 #&gt; antall_jozsef_1990 167.4 #&gt; bajnai_gordon_2009 67.3 #&gt; gyurcsány_ferenc_2005 147.9 #&gt; horn_gyula_1994 74.8 #&gt; medgyessy_peter_2002 60.7 #&gt; orban_viktor_1998 83.6 #&gt; orban_viktor_2018 0 Ezután vizualizálhatjuk is a dokumentumok egymástól való távolságát egy olyan dendogram22 segítségével, amely megmutatja nekünk a lehetséges dokumentumpárokat (ld. 5.3. ábra). dist &lt;- mineln_dfm %&gt;% textstat_dist(margin = &quot;documents&quot;, method = &quot;euclidean&quot;) hierarchikus_klaszter &lt;- hclust(as.dist(dist)) ggdendro::ggdendrogram(hierarchikus_klaszter) Ábra 5.3: A dokumentumok csoportosítása a távolságuk alapján A textstat_simil funkció segítségével azt is meg tudjuk vizsgálni, hogy egy adott kifejezés milyen egyéb kifejezésekkel korrelál. mineln_dfm %&gt;% textstat_simil(y = mineln_dfm[, c(&quot;kormány&quot;)], margin = &quot;features&quot;, method = &quot;correlation&quot;) %&gt;% head(n = 10) #&gt; kormány #&gt; elnök -0.125 #&gt; tisztelt -0.508 #&gt; országgyulés 0.804 #&gt; hölgyeim -0.298 #&gt; uraim -0.298 #&gt; honfitársaim 0.926 #&gt; ünnepi 0.959 #&gt; pillanatban 0.959 #&gt; állok 0.683 #&gt; magyar 0.861 Arra is van lehetségünk, hogy a két alkorpuszt hasonlítsuk össze egymással. Ehhez a textstat_keyness() függvényt használjuk, melynek a bemenete a dfm. A függvény argumentumában a target = után kell megadni, hogy mely alkorpusz a viszonyítási alap. Az összehasonlítás eredményét a textplot_keyness() függvény segítségével ábrázolhatjuk, ami megjeleníti a két alkorpusz leggyakoribb kifejezéseit (ld. 5.4. ábra). dfm_keyness &lt;- corpus_mineln %&gt;% tokens(remove_punct = TRUE) %&gt;% tokens_remove(pattern = custom_stopwords) %&gt;% dfm() %&gt;% quanteda::dfm_group(partoldal) result_keyness &lt;- quanteda.textstats::textstat_keyness(dfm_keyness, target = &quot;jobb&quot;) quanteda.textplots::textplot_keyness(result_keyness, color = c(&quot;#484848&quot;, &quot;#D0D0D0&quot;)) + xlim(c(-65, 65)) + theme(legend.position = c(0.9,0.1)) Ábra 5.4: A korpuszok legfontosabb kifejezései Ha az egyes miniszterelnökök beszédeinek leggyakoribb kifejezéseit szeretnénk összehasonlítani, azt a textstat_frequency() függvény segítségével tehetjük meg, melynek bemenete a megtisztított és súlyozott dfm. Az összehasonlítás eredményét pedig a ggplot2 segítségével ábrázolhatjuk is (ld. 5.5. ábra). Majd ábránkat a plotly segítségével interaktívvá tehetjük. dfm_weighted &lt;- corpus_mineln %&gt;% tokens( remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE ) %&gt;% tokens_tolower() %&gt;% tokens_wordstem(language = &quot;hungarian&quot;) %&gt;% tokens_remove(pattern = custom_stopwords) %&gt;% dfm() %&gt;% dfm_weight(scheme = &quot;prop&quot;) freq_weight &lt;- textstat_frequency(dfm_weighted, n = 5, groups = mineln) data_df &lt;- ggplot(data = freq_weight, aes(x = nrow(freq_weight):1, y = frequency)) + geom_point() + facet_wrap(~ group, scales = &quot;free&quot;, ncol = 1) + theme(panel.spacing = unit(1, &quot;lines&quot;))+ coord_flip() + scale_x_continuous( breaks = nrow(freq_weight):1, labels = freq_weight$feature ) + labs( x = NULL, y = &quot;Relatív szófrekvencia&quot; ) ggplotly(data_df, height = 1000, tooltip = &quot;frequency&quot;) Ábra 5.5: Leggyakoribb kifejezések a miniszterelnöki beszédekben Mivel a szövegösszehasonlítás egy komplex kutatási feladat, a témával bbben is foglalkozunk a Szövegösszhasonlítás fejezetben. 5.5 A kulcsszavak kontextusa Arra is lehetségünk van, hogy egyes kulcszavakat a korpuszon belül szövegkörnyezetükben vizsgáljunk meg. Ehhez a kwic() függvényt használjuk, az argumentumok között a pattern = kifejezés után megadva azt a szót, amelyet vizsgálni szeretnénk, a window = után pedig megadhatjuk, hogy az adott szó hány szavas környezetére vagyunk kíváncsiak. corpus_mineln %&gt;% tokens() %&gt;% quanteda::kwic( pattern = &quot;válság*&quot;, valuetype = &quot;glob&quot;, window = 3, case_insensitive = TRUE ) %&gt;% head(5) #&gt; Keyword-in-context with 5 matches. #&gt; [antall_jozsef_1990, 1167] Átfogó és mély | #&gt; [antall_jozsef_1990, 1283] kell hárítanunk a | #&gt; [antall_jozsef_1990, 2772] és a lakásgazdálkodás | #&gt; [antall_jozsef_1990, 5226] gazdaság egészét juttatta | #&gt; [antall_jozsef_1990, 5286] gazdaság reménytelenül eladósodott | #&gt; #&gt; válságba | süllyedtünk a nyolcvanas #&gt; válságot | , de csakis #&gt; válságos | helyzetbe került. #&gt; válságba | , és amellyel #&gt; válsággócai | ellen. A A beszédeket a Hungarian Comparative Agendas Project miniszterelnöki beszéd korpuszából válogattuk: https://cap.tk.hu/vegrehajto A lubridate használatának részletes leírása megtalálható az alábbi linken: https://rawgit.com/rstudio/cheatsheets/master/lubridate.pdf A lambda leírása megtalálható itt: https://quanteda.io/reference/textstat_collocations.html A különböz indexek leírása megtalálható az alábbi linken: https://quanteda.io/reference/textstat_lexdiv.html (Schütze, Manning, and Raghavan 2008) Olyan ábra, amely hasonlóságaik vagy különbségeik alapján csoportosított objektumok összefüggéseit mutatja meg. "],["sentiment.html", "6 Szótárak és érzelemelemzés 6.1 Fogalmi alapok 6.2 Szótárak az R-ben 6.3 A Magyar Nemzet elemzése 6.4 MNB sajtóközlemények", " 6 Szótárak és érzelemelemzés 6.1 Fogalmi alapok A szentiment- vagy vélemény-, illetve érzelemelemzés a számítógépes nyelvészet részterülete, melynek célja az egyes szövegek tartalmából kinyerni azokat az információkat, amelyek értékelést fejeznek ki.23 A véleményelemzés a szövegeket három szinten osztályozza. A legáltalánosabb a dokumentumszint osztályozás, amikor egy hosszabb szövegegység egészét vizsgáljuk, míg a mondatszint osztályozásnál a vizsgálat alapegysége a mondat. A legrészletesebb adatokat akkor nyerjük, amikor az elemzést target-szinten végezzük, azaz meghatározzuk azt is, hogy egy-egy érzelem a szövegen belül mire vonatkozik. Mindhárom szinten azonos a feladat: egyrészt meg kell állapítani, hogy az adott egységben van-e értékelés, vélemény vagy érzelem, és ha igen, akkor pedig meg kell határozni, hogy milyen azok érzelmi tartalma. A pozitív-negatív-semleges skálán mozgó szentimentelemzés mellett az elmúlt két évtizedben jelents lépések történtek a szövegek emóciótartalmának automatikus vizsgálatára is. A módszer hasonló a szentimentelemzéshez, tartalmilag azonban más skálán mozog. Az emócióelemzés esetén ugyanis nem csak azt kell meghatározni, hogy egy kifejezés pozitív vagy negatív töltettel rendelkezik, hanem azt is, hogy milyen érzelmet (öröm, bánat, undor stb.) hordoz. A szótár alapú szentiment- vagy emócióelemzés alapja az az egyszer ötlet, hogy ha tudjuk, hogy egyes szavak milyen érzelmeket, érzéseket hordoznak, akkor ezeket a szavakat egy szövegben megszámolva képet kaphatunk az adott dokumentum érzelmi tartalmáról. Mivel a szótár alapú elemzés az adott kategórián belüli kulcsszavak gyakoriságán alapul, ezért van, aki nem tekinti statisztikai elemzésnek (lásd például Young and Soroka (2012)). A tágabb kvantitatív szövegelemzési kontextusban az osztályozáson (classification) belül a felügyelt módszerekhez hasonlóan itt is ismert kategóriákkal dolgozunk, azaz elre meghatározzuk, hogy egy-egy adott szó pozitív vagy negatív térték, vagy továbbmenve, milyen érzelmet hordoz, csak egyszerbb módszertannal (Grimmer and Stewart 2013). A kulcsszavakra építés miatt a módszer a kvalitatív és a kvantitatív kutatási vonalak találkozásának is tekinthet, hiszen egy-egy szónak az érzelmi töltete nem mindig ítélhet meg objektíven. Mint minden módszer esetében, itt is kiemelten fontos ellenrni, hogy a használt szótár kategóriák és kulcsszavak fedik-e a valóságot. Más szavakkal: validálás, validálás, validálás. A módszer elnyei: Tökéletesen megbízható: a számításoknak nincs probabilisztikus (azaz valószínségre épül) eleme, mint például a Support Vector alapú osztályozásnak, illetve az emberi szövegkódolásnál elforduló problémákat is elkerüljük (például azt, hogy két kódoló, vagy ugyanazon kódoló két különböz idpontban nem azonosan értékeli ugyanazt a kifejezést). Általa képesek vagyunk mérni a szöveg látens dimenzióit. Széles körben alkalmazható, egyszeren számolható. A politikatudományon és a számítógépes nyelvészeten belül nagyon sok kész szótár elérhet, amelyek különböz módszerekkel készültek és különböz területet fednek le (például populizmus, pártprogramok policy tartalma, érzelmek, gazdasági tartalom). Relatíve könnyen adaptálható egyik nyelvi környezetbl a másikba, bár szótárfordítások esetén külön hangsúlyt kell fektetni a validálásra.24 A módszer lehetséges hátrányai: A szótár hatékonysága és validitása azon múlik, hogy mennyire egyezik a szótár és a vizsgálni kívánt dokumentum területe. Nem mindegy például, hogy a szótárunkkal tzsdei jelentések alapján a gazdasági bizonytalanságot vagy nézk filmekre adott értékeléseit szeretnénk-e vizsgálni. Léteznek általános szentimentszótárak, ezek hatékonysága azonban általában alulmúlja a terület-specifikus szótárakét. A terület-specifikus szótár építése kvalitatív folyamat, éppen ezért id- és emberi erforrás igényes. A szózsák alapú elemzéseknél a kontextus elvész. Gondoljunk például a tagadásra: a nem vagyok boldog kifejezés esetén egy általános szentiment szótár a tagadás miatt félreosztályozná a mondat érzelmi töltését, hiszen a boldog szó önmagában a pozitív kategóriába tartozik. Természetesen az automatikus tagadás kezelésére is vannak lehetségek, de a kérdés komplexitása miatt ezek bemutatásától most eltekintünk. A legnagyobb méret általános szentimentszótár az angol nyelv SentiWordNet (SWN), ami kb. 150 000 szót tartalmaz, amelyek mindegyike a három szentimentérték  pozitív, negatív, semleges  közül kapott egyet.25(Baccianella, Esuli, and Sebastiani 2010) Az R-ben végzett szentimentelemzés során az angol nyelv szövegekhez több beépített általános szentimentszótár is a rendelkezésünkre áll.26 A teljesség igénye nélkül említhetjük az AFINN,27 a bing28 és az nrc29 szótárakat. Az elemzés sikere több faktortól is függ. Fontos, hogy a korpuszban lév dokumentumokat körültekinten tisztítsuk meg az elemzés elején (lásd a Korpuszépítés és elkészítés fejezetet). A következ lépésben meg kell bizonyosodnunk arról, hogy a kiválasztott szentiment szótár alkalmazható a korpuszunkra. Amennyiben nem találunk alkalmas szótárt, akkor a saját szótár validálására kell figyelni. A negyedik fejezetben leírtak itt is érvényesek, a dokumentum-kifejezés mátrixot érdemes valamilyen módon súlyozni. 6.2 Szótárak az R-ben A szótár alapú elemzéshez a quanteda csomagot fogjuk használni, illetve a 3. fejezetben már megismert readr, stringr, dplyr tidyverse csomagokat.30 library(stringr) library(dplyr) library(tidyr) library(ggplot2) library(quanteda) library(HunMineR) library(plotly) Mieltt két esettanulmányt bemutatnánk, vizsgáljuk meg, hogyan néz ki egy szentimentszótár az R-ben. A szótárt kézzel úgy tudjuk elkészíteni, hogy egy listán belül létrehozzuk karaktervektorként a kategóriákat és a kulcsszavakat, és ezt a listát a quanteda dictionary függvényével eltároljuk. szentiment_szotar &lt;- dictionary( list( pozitiv = c(&quot;jó&quot;, &quot;boldog&quot;, &quot;öröm&quot;), negativ = c(&quot;rossz&quot;, &quot;szomorú&quot;, &quot;lehangoló&quot;) ) ) szentiment_szotar #&gt; Dictionary object with 2 key entries. #&gt; - [pozitiv]: #&gt; - jó, boldog, öröm #&gt; - [negativ]: #&gt; - rossz, szomorú, lehangoló A quanteda, quanteda.corpora és tidytext R csomagok több széles körben használt szentiment szótárat tartalmaznak, így nem kell kézzel replikálni minden egyes szótárat, amit használni szeretnénk. A szentiment elemzési munkafolyamat, amit ebben a részfejezetben bemutatunk, a következ lépésekbl áll: dokumentumok betöltése, szöveg elkészítése, a korpusz létrehozása, dokumentum-kifejezés mátrix létrehozása, szótár betöltése, a dokumentum-kifejezés mátrix szrése a szótárban lév kulcsszavakkal, az eredmény vizualizálása, további felhasználása. A fejezetben két különböz korpuszt fogunk elemezni: a 2006-os Magyar Nemzet címlapjainak egy 252 cikkbl álló mintáját vizsgáljuk egy magyar szentiment szótárral.31 A második korpusz a Magyar Nemzeti Bank angol nyelv sajtóközleményeibl áll, amin egy széles körben használt gazdasági szótár használatát mutatjuk be.32 6.3 A Magyar Nemzet elemzése mn_minta &lt;- HunMineR::data_magyar_nemzet_small A HunMineR csomag segítségével beolvassuk a Magyar Nemzet adatbázis egy kisebb részét, ami az esetünkben a 2006-os címlapokon szerepl híreket jelenti. A summary() parancs, ahogy a neve is mutatja, gyors áttekintést nyújt a betöltött adatbázisról. Látjuk, hogy 2834 sorból (megfigyelés) és 3 oszlopból (változó) áll. Els ránézésre látszik, hogy a text változónk tartalmazza a szövegeket, és hogy azok tisztításra szorulnak. glimpse(mn_minta) #&gt; Rows: 2,834 #&gt; Columns: 3 #&gt; $ doc_id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, ~ #&gt; $ text &lt;chr&gt; &quot;Hat fovárosi képviselo öt percnél is kevesebbet be~ #&gt; $ doc_date &lt;date&gt; 2006-01-02, 2006-01-02, 2006-01-02, 2006-01-02, 20~ A glimpse függvény segítségével belepillanthatunk a használt korpuszba és láthatjuk, hogy az 3 oszlopból áll a dokumentum azonosítójából, amely csak egy sorszám, a dokumentum szövegébl, és a dokumentumhoz tartozó azonosítóból. Az els szöveget megnézve látjuk, hogy a standard elkészítési lépések mellett a sortörést (\\n) is ki kell törölnünk. mn_minta$text[1] #&gt; [1] &quot;Hat fovárosi képviselo öt percnél is kevesebbet beszélt egy év alatt a közgyulésben.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n&quot; Habár a quanteda is lehetséget ad néhány elkészít lépésre, érdemes ezt olyan céleszközzel tenni, ami nagyobb rugalmasságot ad a kezünkbe. Mi erre a célra a stringr csomagot használjuk. Els lépésben kitöröljük a sortöréseket (\\n), a központozást, a számokat és kisbetsítünk minden szót. Elfordulhat, hogy (számunkra nehezen látható) extra szóközök maradnak a szövegben. Ezeket az str_squish()függvénnyel tüntetjük el. A szöveg eleji és végi extra szóközöket (leading vagy trailing white space) az str_trim() függvény vágja le. mn_tiszta &lt;- mn_minta %&gt;% mutate( text = stringr::str_remove_all(string = text, pattern = &quot;\\n&quot;), text = stringr::str_remove_all(string = text, pattern = &quot;[:punct:]&quot;), text = stringr::str_remove_all(string = text, pattern = &quot;[:digit:]&quot;), text = stringr::str_to_lower(text), text = stringr::str_trim(text), text = stringr::str_squish(text) ) A szöveg sokkal jobban néz ki, habár észrevehetjük, hogy maradhattak benne problémás részek, fleg a sortörés miatt, ami sajnos hol egyes szavak közepén van (a jobbik eset), vagy pedig pont szóhatáron, ez esetben a két szó sajnos összevonódik. Az egyszerség kedvéért feltételezzük, hogy ez kellen ritkán fordul el ahhoz, hogy ne befolyásolja az elemzésünk eredményét. mn_tiszta$text[1] #&gt; [1] &quot;hat fovárosi képviselo öt percnél is kevesebbet beszélt egy év alatt a közgyulésben&quot; Miután kész a tisztá(bb) szövegünk, korpuszt hozunk létre a quanteda corpus() függvényével. A korpusz objektum a szöveg mellett egyéb dokumentum meta adatokat is tud tárolni (dátum, író, hely, stb.) Ezeket mi is hozzáadhatjuk (erre majd látunk példát), illetve amikor létrehozzuk a korpuszt a data frame-ünkbl, automatikusan metaadatokként tárolódnak a változóink. Jelen esetben az egyetlen dokumentum változónk a szöveg mellett a dátum lesz. A korpusz dokumentum változóihoz a docvars() függvény segítségével tudunk hozzáférni. mn_corpus &lt;- corpus(mn_tiszta) head(docvars(mn_corpus), 5) #&gt; doc_date #&gt; 1 2006-01-02 #&gt; 2 2006-01-02 #&gt; 3 2006-01-02 #&gt; 4 2006-01-02 #&gt; 5 2006-01-02 A következ lépés a dokumentum-kifejezés mátrix létrehozása a dfm() függvénnyel. Elször tokenekre bontjuk a szövegeket a tokens() paranccsal, és aztán ezt a tokenizált szózsákot kapja meg a dfm inputnak. A sor a végén a létrehozott mátrixunkat TF-IDF módszerrel súlyozzuk a dfm_tfidf() függvény használatával. mn_dfm &lt;- mn_corpus %&gt;% tokens(what = &quot;word&quot;) %&gt;% dfm() %&gt;% dfm_tfidf() A cikkek szentimentjét egy magyar szótárral fogjuk becsülni, amit a Társadalomtudományi Kutatóközpont kutatói a Mesterséges Intelligencia Nemzeti Laboratórium projekt keretében készítettek.33 Két dimenziót tarlamaz (pozitív és negatív), 2614 pozitív és 2654 negatív kulcsszóval. Ez nem számít kirívóan nagynak a szótárak között, mivel az adott kategóriák minél teljesebb lefedése a cél. poltext_szotar &lt;- HunMineR::dictionary_poltext poltext_szotar #&gt; Dictionary object with 2 key entries. #&gt; - [positive]: #&gt; - abszolút, ad, adaptív, adekvát, adócsökkentés, adókedvezmény, adomány, adományoz, adóreform, adottság, adottságú, áfacsökkentés, agilis, agytröszt, áhított, ajándék, ajándékoz, ajánl, ajánlott, akadálytalan [ ... and 2,279 more ] #&gt; - [negative]: #&gt; - aberrált, abnormális, abnormalitás, abszurd, abszurditás, ádáz, adócsalás, adócsaló, adós, adósság, áfacsalás, áfacsaló, affér, aggasztó, aggodalom, aggódik, aggódás, agresszió, agresszíven, agresszivitás [ ... and 2,568 more ] Az egyes dokumentumok szentimentjét a dfm_lookup() becsüli, ahol az elz lépésben létrehozott súlyozott dfm az input és a magyar szentimentszótár a dictionary. Egy gyors pillantás az eredményre és látjuk hogy minden dokumentumhoz készült egy pozitív és egy negatív érték. A TF-IDF súlyozás miatt nem látunk egész számokat (a súlyozás nélkül a sima szófrekvenciát kapnánk). mn_szentiment &lt;- quanteda::dfm_lookup(mn_dfm, dictionary = poltext_szotar) head(mn_szentiment, 5) #&gt; Document-feature matrix of: 5 documents, 2 features (40.00% sparse) and 1 docvar. #&gt; features #&gt; docs positive negative #&gt; 1 0 0 #&gt; 2 0.838 12.50 #&gt; 3 0 0 #&gt; 4 21.104 6.45 #&gt; 5 11.036 8.13 Ahhoz, hogy fel tudjuk használni a kapott eredményt, érdemes dokumentumváltozóként eltárolni a korpuszban. Ezt a fent már használt docvars() függvény segítségével tudjuk megtenni, ahol a második argumentumként az új változó nevét adjuk meg. docvars(mn_corpus, &quot;pos&quot;) &lt;- as.numeric(mn_szentiment[, 1]) docvars(mn_corpus, &quot;neg&quot;) &lt;- as.numeric(mn_szentiment[, 2]) head(docvars(mn_corpus), 5) #&gt; doc_date pos neg #&gt; 1 2006-01-02 0.000 0.00 #&gt; 2 2006-01-02 0.838 12.50 #&gt; 3 2006-01-02 0.000 0.00 #&gt; 4 2006-01-02 21.104 6.45 #&gt; 5 2006-01-02 11.036 8.13 Végül a kapott korpuszt a kiszámolt szentimentértékekkel a quanteda-ban lév convert() függvénnyel adattáblává alakítjuk. Aconvert() függvény dokumentációját érdemes elolvasni, mert ennek segítségével tudjuk a quanteda-ban elkészült objektumainkat átalakítani úgy, hogy azt más csomagok is tudják használni. mn_df &lt;- quanteda::convert(mn_corpus, to = &quot;data.frame&quot;) Mieltt vizualizálnánk az eredményt érdemes a napi szintre aggregálni a szentimentértéket és egy nettó értéket kalkulálni (ld. 6.1. ábra).34 mn_df &lt;- mn_df %&gt;% group_by(doc_date) %&gt;% summarise( daily_pos = sum(pos), daily_neg = sum(neg), net_daily = daily_pos - daily_neg ) Az így kapott plot y tengelyén az adott cikkek idpontját láthatjuk, míg az x tengelyén a szentiment értékeiket. Ebben több kiugrást is tapasztalhatunk. Természetesen messzemen következtetéseket egy ilyen kis korpusz alapján nem vonhatunk le, de a kiugrásokhoz tartozó cikkek kvalitatív vizsgálatával megállapíthatjuk, hogy az áprilisi kiugrást a választásokhoz kötd cikkek pozitív hangulata, míg az októberi negatív kilengést az öszödi beszéd nyilvánosságra kerüléséhez köthet cikkek negatív szentimentje okozza. mncim_df &lt;- ggplot(mn_df, aes(doc_date, net_daily)) + geom_line() + labs( y = &quot;Szentiment&quot;, x = NULL, caption = &quot;Adatforrás: https://cap.tk.hu/&quot; ) ggplotly(mncim_df) Ábra 6.1: Magyar Nemzet címlap szentimentje 6.4 MNB sajtóközlemények A második esettanulmányban a kontextuális szótárelemzést mutatjuk be egy angol nyelv korpusz és specializált szótár segítségével. A korpusz az MNB kamatdöntéseit kísér nemzetközi sajtóközleményei, a szótár pedig a Loughran and McDonald (2011) pénzügyi szentimentszótár.35 penzugy_szentiment &lt;- HunMineR::dictionary_LoughranMcDonald penzugy_szentiment #&gt; Dictionary object with 9 key entries. #&gt; - [NEGATIVE]: #&gt; - abandon, abandoned, abandoning, abandonment, abandonments, abandons, abdicated, abdicates, abdicating, abdication, abdications, aberrant, aberration, aberrational, aberrations, abetting, abnormal, abnormalities, abnormality, abnormally [ ... and 2,335 more ] #&gt; - [POSITIVE]: #&gt; - able, abundance, abundant, acclaimed, accomplish, accomplished, accomplishes, accomplishing, accomplishment, accomplishments, achieve, achieved, achievement, achievements, achieves, achieving, adequately, advancement, advancements, advances [ ... and 334 more ] #&gt; - [UNCERTAINTY]: #&gt; - abeyance, abeyances, almost, alteration, alterations, ambiguities, ambiguity, ambiguous, anomalies, anomalous, anomalously, anomaly, anticipate, anticipated, anticipates, anticipating, anticipation, anticipations, apparent, apparently [ ... and 277 more ] #&gt; - [LITIGIOUS]: #&gt; - abovementioned, abrogate, abrogated, abrogates, abrogating, abrogation, abrogations, absolve, absolved, absolves, absolving, accession, accessions, acquirees, acquirors, acquit, acquits, acquittal, acquittals, acquittance [ ... and 883 more ] #&gt; - [CONSTRAINING]: #&gt; - abide, abiding, bound, bounded, commit, commitment, commitments, commits, committed, committing, compel, compelled, compelling, compels, comply, compulsion, compulsory, confine, confined, confinement [ ... and 164 more ] #&gt; - [SUPERFLUOUS]: #&gt; - aegis, amorphous, anticipatory, appertaining, assimilate, assimilating, assimilation, bifurcated, bifurcation, cessions, cognizable, concomitant, correlative, deconsolidation, delineation, demonstrable, demonstrably, derecognized, derecognizes, derivatively [ ... and 36 more ] #&gt; [ reached max_nkey ... 3 more keys ] A szentimentszótár 9 kategóriából áll. A legtöbb kulcsszó a negatív dimenzióhoz van (2355). A munkamenet hasonló az elz példához: adat betöltés, szövegtisztítás, korpusz létrehozás, tokenizálás, kulcs kontextuális tokenek szrése, dfm elállítás és szentiment számítás, az eredmény vizualizálása, további felhasználása. mnb_pr &lt;- HunMineR::data_mnb_pr Adatbázisunk 180 megfigyelésbl és 4 változóból áll. Az egyetlen lényeges dokumentum metaadat itt is a szövegek megjelenési ideje, de a glimpse függvénnyel itt is ellenrizhetjük hogyan néz ki a korpusz felépítése és milyen metaadatokat tartalmaz pontosan. glimpse(mnb_pr) #&gt; Rows: 180 #&gt; Columns: 4 #&gt; $ date &lt;date&gt; 2005-01-24, 2005-02-21, 2005-03-29, 2005-04-25, 2005-0~ #&gt; $ text &lt;chr&gt; &quot;At its meeting on January the Monetary Council conside~ #&gt; $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, ~ #&gt; $ year &lt;dbl&gt; 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2~ Ez alapján pedig láthatjuk, hogy a korpusz a tényleges szövegek mellett tartalmaz még id sorszámot, pontos dátumot és évet is. A szövegeket ugyanazokkal a standard eszközökkel kezeljük, mint a Magyar Nemzet esetében. Érdemes minden esetben ellenrizni, hogy az R-kód, amit használunk, tényleg azt csinálja-e, amit szeretnénk. Ez hatványozottan igaz abban az esetben, amikor szövegekkel és reguláris kifejezésekkel dolgozunk. mnb_tiszta &lt;- mnb_pr %&gt;% mutate( text = str_remove_all(string = text, pattern = &quot;[:cntrl:]&quot;), text = str_remove_all(string = text, pattern = &quot;[:punct:]&quot;), text = str_remove_all(string = text, pattern = &quot;[:digit:]&quot;), text = str_to_lower(text), text = str_trim(text), text = str_squish(text) ) Miután rendelkezésre állnak a tiszta dokumentumaink, egy karaktervektorba gyjtjük azokat a kulcsszavakat, amelyek környékén szeretnénk megfigyelni a szentiment alakulását. A példa kedvéért mi az unemp*, growth, gdp, inflation* szótöveket és szavakat választottuk. A tokens_keep() megtartja a kulcsszavainkat és egy általunk megadott +/- n tokenes környezetüket (jelen esetben 10). A szentimentelemzést pedig már ezen a jóval kisebb mátrixon fogjuk lefuttatni. A phrase() segítségével több szóból álló kifejezéséket is vizsgálhatunk. Ilyen szókapcsolat például az Európai Unió is, ahol lényeges, hogy egyben kezeljük a két szót. mnb_corpus &lt;- corpus(mnb_tiszta) gazdasag &lt;- c(&quot;unemp*&quot;, &quot;growth&quot;, &quot;gdp&quot;, &quot;inflation*&quot;, &quot;inflation expectation*&quot;) mnb_token &lt;- tokens(mnb_corpus) %&gt;% tokens_keep(pattern = phrase(gazdasag), window = 10) A szentimentet most is egy súlyozott dfm-bl számoljuk. A kész eredményt hozzáadjuk a korpuszhoz, majd adattáblát hozunk létre belle. A 9 kategóriából 5-öt használunk csak, amelyeknek jegybanki környezetben értelmezhet tartalma van. mnb_szentiment &lt;- tokens_lookup(mnb_token, dictionary = penzugy_szentiment) %&gt;% dfm() %&gt;% dfm_tfidf() docvars(mnb_corpus, &quot;negative&quot;) &lt;- as.numeric(mnb_szentiment[, &quot;negative&quot;]) docvars(mnb_corpus, &quot;positive&quot;) &lt;- as.numeric(mnb_szentiment[, &quot;positive&quot;]) docvars(mnb_corpus, &quot;uncertainty&quot;) &lt;- as.numeric(mnb_szentiment[, &quot;uncertainty&quot;]) docvars(mnb_corpus, &quot;constraining&quot;) &lt;- as.numeric(mnb_szentiment[, &quot;constraining&quot;]) docvars(mnb_corpus, &quot;superfluous&quot;) &lt;- as.numeric(mnb_szentiment[, &quot;superfluous&quot;]) mnb_df &lt;- convert(mnb_corpus, to = &quot;data.frame&quot;) A célunk, hogy szentiment kategóriánkénti bontásban mutassuk be az elemzésünk eredményét, de eltte egy kicsit alakítani kell az adattáblán, hogy a korábban már tárgyalt tidy formára hozzuk. A különböz szentiment értékeket tartalmazó oszlopokat fogjuk átrendezni úgy, hogy kreálunk egy sent_type változót, ahol a kategória nevet fogjuk eltárolni és egy sent_score változót, ahol a szentiment értéket. Ehhez a tidyr-ben található pivot_longer() föggvényt használjuk. mnb_df &lt;- mnb_df %&gt;% tidyr::pivot_longer( cols = negative:superfluous, names_to = &quot;sent_type&quot;, values_to = &quot;sent_score&quot; ) Az átalakítás után már könnyedén tudjuk kategóriákra bontva megjeleníteni az MNB közlemények különböz látens dimenzióit. Fontos emlékezni arra, hogy ez az eredmény a kulcsszavaink +/- 10 tokenes környezetében lév szavak szentimentjét méri. Az így kapott ábránk a három alkalmazott szentiment kategória idbeli elfordulását mutatja be. Ami érdekes eredmény, hogy a felesleges töltelék (superfluous) szövegek szinte soha nem fordulnak el a kulcsszavaink körül. A többi érték is nagyjából megfelel a várakozásainknak, habár a 2008-as gazdasági válság nem tnik kiugró pontnak. Azonban a 2010 utáni európai válság már láthatóan megjelenik az idsorainkban (ld. 6.2. ábra). Az általunk használt szótár alapveten az Egyesült Államokban a tzsdén keresked cégek publikus beszámolóiból készült, így elképzelhet, hogy egyes jegybanki környezetben sokat használt kifejezések nincsenek benne. A kapott eredmények validálása ezért is nagyon fontos, illetve érdemes azzal is tisztában lenni, hogy a szótáras módszer nem tökéletes (ahogy az emberi vagy más gépi kódolás sem). mnsent_df &lt;- ggplot(mnb_df, aes(date, sent_score)) + geom_line() + labs( y = NULL, x = NULL ) + facet_wrap(~sent_type, ncol = 1)+ theme(panel.spacing = unit(2, &quot;lines&quot;)) ggplotly(mnsent_df) Ábra 6.2: Magyar Nemzeti Bank közleményeinek szentimentje Bvebben lásd például: (Liu 2010) A lehetséges, területspecifikus szótáralkotási módszerekrl részletesebben ezekben a tanulmányokban lehet olvasni: Laver and Garry (2000); Young and Soroka (2012); Loughran and McDonald (2011); Máté, Sebk, and Barczikay (2021) A szótár és dokumentációja elérhet az alábbi linken: https://github.com/aesuli/SentiWordNet A quanteda.dictionaries csomag leírása és a benne található szótárak az alábbi github linken érhetek el: https://github.com/kbenoit/quanteda.dictionaries A szótár és dokumentációja elérhet itt: http://www2.imm.dtu.dk/pubdb/pubs/6010-full.html A szótár és dokumentációja elérhet itt: https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html A szótár és dokumentációja elérhet itt: http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm A szentimentelemzéshez gyakran használt csomag még a tidytext. A szerzk online is szabadon elérhet könyvük Silge and Robinson (2017) 2. fejezetében részletesen is bemutatják a tidytext munkafolyamatot: (https://www.tidytextmining.com/sentiment.html). A korpusz a Hungarian Compartive Agendas Project keretében készült és regisztáció után, kutatási célra elérhet az alábbi linken: https://cap.tk.hu/a-media-es-a-kozvelemeny-napirendje. A korpusz, a szótár és az elemzés teljes dokumentációja elérhet az alábbi github linken: https://github.com/poltextlab/central_bank_communication, a teljes elemzés (Máté, Sebk, and Barczikay 2021) elérhet: https://doi.org/10.1371/journal.pone.0245515 ELKH TK MILAB: https://milab.tk.hu/hu A szótár és a hozzátartozó dokumentáció elérhet az alábbi github oldalon: https://github.com/poltextlab/sentiment_hun A csoportosított adatokkal való munka bvebb bemutatását lásd a Függelékben. A témával részletesen foglalkozó tanulmányban egy saját monetáris szentimentszótárat mutatunk be: Az implementáció és a hozzá tartozó R forráskód nyilvános: https://doi.org/10.6084/m9.figshare.13526156.v1 "],["lda_ch.html", "7 Felügyelet nélküli tanulás - Topikmodellezés 7.1 Fogalmi alapok 7.2 K-közép klaszterezés 7.3 LDA topikmodellek36 7.4 Strukturális topikmodellek", " 7 Felügyelet nélküli tanulás - Topikmodellezés 7.1 Fogalmi alapok A felügyelet nélküli tanulási során az alkalmazott algoritmus a dokumentum tulajdonságait és a modell becsléseit felhasználva végez csoportosítást, azaz hoz létre különböz kategóriákat, melyekhez késbb hozzárendeli a szöveget. Jelen fejezetben a felügyelet nélküli módszerek közül a topikmodellezést tárgyaljuk részletesen, majd a következkben bemutatjuk a szintén ide sorolható szóbeágyazást és a szövegskálázás wordfish módszerét is. 7.2 K-közép klaszterezés A klaszterezés egy adathalmaz pontjainak, rekordjainak hasonlóság alapján való csoportosítása, ami szinte minden nagyméret adathalmaz leíró modellezésére alkalmas. A klaszterezés során az adatpontokat diszjunkt halmazokba, azaz klaszterekbe soroljuk, hogy az elemeknek egy olyan partíciója jöjjön létre, amelyben a közös csoportokba kerül elempárok lényegesen jobban hasonlítanak egymáshoz, mint azok a pontpárok, melyek két különböz csoportba sorolódtak. Klaszterezés során a megfelel csoportok kialakítása nem egyértelm feladat, mivel a különböz adatok eltér jelentése és felhasználása miatt adathalmazonként más szempontokat kell figyelembe vennünk. Egy klaszterezési feladat megoldásához ismernünk kell a különböz algoritmusok alapvet tulajdonságait és mindig szükség van az eredményként kapott klaszterezés kiértékelésére. Mivel egy klaszterezés az adatpontok hasonlóságából indul ki, ezért az eljárás során az els fontos lépés az adatpontok páronkénti hasonlóságát a lehet legjobban megragadó hasonlósági függvény kiválasztása (Tan, Steinbach, and Kumar 2011). Számos klaszterezési eljárás létezik, melyek között az egyik leggyakoribb különbségtétel, hogy a klaszterek egymásba ágyazottak vagy sem. Ez alapján beszélhetünk hierarchikus és felosztó klaszterezésrl. A hierarchikus klaszterezés egymásba ágyazott klaszterek egy fába rendezett halmaza, azaz ahol a klaszterek alklaszterekkel rendelkeznek. A fa minden csúcsa (klasztere), a levélcsúcsokat kivéve, a gyermekei (alklaszterei) uniója, és a fa gyökere az összes objektumot tartalmazó klaszter. Felosztó (partitional) klaszterezés esetén az adathalmazt olyan, nem átfed alcsoportokra bontjuk, ahol minden adatobjektum pontosan egy részhalmazba kerül (Tan, Steinbach, and Kumar 2011; Tikk 2007). A klaszterezési eljárások között aszerint is különbséget tehetünk, hogy azok egy objektumot csak egy vagy több klaszterbe is beilleszthetnek. Ez alapján beszélhetünk kizáró (exclusive), illetve nem-kizáró (non exclusive), vagy átfed (overlapping) klaszterezésrl. Az elbbi minden objektumot csak egyetlen klaszterhez rendel hozzá, az utóbbi esetén egy pont több klaszterbe is beleillik. Fuzzy klaszterezés esetén minden objektum minden klaszterbe beletartozik egy tagsági súly erejéig, melynek értéke 0 (egyáltalán nem tartozik bele) és 1 (teljesen beletartozik) közé esik. A klasztereknek is különböz típusai vannak, így beszélhetünk prototípus-alapú, gráf-alapú vagy srség-alapú klaszterekrl. A prototípus-alapú klaszter olyan objektumokat tartalmazó halmaz, amelynek mindegyik objektuma jobban hasonlít a klasztert definiáló objektumhoz, mint bármelyik másik klasztert definiáló objektumhoz. A prototípus-alapú klaszterek közül a K-közép klaszter az egyik leggyakrabban alkalmazott. A K-közép klaszterezési módszer els lépése k darab kezd középpont kijelölése, ahol k a klaszterek kívánt számával egyenl. Ezután minden adatpontot a hozzá legközelebb es középponthoz rendelünk. Az így képzett csoportok lesznek a kiinduló klaszterek. Ezután újra meghatározzuk mindegyik klaszter középpontját a klaszterhez rendelt pontok alapján. A hozzárendelési és frissítési lépéseket felváltva folytatjuk addig, amíg egyetlen pont sem vált klasztert, vagy ameddig a középpontok ugyanazok nem maradnak (Tan, Steinbach, and Kumar 2011). A K-közép klaszterezés tehát a dokumentumokat alkotó szavak alapján keresi meg a felhasználó által megadott számú k klasztert, amelyeket a középpontjaik képviselnek, és így rendezi a dokumentumokat csoportokba. A klaszterezés vagy csoportosítás egy induktív kategorizálás, ami akkor hasznos, amikor nem állnak a kutató rendelkezésére elzetesen ismert csoportok, amelyek szerint a vizsgált dokumentumokat rendezni tudná. Hiszen ebben az esetben a korpusz elemeinek rendezéséhez nem határozunk meg elzetesen csoportokat, hanem az eljárás során olyan különálló csoportokat hozunk létre a dokumentumokból, amelynek tagjai valamilyen szempontból hasonlítanak egymásra. A csoportosítás legfbb célja az, hogy az egy csoportba kerül szövegek minél inkább hasonlítsanak egymásra, miközben a különböz csoportba kerülk minél inkább eltérjenek egymástól. Azaz klaszterezésnél nem egy-egy szöveg jellemzire vagyunk kíváncsiak, hanem arra, hogy a szövegek egy-egy csoportja milyen hasonlóságokkal bír (Burtejin 2016; Tikk 2007). A gépi kódolással végzett klaszterezés egy felügyelet nélküli tanulás, mely a szöveg tulajdonságaiból tanul, anélkül, hogy elre meghatározott csoportokat ismerne. Alkalmazása során a dokumentum tulajdonságait és a modell becsléseit felhasználva jönnek létre a különböz kategóriák, melyekhez késbb hozzárendeli a szöveget (Grimmer and Stewart 2013). Az osztályozással ellentétben a csoportosítás esetén tehát nincs ismert címkékkel ellátott kategóriarendszer vagy olyan minta, mint az osztályozás esetében a tanítókörnyezet, amibl tanulva a modellt fel lehet építeni (Tikk 2007). A gépi kódolással végzett csoportosítás (klaszterezés) esetén a kutató feladata a megfelel csoportosító mechanizmus kiválasztása, mely alapján egy program végzi el a szövegek különböz kategóriákba sorolását. Ezt követi a hasonló szövegeket tömörít csoportok elnevezésének lépése. A több dokumentumból álló korpuszok esetében a gépi klaszterelemzés különösen eredményes és költséghatékony lehet, mivel egy nagy korpusz vizsgálata sok erforrást igényel (Grimmer and Stewart 2013, 1.). A klaszterezés bemutatásához a rendszerváltás utáni magyar miniszterelnökök egy-egy véletlenszeren kiválasztott beszédét használjuk. library(readr) library(dplyr) library(purrr) library(stringr) library(readtext) library(quanteda) library(quanteda.textstats) library(tidytext) library(ggplot2) library(topicmodels) library(factoextra) library(stm) library(igraph) library(plotly) library(HunMineR) A beszédek szövege meglehetsen tiszta, ezért az egyszerség kedvéért most kihagyjuk a szövegtisztítás lépéseit. Az elemzés els lépéseként a .csv fájlból beolvasott szövegeinkbl a quanteda csomaggal korpuszt hozunk létre, majd abból egy dokumentum-kifejezés mátrixot készítünk a dfm() függvénnyel. Láthatjuk, hogy márixunk 7 megfigyelést és 4 változót tartalmaz. beszedek &lt;- HunMineR::data_miniszterelnokok beszedek_corpus &lt;- corpus(beszedek) beszedek_dfm &lt;- beszedek_corpus %&gt;% tokens() %&gt;% dfm() A glimpse funkció segítségével ismét megtekinthetjük az adatainkat és láthatjuk, hogy a 4 változó a miniszterelnökök neve, az év, a dokumentum azonosító, amely az elz két változó kombinációja, valamint maguk a beszédek szövegei. glimpse(data_miniszterelnokok) #&gt; Rows: 7 #&gt; Columns: 4 #&gt; $ doc_id &lt;chr&gt; &quot;antall_jozsef_1990&quot;, &quot;bajnai_gordon_2009&quot;, &quot;gyurcsán~ #&gt; $ text &lt;chr&gt; &quot;Elnök Úr! Tisztelt Országgyulés! Hölgyeim és Uraim! ~ #&gt; $ year &lt;dbl&gt; 1990, 2009, 2005, 1994, 2002, 1995, 2018 #&gt; $ pm &lt;chr&gt; &quot;antall_jozsef&quot;, &quot;bajnai_gordon&quot;, &quot;gyurcsány_ferenc&quot;,~ A beszédek klaszterekbe rendezését az R egyik alapfüggvénye, a kmeans() végzi. Els lépésben 2 klasztert készítünk. A table() függvénnyel megnézhetjük, hogy egy-egy csoportba hány dokumentum került. beszedek_klaszter &lt;- kmeans(beszedek_dfm, centers = 2) table(beszedek_klaszter$cluster) #&gt; #&gt; 1 2 #&gt; 2 5 A felügyelet nélküli klasszifikáció nagy kérdése, hány klasztert alakítsunk ki, hogy megközelítsük a valóságot, és ne csak mesterségesen kreáljunk csoportokat. Ez ugyanis azzal a kockázattal jár, hogy ténylegesen nem létez csoportok is létrejönnek. A klaszterek optimális számának meghatározására kvalitatív és kvantitatív lehetségeink is vannak. A következkben az utóbbira mutatunk példát, amihez a factoextra csomagot használjuk. A 7.1.-es ábra azt mutatja, hogy a klasztereken belüli négyzetösszegek hogyan változnak a k paraméter változásának függvényében. Minél kisebb a klasztereken belüli négyzetösszegek értéke, annál közelebbi pontok tartoznak össze, így a kisebb értékekkel definiált klasztereket kapunk. Az ábra alapján tehát az ideális k 4 vagy 2, attól fuggen, hogy milyen feltevésekkel élünk a kutatásunk során. A 2-es érték azért lehet jó, mert a \\(k &gt; 2\\) értékek esetén a négyzetösszegek értéke nem csökken drasztikusan és a korpuszunk alapján a két (jobb-bal) klaszter kvalitativ alapon is jól definiálható. A \\(k = 4\\) pedig azért lehet jó, mert utánna gyakorlatilag nem változik a kapott négyzetösszeg, ami azt jelzi, hogy a további klaszterek hozzáadásával nem lesz pontosabb a csoportosítás. klas_df &lt;- factoextra::fviz_nbclust(as.matrix(beszedek_dfm), kmeans, method = &quot;wss&quot;, k.max = 5, linecolor = &quot;black&quot;) + labs( title = NULL, x = &quot;Klaszterek száma&quot;, y = &quot;Klasztereken belüli négyzetösszeg&quot;) ggplotly(klas_df) Ábra 7.1: A klaszterek optimális száma A kialakított csoportokat vizuálisan is megjeleníthetjük (ld. 7.2. ábra). min_besz_plot &lt;- factoextra::fviz_cluster( beszedek_klaszter, data = beszedek_dfm, pointsize = 2, repel = TRUE, ggtheme = theme_minimal() ) + labs( title = &quot;&quot;, x = &quot;Els dimenzió&quot;, y = &quot;Második dimenzió&quot; ) + theme(legend.position = &quot;none&quot;) ggplotly(min_besz_plot) Ábra 7.2: A miniszterelnöki beszédek klaszterei Az így kapott ábrán láthatjuk nem csak a két klaszter középpontjainak egymáshoz viszonyított pozícióját, de a klaszterek elemeinek egymáshoz viszonyított pozícióját is. Ez alapján láthatjuk, hogy a 2 klaszterünk nem ugyanakkora elemszámból állnak, valamint a bels hasonlóságuk is eltér. valamint a klaszter elemek neveibl láthatjuk, hogy nem vált be a hipotézisünk arra vonatkozóan, hogy két klaszter beállítása esetén azok a jobb és baloldaliságot fogják tükrözni, ez valószínleg azért van így mert a korpuszokat ebben az esetben nem tisztítottuk, tehát stopszavazást sem végeztünk rajtuk, ebbl kifolyólag pedig a politikai pozíciók mellett a szövegek hasonlósága és különbözsége sok más tényezt is tükröz (pl: hogy mennyire összetetten vagy egyszeren fogalmaz az illet, vagy a beszéd elhangzásának idején különös relevanciával bíró közpolitikákat) 7.3 LDA topikmodellek36 A topikmodellezés a dokumentumok téma klasztereinek meghatározására szolgáló valószínség alapú eljárás, amely szógyakoriságot állapít meg minden témához, és minden dokumentumhoz hozzárendeli az adott témák valószínségét. A topikmodellezés egy felügyelet nélküli tanulási módszer, amely során az alkalmazott algoritmus a dokumentum tulajdonságait és a modell becsléseit felhasználva hoz létre különböz kategóriákat, melyekhez késbb hozzárendeli a szöveget (Burtejin 2016; Grimmer and Stewart 2013; Tikk 2007). Az egyik leggyakrabban alkalmazott topikmodellezési eljárás, a Látens Dirichlet Allokáció (LDA) alapja az a feltételezés, hogy minden korpusz topikok/témák keverékébl áll, ezen témák pedig statisztikailag a korpusz szókészlete valószínségi függvényeinek (eloszlásának) tekinthetek (Blei, Ng, and Jordan 2003). Az LDA a korpusz dokumentumainak csoportosítása során az egyes dokumentumokhoz topik szavakat rendel, a topikok megbecsléséhez pedig a szavak együttes megjelenését vizsgálja a dokumentum egészében. Az LDA algoritmusnak elzetesen meg kell adni a keresett klaszterek (azaz a keresett topikok) számát, ezt követen a dokumentumhalmazban szerepl szavak eloszlása alapján az algoritmus azonosítja a kulcsszavakat, amelyek eloszlása kirajzolja a topikokat (Blei, Ng, and Jordan 2003; Burtejin 2016; Jacobi, Van Atteveldt, and Welbers 2016). A következkben a magyar törvények korpuszán szemléltetjük a topikmodellezés módszerét, hogy a mesterséges intelligencia segítségével feltárjuk a korpuszon belüli rejtett összefüggéseket. A korábban leírtak szerint tehát nincsenek elre meghatározott kategóriáink, dokumentumainkat a klaszterezés segítségével szeretnénk csoportosítani. Egy-egy dokumentumban keveredhetnek a témák és az azokat reprezentáló szavak. Mivel ugyanaz a szó több topikhoz is kapcsolódhat, így az eljárás komplex elemzési lehetséget nyújt, az egy szövegen belüli témák és akár azok dokumentumon belüli súlyának azonosítására. Az alábbiakban a 19982002-es és a 20022006-os parlamenti ciklus 1032 törvényszövegének topikmodellezését és a szükséges elkészít, korpusztisztító lépéseket mutatjuk be. A HunMineR csomag segítségével beolvassuk az elemezni kívánt fájlokat.37 Töltsük be az elemezni kívánt csv fájlt, megadva az elérési útvonalát. torvenyek &lt;- HunMineR::data_lawtext_1998_2006 glimpse(torvenyek) #&gt; Rows: 1,032 #&gt; Columns: 2 #&gt; $ doc_id &lt;chr&gt; &quot;1998L&quot;, &quot;1998LI&quot;, &quot;1998LII&quot;, &quot;1998LIII&quot;, &quot;1998LIV&quot;, ~ #&gt; $ text &lt;chr&gt; &quot;1998. évi L. törvény\\n\\naz Egyesült Nemzetek Szervez~ Láthatjuk, hogy ezek az objektum a dokumentum azonosítón kívül (amely a törvény évét és számát tartalmazza) nem rendelkezik egyéb metaadatokkal. Az elz fejezetekben láthattuk, hogyan lehet használni a stringr csomagot a szövegtisztításra. A lépések a már megismert sztenderd folyamatot követik: számok, központozás, sortörések, extra szóközök eltávolítása, illetve a szöveg kisbetsítése. Az eddigieket további szövegtisztító lépésekkel is kiegészíthetjük. Olyan elemek esetében, amelyek nem feltétlenül különálló szavak és el akarjuk távolítani ket a korpuszból, szintén a str_remove_all() a legegyszerbb megoldás. torvenyek_tiszta &lt;- torvenyek %&gt;% mutate( text = str_remove_all(string = text, pattern = &quot;[:cntrl:]&quot;), text = str_remove_all(string = text, pattern = &quot;[:punct:]&quot;), text = str_remove_all(string = text, pattern = &quot;[:digit:]&quot;), text = str_to_lower(text), text = str_trim(text), text = str_squish(text), text = str_remove_all(string = text, pattern = &quot;&quot;), text = str_remove_all(string = text, pattern = &quot;&quot;), text = str_remove_all(string = text, pattern = &quot;&quot;), text = str_remove_all(string = text, pattern = &quot;&quot;), text = str_remove_all(string = text, pattern = &quot;&quot;), text = str_remove_all(string = text, pattern = &quot;&quot;), text = str_remove_all(string = text, pattern = &quot;«&quot;), text = str_remove_all(string = text, pattern = &quot;»&quot;), text = str_remove_all(string = text, pattern = &quot;§&quot;), text = str_remove_all(string = text, pattern = &quot;°&quot;), text = str_remove_all(string = text, pattern = &quot;&lt;U+25A1&gt;&quot;), text = str_remove_all(string = text, pattern = &quot;@&quot;) ) A dokumentum változókat egy külön fájlból töltjük be, ami a törvények keletkezési évét tartalmazza, illetve azt, hogy melyik kormányzati ciklusban születtek. Mindkét adatbázisban egy közös egyedi azonosító jelöli az egyes törvényeket, így ki tudjuk használni a dplyr left_join() függvényét, ami hatékonyan és gyorsan kapcsol össze adatbázisokat közös egyedi azonosító mentén. Jelen esetben ez az egyedi azonosító a txt_filename oszlopból fog elkészülni, amely a törvények neveit tartalmazza. Els lépésben betöltjük a metaadatokat tartalmazó adattáblát, majd a .txt rész eltti törvényneveket tartjuk csak meg a létrehozott doc_id- oszlopban. A [^\\\\.]* regular expression itt a string elejétl indulva kijelöl mindent az elso . karakterig. A str_extract() pedig ezt a kijelölt string szakaszt (ami a törvények neve) menti át az új változónkba. torveny_meta &lt;- HunMineR::data_lawtext_meta torveny_meta &lt;- torveny_meta %&gt;% mutate(doc_id = str_extract(txt_filename, &quot;[^\\\\.]*&quot;)) %&gt;% select(-txt_filename) head(torveny_meta, 5) #&gt; # A tibble: 5 x 4 #&gt; year electoral_cycle majortopic doc_id #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 1998 1998-2002 13 1998XXXV #&gt; 2 1998 1998-2002 20 1998XXXVI #&gt; 3 1998 1998-2002 3 1998XXXVII #&gt; 4 1998 1998-2002 6 1998XXXVIII #&gt; 5 1998 1998-2002 13 1998XXXIX Végül összefzzük a dokumentumokat és a metaadatokat tartalmazó data frame-eket. torveny_final &lt;- dplyr::left_join(torvenyek_tiszta, torveny_meta, by = &quot;doc_id&quot;) Majd létrehozzuk a korpuszt és ellenrizzük azt. torvenyek_corpus &lt;- corpus(torveny_final) head(summary(torvenyek_corpus), 5) #&gt; Text Types Tokens Sentences year electoral_cycle majortopic #&gt; 1 1998L 2879 9628 1 1998 1998-2002 3 #&gt; 2 1998LI 352 680 1 1998 1998-2002 20 #&gt; 3 1998LII 446 992 1 1998 1998-2002 9 #&gt; 4 1998LIII 126 221 1 1998 1998-2002 9 #&gt; 5 1998LIV 835 2013 1 1998 1998-2002 9 Az RStudio environments fülén láthatjuk, hogy egy 1032 elembl álló korpusz jött létre, amelynek tartalmát a summary() paranccsal kiíratva, a console ablakban megjelenik a dokumentumok listája és a fbb leíró statisztikai adatok (egyedi szavak  types; szószám  tokens; mondatok  sentences). Az elbbi fejezettl eltéren most a tokenizálás során is végzünk még egy kis tisztítást: a felesleges stop szavakat kitöröljük a tokens_remove() és stopwords() kombinálásával. A quanteda tartalmaz egy beépített magyar stopszó szótárat. A második lépésben szótövesítjük a tokeneket a tokens_words() használatával, ami szintén képes a magyar nyelv szövegeket kezelni. Szükség esetén a beépített magyar nyelv stopszó szótárat saját stopszavakkal is kiegészíthetjük. Példaként a HunMineR csomagban lév kiegészít stopszó data frame-t töltsük be. custom_stopwords &lt;- HunMineR::data_legal_stopwords Mivel a korpusz ellenrzése során találunk még olyan kifejezéseket, amelyeket el szeretnénk távolítani, ezeket is kiszrjük. custom_stopwords_egyeb &lt;- c(&quot;lábjegyzet&quot;, &quot;országgylés&quot;, &quot;ülésnap&quot;) Aztán pedig a korábban már megismert pipe operátor használatával elkészítjük a token objektumunkat. A szótövesített tokeneket egy külön objektumban tároljuk, mert gyakran elfordul, hogy késbb vissza kell térnünk az eredeti token objektumhoz, hogy egyéb mveleteket végezzünk el, például további stopszavakat távolítsunk el. torvenyek_tokens &lt;- tokens(torvenyek_corpus) %&gt;% tokens_remove(stopwords(&quot;hungarian&quot;)) %&gt;% tokens_remove(custom_stopwords) %&gt;% tokens_remove(custom_stopwords_egyeb) %&gt;% tokens_wordstem(language = &quot;hun&quot;) Végül eltávolítjuk a dokumentum-kifejezés mátrixból a túl gyakori kifejezéseket. A dfm_trim() függvénnyel a nagyon ritka és nagyon gyakori szavak megjelenését kontrollálhatjuk. Ha termfreq_type opció értéke prop (úgymint proportional) akkor 0 és 1.0 közötti értéket vehetnek fel a max_termfreq/docfreq és min_termfreq/docfreq paraméterek. A lenti példában azokat a tokeneket tartjuk meg, amelyek legalább egyszer elfordulnak ezer dokumentumonként (így kizárva a nagyon ritka kifejezéseket). torvenyek_dfm &lt;- dfm(torvenyek_tokens) %&gt;% quanteda::dfm_trim(min_termfreq = 0.001, termfreq_type = &quot;prop&quot;) A szövegtisztító lépesek eredményét úgy ellenrizhetjük, hogy szógyakorisági listát készítünk a korpuszban maradt kifejezésekrl. Itt kihasználhatjuk a korpuszunkban lév metaadatokat és megnézhetjük ciklus szerinti bontásban a szófrekvencia ábrát. Az ábránál figyeljünk arra, hogy a tidytext reorder_within() függvényét használjuk, ami egy nagyon hasznos megoldás a csoportosított sorrendbe rendezésre a ggplot2 ábránál (ld. 7.3. ábra). top_tokens &lt;- textstat_frequency( torvenyek_dfm, n = 15, groups = docvars(torvenyek_dfm, field = &quot;electoral_cycle&quot;) ) tok_df &lt;- ggplot(top_tokens, aes(reorder_within(feature, frequency, group), frequency)) + geom_point(aes(shape = group), size = 2) + coord_flip() + labs( x = NULL, y = &quot;szófrekvenica&quot; ) + facet_wrap(~group, nrow = 4, scales = &quot;free&quot;) + theme(panel.spacing = unit(1, &quot;lines&quot;)) + tidytext::scale_x_reordered() + theme(legend.position = &quot;none&quot;) ggplotly(tok_df, height = 1000, tooltip = &quot;frequency&quot;) Ábra 7.3: A 15 leggyakoribb token a korpuszban A szövegtisztító lépéseket késbb újabbakkal is kiegészíthetjük, ha észrevesszük, hogy az elemzést zavaró tisztítási lépés maradt ki. Ilyen esetben tovább tisztíthatjuk a korpuszt, majd újra lefuttathatjuk az elemzést. Például, ha szükséges, további stopszavak eltávolítását is elvégezhetjük egy újabb stopszólista hozzáadásával. Ilyenkor ugyanúgy járunk el, mint az elz stopszólista esetén. custom_stopwords2 &lt;- HunMineR::data_legal_stopwords2 torvenyek_tokens_final &lt;- torvenyek_tokens %&gt;% tokens_remove(custom_stopwords2) Ezután újra ellenrizzük az eredményt. torvenyek_dfm_final &lt;- dfm(torvenyek_tokens_final) %&gt;% dfm_trim(min_termfreq = 0.001, termfreq_type = &quot;prop&quot;) top_tokens_final &lt;- textstat_frequency(torvenyek_dfm_final, n = 15, groups = docvars(torvenyek_dfm, field = &quot;electoral_cycle&quot;) ) Ezt egy interaktív ábrán is megjelenítjük (ld. 7.4. ábra). tokclean_df &lt;- ggplot(top_tokens_final, aes(reorder_within(feature, frequency, group), frequency)) + geom_point(aes(shape = group), size = 2) + coord_flip() + labs( x = NULL, y = &quot;szófrekvencia&quot; ) + facet_wrap(~group, nrow = 2, scales = &quot;free&quot;) + theme(panel.spacing = unit(1, &quot;lines&quot;)) + tidytext::scale_x_reordered() + theme(legend.position = &quot;none&quot;) ggplotly(tokclean_df, height = 1000, tooltip = &quot;frequency&quot;) Ábra 7.4: A 15 leggyakoribb token a korpuszban, a bovített stop szó listával A szövegtisztító és a korpusz elkészít mveletek után következhet az LDA illesztése. Az alábbiakban az LDA illesztés két módszerét, a VEM-et és a Gibbs-et mutatjuk be. A modell mindkét módszer esetén ugyanaz, a különbség a következtetés módjában van. A VEM módszer variációs következtetés, míg a Gibbs mintavételen alapuló következtetés. (Blei, Ng, and Jordan 2003; Griffiths and Steyvers 2004; Phan, Nguyen, and Horiguchi 2008). A két modell illesztése nagyon hasonló, meg kell adnunk az elemezni kívánt dfm nevét, majd a k értékét, ami egyenl az általunk létrehozni kívánt topikok számával, ezt követen meg kell jelölnünk, hogy a VEM vagy a Gibbs módszert alkalmazzuk. A set.seed() funkció az R véletlen szám generátor magjának beállítására szolgál, ami ahhoz kell, hogy a kapott eredmények, ábrák stb. pontosan reprodukálhatóak legyenek. A set.seed() bármilyen tetszleges egész szám lehet. Mivel az elemzésünk célja a két ciklus jogalkotásának összehasonlítása, a korpuszunkat két alkorpuszra bontjuk, ehhez a dokumentumok kormányzati ciklus azonosítóját használjuk fel. A dokumentum változók alapján a dfm_subset() parancs segítségével választjuk szét a már elkészült és a tisztított mátrixunkat. dfm_98_02 &lt;- dfm_subset(torvenyek_dfm_final, electoral_cycle == &quot;1998-2002&quot;) dfm_02_06 &lt;- dfm_subset(torvenyek_dfm_final, electoral_cycle == &quot;2002-2006&quot;) 7.3.1 A VEM módszer alkalmazása a magyar törvények korpuszán Saját korpuszunkon elször a VEM módszert alkalmazzuk, ahol k = 10, azaz a modell 10 témacsoportot alakít ki. Ahogyan korábban arról már volt szó, a k értékének meghatározása kutatói döntésen alapul, a modell futtatása során bevett gyakorlat a különböz k értékekkel való kísérletezés. Az elkészült modell kiértékelésére az elemzés elkészülte után a perplexity() függvény segítségével van lehetségünk  ahol a theta az adott topikhoz való tartozás valószínsége. A függvény a topikok által reprezentált elméleti szóeloszlásokat hasonlítja össze a szavak tényleges eloszlásával a dokumentumokban. A függvény értéke nem önmagában értelmezend, hanem két modell összehasonlításában, ahol a legalacsonyabb perplexity (zavarosság) értékkel rendelkez modellt tekintik a legjobbnak.38 Az illusztráció kedvéért lefuttatunk 4 LDA modellt az 19982002-es kormányzati ciklushoz tartozó dfm-en. Az iterációhoz a purrr csomag map függvényét használtuk. Fontos megjegyezni, hogy minél nagyobb a korpuszunk, annál több számítási kapacitásra van szükség (és annál tovább tart a számítás). k_topics &lt;- c(5, 10, 15, 20) lda_98_02 &lt;- k_topics %&gt;% purrr::map(topicmodels::LDA, x = dfm_98_02, control = list(seed = 1234)) perp_df &lt;- dplyr::tibble( k = k_topics, perplexity = purrr::map_dbl(lda_98_02, topicmodels::perplexity) ) perp_df &lt;- ggplot(perp_df, aes(k, perplexity)) + geom_point() + geom_line() + labs( x = &quot;Klaszterek száma&quot;, y = &quot;Zavarosság&quot; ) ggplotly(perp_df) Ábra 7.5: Zavarosság változása a k függvényében A zavarossági mutató alapján a 20 topikos modell szerepel a legjobban, de a megfelel k kiválasztása a kutató kvalitatív döntésén múlik. Természetesen könnyen elképzelhet az is, hogy egy 40 topikos modellnél jelentsen kisebb zavarossági értéket kapjunk. Egy ilyen esetben mérlegelni kell, hogy a kapott csoportosítás kvalitatívan értelmezhet-e, illetve hogy tisztább-e annyival a kapott modell, mint amennyivel több id és számítási energia a becslése. A zavarossági pontszám ehhez a kvalitatív döntéshez ad kvantitatív szempontokat, de érdemes általános sorvezetként tekinteni rá, nem pedig mint egy áthághatatlan szabályra (ld. 7.5).39 A reprodukálhatóság és a futási sebesség érdekében a fejezet további részeiben a k paraméternek 10-es értéket adunk. Ezzel lefuttatunk egy-egy modellt a két ciklusra. vem_98_02 &lt;- LDA(dfm_98_02, k = 10, method = &quot;VEM&quot;, control = list(seed = 1234)) vem_02_06 &lt;- LDA(dfm_02_06, k = 10, method = &quot;VEM&quot;, control = list(seed = 1234)) Ezt követen a modell által létrehozott topikokat tidy formátumba tesszük és egyesítjük egy adattáblában.40 topics_98_02 &lt;- tidytext::tidy(vem_98_02, matrix = &quot;beta&quot;) %&gt;% mutate(electoral_cycle = &quot;1998-2002&quot;) topics_02_06 &lt;- tidytext::tidy(vem_02_06, matrix = &quot;beta&quot;) %&gt;% mutate(electoral_cycle = &quot;2002-2006&quot;) lda_vem &lt;- dplyr::bind_rows(topics_98_02, topics_02_06) Ezután listázzuk az egyes topikokhoz tartozó leggyakoribb kifejezéseket. top_terms &lt;- lda_vem %&gt;% group_by(electoral_cycle, topic) %&gt;% top_n(5, beta) %&gt;% top_n(5, term) %&gt;% ungroup() %&gt;% arrange(topic, -beta) Végül a ggplot2 csomag segítségével ábrán is megjeleníthetjük az egyes topikok 10 legfontosabb kifejezését (ld. 7.6. és 7.7. ábra). toptermsplot9802vem &lt;- top_terms %&gt;% filter(electoral_cycle == &quot;1998-2002&quot;) %&gt;% ggplot(aes(reorder_within(term, beta, topic), beta)) + geom_col(show.legend = FALSE) + facet_wrap(~topic, scales = &quot;free&quot;, ncol = 1) + theme(panel.spacing = unit(4, &quot;lines&quot;)) + coord_flip() + labs( x = NULL, y = NULL ) + tidytext::scale_x_reordered() ggplotly(toptermsplot9802vem, tooltip = &quot;beta&quot;) Ábra 7.6: 19982002-es ciklus: topikok és kifejezések (VEM mintavételezéssel) toptermsplot0206vem &lt;- top_terms %&gt;% filter(electoral_cycle == &quot;2002-2006&quot;) %&gt;% ggplot(aes(reorder_within(term, beta, topic), beta)) + geom_col(show.legend = FALSE) + facet_wrap(~topic, scales = &quot;free&quot;, ncol = 1) + theme(panel.spacing = unit(4, &quot;lines&quot;)) + coord_flip() + labs( x = NULL, y = NULL ) + tidytext::scale_x_reordered() ggplotly(toptermsplot0206vem, tooltip = &quot;beta&quot;) Ábra 7.7: 20022006-os ciklus topikok és kifejezések (VEM mintavételezéssel) 7.3.2 Az LDA Gibbs módszer alkalmazása a magyar törvények korpuszán A következkben ugyanazon a korpuszon az LDA Gibbs módszert alkalmazzuk. A szövegelkészít és tisztító lépések ennél a módszernél is ugyanazok, mint a fentebb bemutatott VEM módszer esetében, így itt most csak a modell illesztését mutatjuk be. gibbs_98_02 &lt;- LDA(dfm_98_02, k = 10, method = &quot;Gibbs&quot;, control = list(seed = 1234)) gibbs_02_06 &lt;- LDA(dfm_02_06, k = 10, method = &quot;Gibbs&quot;, control = list(seed = 1234)) Itt is elvégezzük a topikok tidy formátumra alakítását. topics_g98_02 &lt;- tidy(gibbs_98_02, matrix = &quot;beta&quot;) %&gt;% mutate(electoral_cycle = &quot;1998-2002&quot;) topics_g02_06 &lt;- tidy(gibbs_02_06, matrix = &quot;beta&quot;) %&gt;% mutate(electoral_cycle = &quot;2002-2006&quot;) lda_gibbs &lt;- bind_rows(topics_g98_02, topics_g02_06) Majd listázzuk az egyes topikokhoz tartozó leggyakoribb kifejezéseket. top_terms_gibbs &lt;- lda_gibbs %&gt;% group_by(electoral_cycle, topic) %&gt;% top_n(5, beta) %&gt;% top_n(5, term) %&gt;% ungroup() %&gt;% arrange(topic, -beta) Ezután a ggplot2 csomag segítségével ábrán is megjeleníthetjük (ld. 7.8. és @ref(fig:Gibbs0206 ábra). toptermsplot9802gibbs &lt;- top_terms_gibbs %&gt;% filter(electoral_cycle == &quot;1998-2002&quot;) %&gt;% ggplot(aes(reorder_within(term, beta, topic), beta)) + geom_col(show.legend = FALSE) + facet_wrap(~topic, scales = &quot;free&quot;, ncol = 1) + theme(panel.spacing = unit(4, &quot;lines&quot;)) + coord_flip() + labs( title = , x = NULL, y = NULL ) + tidytext::scale_x_reordered() ggplotly(toptermsplot9802gibbs, tooltip = &quot;beta&quot;) Ábra 7.8: 19982002-es ciklus topikok és kifejezések (Gibbs mintavétellel) toptermsplot0206gibbs &lt;- top_terms_gibbs %&gt;% filter(electoral_cycle == &quot;2002-2006&quot;) %&gt;% ggplot(aes(reorder_within(term, beta, topic), beta)) + geom_col(show.legend = FALSE) + facet_wrap(~topic, scales = &quot;free&quot;, ncol = 1) + theme(panel.spacing = unit(4, &quot;lines&quot;)) + coord_flip() + labs( x = NULL, y = NULL ) + scale_x_reordered() ggplotly(toptermsplot0206gibbs, tooltip = &quot;beta&quot;) Ábra 7.9: 20022006-os ciklus topikok és kifejezések (Gibbs mintavétellel) 7.4 Strukturális topikmodellek A kvantitatív szövegelemzés elterjedésével együtt megjelentek a módszertani innovációk is. Roberts et al. (2014) kiváló cikkben mutatták be a strukturális topikmodelleket (structural topic models  stm), amelyek f újítása, hogy a dokumentumok metaadatai kovariánsként41 tudják befolyásolni, hogy egy-egy kifejezés mekkora valószínséggel lesz egy-egy téma része. A kovariánsok egyrészrl megmagyarázhatják, hogy egy-egy dokumentum mennyire függ össze egy-egy témával (topical prevalence), illetve hogy egy-egy szó mennyire függ össze egy-egy témán belül (topical content). Az stm modell becslése során mindkét típusú kovariánst használhatjuk, illetve ha nem adunk meg dokumentum metaadatot, akkor az stm csomag stm függvénye a Korrelált Topic Modell-t fogja becsülni. Az stm modelleket az R-ben az stm csomaggal tudjuk kivitelezni. A csomag fejleszti között van a módszer kidolgozója is, ami nem ritka az R csomagok esetében. A lenti lépésekben a csomag dokumentációjában szerepl ajánlásokat követjük, habár a könyv írásakor a stm már képes volt a quanteda-ban létrehozott dfm-ek kezelésére is. A kiinduló adatbázisunk a törvény_final, amit a fejezet elején hoztunk létre a dokumentumokból és a metaadatokból. A javasolt munkafolyamat a textProcessor() függvény használatával indul, ami szintén tartalmazza az alap szöveg elkészítési lépéseket. Az egyszerség és a futási sebesség érdekében itt most ezek többségétl eltekintünk, mivel a fejezet korábbi részeiben részletesen tárgyaltuk ket. Az elkészítés utolsó szakaszában az out objektumban tároljuk el a dokumentumokat, az egyedi szavakat, illetve a metaadatokat (kovariánsokat). data_stm &lt;- torveny_final processed_stm &lt;- stm::textProcessor( torveny_final$text, metadata = torveny_final, lowercase = FALSE, removestopwords = FALSE, removenumbers = FALSE, removepunctuation = FALSE, ucp = FALSE, stem = TRUE, language = &quot;hungarian&quot;, verbose = FALSE ) out &lt;- stm::prepDocuments(processed_stm$documents, processed_stm$vocab, processed_stm$meta) #&gt; Removing 96264 of 180243 terms (96264 of 1252793 tokens) due to frequency #&gt; Your corpus now has 1032 documents, 83979 terms and 1156529 tokens. A strukturális topikmodellünket az stm függvénnyel becsüljük és a kovariánsokat a prevalence opciónál tudjuk formulaként megadni. A lenti példában a Hungarian Comparative Agendas Project42 kategóriáit (például gazdaság, egészségügy stb.) és a kormányciklusokat használjuk. A futási id kicsit hosszabb mint az LDA modellek esetében. stm_fit &lt;- stm::stm( out$documents, out$vocab, K = 10, prevalence = ~ majortopic + electoral_cycle, data = out$meta, init.type = &quot;Spectral&quot;, seed = 1234, verbose = FALSE ) Amennyiben a kutatási kérdés megkívánja, akkor megvizsgálhatjuk, hogy a kategorikus változóinknak milyen hatása volt az egyes topikok esetében. Ehhez az estimateEffect() függvénnyel lefuttatunk egy lineáris regressziót és a summary() használatával láthatjuk az egyes kovariánsok koefficienseit. Itt az els topikkal illusztráljuk az eredményt, ami azt mutatja, hogy (a kategórikus változóink els kategóriájához mérten) statisztikailag szignifikáns mind a téma, mind pedig a kormányzati ciklusok abban, hogy egyes dokumentumok milyen témákból épülnek fel. out$meta$electoral_cycle &lt;- as.factor(out$meta$electoral_cycle) out$meta$majortopic &lt;- as.factor(out$meta$majortopic) cov_estimate &lt;- stm::estimateEffect(1:10 ~ majortopic + electoral_cycle, stm_fit, meta = out$meta, uncertainty = &quot;Global&quot;) summary(cov_estimate, topics = 1) #&gt; #&gt; Call: #&gt; stm::estimateEffect(formula = 1:10 ~ majortopic + electoral_cycle, #&gt; stmobj = stm_fit, metadata = out$meta, uncertainty = &quot;Global&quot;) #&gt; #&gt; #&gt; Topic 1: #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 0.3034 0.0310 9.79 &lt; 2e-16 *** #&gt; majortopic2 -0.2040 0.0677 -3.01 0.00265 ** #&gt; majortopic3 -0.2044 0.0595 -3.43 0.00062 *** #&gt; majortopic4 -0.2211 0.0589 -3.75 0.00018 *** #&gt; majortopic5 0.1030 0.0472 2.18 0.02928 * #&gt; majortopic6 -0.2231 0.0587 -3.80 0.00015 *** #&gt; majortopic7 -0.1557 0.0681 -2.29 0.02244 * #&gt; majortopic8 -0.2157 0.0729 -2.96 0.00315 ** #&gt; majortopic9 0.5250 0.0876 5.99 2.9e-09 *** #&gt; majortopic10 -0.1087 0.0549 -1.98 0.04805 * #&gt; majortopic12 -0.1741 0.0407 -4.28 2.0e-05 *** #&gt; majortopic13 -0.1358 0.0560 -2.43 0.01543 * #&gt; majortopic14 -0.2172 0.0755 -2.88 0.00411 ** #&gt; majortopic15 -0.1475 0.0427 -3.46 0.00057 *** #&gt; majortopic16 -0.0959 0.0531 -1.81 0.07100 . #&gt; majortopic17 -0.2243 0.0580 -3.86 0.00012 *** #&gt; majortopic18 0.2104 0.0573 3.67 0.00025 *** #&gt; majortopic19 0.0738 0.0512 1.44 0.14966 #&gt; majortopic20 -0.2105 0.0392 -5.37 1.0e-07 *** #&gt; majortopic21 -0.2247 0.0698 -3.22 0.00131 ** #&gt; majortopic23 -0.1670 0.0939 -1.78 0.07566 . #&gt; electoral_cycle2002-2006 -0.1036 0.0210 -4.94 9.2e-07 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Az LDA modelleknél már bemutatott munkafolyamat az stm modellünk esetében is alkalmazható, hogy vizuálisan is megjelenítsük az eredményeinket. A tidy() függvény data frammé alakítja az stm objektumot, amit aztán a már ismers dplyr csomagban lév függvényekkel tudunk átalakítani és végül vizualizálni a ggplot2 csomaggal. A 7.10-es ábrán az egyes témákhoz tartozó 5 legvalószínbb szót mutatjuk be. tidy_stm &lt;- tidytext::tidy(stm_fit) topicplot &lt;- tidy_stm %&gt;% group_by(topic) %&gt;% top_n(5, beta) %&gt;% ungroup() %&gt;% mutate( topic = paste0(&quot;Topic &quot;, topic), term = reorder_within(term, beta, topic) ) %&gt;% ggplot(aes(term, beta)) + geom_col() + facet_wrap(~topic, scales = &quot;free_y&quot;, ncol = 1) + theme(panel.spacing = unit(4, &quot;lines&quot;)) + coord_flip() + scale_x_reordered() + labs( x = NULL, y = NULL ) ggplotly(topicplot, tooltip = &quot;beta&quot;) Ábra 7.10: Topikonkénti legmagasabb valószínuségu szavak Egy-egy topichoz tartozó meghatározó szavak annak függvényében változhatnak, hogy milyen algoritmust használunk. A labelTopics() függvény a már becsült stm modellünket alapul véve kínál négyféle alternatív opciót. Az egyes algoritmusok részletes magyarázatáért érdemes elolvasni a csomag részletes leírását.43 stm::labelTopics(stm_fit, c(1:2)) #&gt; Topic 1 Top Words: #&gt; Highest Prob: szerzodo, vagi, egyezméni, fél, államban, nem, másik #&gt; FREX: megadóztatható, haszonhúzója, beruházóinak, segélycsapatok, adóztatást, jövedelemadók, kijelölések #&gt; Lift: árucikkeket, átalányösszegben, átléphetik, átszállítást, beruházóikat, célországban, cikktanulók #&gt; Score: szerzodo, államban, illetoségu, egyezméni, megadóztatható, adóztatható, cikka #&gt; Topic 2 Top Words: #&gt; Highest Prob: muködési, célú, támogatások, költségvetésegyéb, felhalmozási, terhelo, beruházási #&gt; FREX: kiadásokfelújításegyéb, kiadásokintézményi, kiadásokközponti, költségvetésfelhalmozási, kiadásokkormányzati, felújításegyéb, rek #&gt; Lift: a+b+c, a+b+c+d, adago, adódóa, adósságállományából, adósságrendezésr, adótartozásának #&gt; Score: költségvetésegyéb, költségvetésszemélyi, kiadásokfelhalmozási, járulékokdolog, költségvetésintézményi, kiadásokegyéb, juttatásokmunkaadókat A korpuszunkon belüli témák megoszlását a plot.STM()-el tudjuk ábrázolni. Jól látszik, hogy a Topic 6-ba tartozó szavak vannak jelen a legnagyobb arányban a dokumentumaink között. stm::plot.STM(stm_fit, &quot;summary&quot;, main = &quot;&quot;, labeltype = &quot;frex&quot;, xlab = &quot;Várható topic arányok&quot;, xlim=c(0,1) ) Ábra 7.11: Leggyakoribb témák és kifejezések Végezetül a témák közötti korrelációt a topicCorr() függvénnyel becsülhetjük és az igraph csomagot betöltve a plot() paranccsal tudjuk vizualizálni. Az eredmény egy hálózat lesz, amit gráfként ábrázolunk. A gráfok élei a témák közötti összefüggést (korrelációt) jelölik. A 7.12-es ábrán a 10 topikos modellünket látjuk, azonban ilyen kis k esetén nem látunk jelents korrelációs kapcsolatot, csak a 4-es és 1-es témák között. A korrelációs gráfok jellemzen hasznosabbak, hogyha a nagy témaszám miatt (több száz, vagy akár több ezer) egy magasabb absztrakciós szinten szeretnénk vizsgálni az eredményeinket. plot(stm::topicCorr(stm_fit)) Ábra 7.12: Témák közötti korreláció hálózat A kód részben az alábbiakon alapul: tidytextmining.com/topicmodeling.html. Az általunk is használt topicmodels csomag interfészt biztosít az LDA modellek és a korrelált témamodellek (CTM) C kódjához, valamint az LDA modellek illesztéséhez szükséges C ++ kódhoz. A törvényeket és a metaadatokat tartalmazó adatbázisokat regisztációt követen a Hungarian Comparative Agendas Projekt honlapjáról https://cap.tk.hu/ lehet letölteni. Részletesebben lásd például: http://brooksandrew.github.io/simpleblog/articles/latent-dirichlet-allocation-under-the-hood/ A ldatuning csomagban további indikátor implementációja található, ami a perplexityhez hasonlóan minimalizálásra (Arun et al. 2010; Cao et al. 2009), illetve maximalizálásra alapoz (Deveaud, SanJuan, and Bellot 2014; Griffiths and Steyvers 2004) A tidy formátumról bvebben: https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html A kovariancia megadja két egymástól különböz változó együttmozgását. Kis értékei gyenge, nagy értékei ers lineáris összefüggésre utalnak. A kódkönyv elérhet az alábbi linken: Comparative Agendas Project. Az stm csomaghoz tartozó leírás: https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf "],["embedding.html", "8 Szóbeágyazások 8.1 A szóbeágyazás célja 8.2 Word2Vec és GloVe", " 8 Szóbeágyazások 8.1 A szóbeágyazás célja Az eddigi fejezetekben elssorban a szózsák (bag of words) alapú módszerek voltak eltérben. A szózsák alapú módszerekkel szemben, amelyek alkalmazása során elveszik a kontextuális tartalom, a szóbeágyazáson (word embedding) alapuló modellek kimondottan a kontextuális információt ragadják meg. A szóbeágyazás a topikmodellekhez hasonlóan a felügyelet nélküli tanulás módszerére épül, azonban itt a dokumentum domináns kifejezéseinek és témáinak feltárása helyett a szavak közötti szemantikai kapcsolat megértése a cél. Vagyis a modellnek képesnek kell lennie az egyes szavak esetén szinonimáik és ellentétpárjaik megtalálására. A hagyományos topikmodellezés esetén a modell a szavak dokumentumokon belüli együttes megjelenési statisztikái alapján becsül dokumentum-topik, illetve topik-szó eloszlásokat, azzal a céllal, hogy koherens téma-csoportokat képezzen. Ezzel szemben a szóbeágyazás legújabb iskolája már neurális halókon alapul. A neurális háló a tanítási folyamata során az egyes szavak vektorreprezentációját állítja el. A vektorok jellemzen 100300 dimenzióból állnak, a távolságuk alapján pedig megállapítható, hogy az egyes kifejezések milyen szemantikai kapcsolatban állnak egymással. A szóbeágyazás célja tehát a szemantikai relációk feltárása. A szavak vektorizálásának köszönheten bármely (a korpuszunkban szerepl) tetszleges számú szóról eldönthetjük, hogy azok milyen szemantikai kapcsolatban állnak egymással, azaz szinonimaként vagy ellentétes fogalompárként szerepelnek. A szóvektorokon dimenziócsökkent eljárást alkalmazva, s a multidimenzionális (100300 dimenziós) teret 2 dimenziósra szkítve könnyen vizualizálhatjuk is a korpuszunk kifejezései között fennálló szemantikai távolságot, és ahogy a lenti ábrákon láthatjuk, azt, hogy az egyes kifejezések milyen relációban állnak egymással  a szemantikailag hasonló tartalmú kifejezések egymáshoz közel, míg a távolabbi jelentéstartalmú kifejezések egymástól távolabb foglalnak helyet. A klasszikus példa, amivel jól lehet szemléltetni a szóvektorok közötti összefüggést: king - man + woman = queen. 8.2 Word2Vec és GloVe A társadalomtudományokban szóbeágyazásra a két legnépszerbb algoritmus  a Word2Vec és a GloVe  a kontextuális szövegeloszláson (distributional similarity based representations) alapul, vagyis abból a feltevésbl indul ki, hogy a hasonló kifejezések hasonló kontextusban fordulnak el, emellett mindkett sekély neurális hálón (2 rejtett réteg) alapuló modell.44 A Word2Vec-nek két verziója van: Continuous Bag-of-words (CBOW) és SkipGram (SG). Elbbi a kontextuális szavakból jelzi elre (predicting) a kontextushoz legszorosabban kapcsolódó kifejezést, míg utóbbi adott kifejezésbl jelzi elre a kontextust Mikolov et al. (2013). A GloVe (Global Vectors for Word Representation) a Word2Vec-hez hasonlóan neurális hálón alapuló, szóvektorok elállítását célzó modell, a Word2Vec-kel szemben azonban nem a meghatározott kontextus-ablakban (context window) megjelen kifejezések közti kapcsolatokat tárja fel, hanem a szöveg globális jellemzit igyekszik megragadni az egész szöveget jellemz együttes elfordulási gyakoriságok (co-occurrance) meghatározásával Pennington, Socher, and Manning (2014). Míg a Word2Vec modell prediktív jelleg, addig a GloVe egy statisztikai alapú (count-based) modell, melyek gyakorlati hasznosításukat tekintve nagyon hasonlóak. A szóvektor modellek között érdemes megemlíteni a fastText-et is, mely 157 nyelvre (köztük a magyarra is) kínál a szóbeágyazás módszerén alapuló, elre tanított szóvektorokat, melyet tovább lehet tanítani speciális szövegkorpuszokra, ezzel jelentsen lerövidítve a modell tanításához szükséges id- és kapacitásszükségletet (Mikolov et al. (2018)). Habár a GloVe és Word2Vec skip-gram módszerek hasonlóságát a szakirodalom adottnak veszi, a tényleges kép ennél árnyaltabb. A GloVe esetében a ritkán elforduló szavak kisebb súlyt kapnak a szóvektorok számításánál, míg a Word2Vec alulsúlyozza a nagy frekvenciájú szavakat. Ennek a következménye, hogy a Word2Vec esetében gyakori, hogy a szemantikailag legközelebbi szó az egy elütés, nem pedig valid találat. Ennek ellenére a két módszer (amennyiben a Word2Vec algoritmusnál a kisfrekvenciájú tokeneket kiszrjük) az emberi validálás során nagyon hasonló eredményeket hozott (Spirling and Rodriguez 2021). A fejezetben a gyakorlati példa során a GloVe algoritmust használjuk majd, mivel véleményünk szerint jobb és könnyebben követhet a dokumentációja az implementációt tartalmazó R csomagnak, mint a többi alternatívának. 8.2.1 GloVe használata magyar média korpuszon Az elemzéshez a text2vec csomagot használjuk, ami a GloVe implementációt tartalmazza (Selivanov, Bickel, and Wang 2020). A lenti kód a csomag dokumentáción alapul és a Társadalomtudományi Kutatóközpont által a Hungarian Comparative Agendas Project (CAP) adatbázisában tárolt Magyar Nemzet korpuszt használja.45 library(text2vec) library(quanteda) library(quanteda.textstats) library(readtext) library(readr) library(dplyr) library(tibble) library(stringr) library(ggplot2) library(plotly) library(HunMineR) A lenti kód blokk azt mutatja be, hogyan kell a betöltött korpuszt tokenizálni és mátrix formátumba alakítani. A korpusz a Magyar Nemzet 2004 és 2014 közötti címlapos cikkeit tartalmazza. Az eddigi elkészít lépéseket most is megtesszük: kitöröljük a központozást, a számokat, a magyar töltelékszavakat, illetve kisbetsítünk és eltávolítjuk a felesleges szóközöket és töréseket. mn &lt;- HunMineR::data_magyar_nemzet_large mn_clean &lt;- mn %&gt;% mutate( text = str_remove_all(string = text, pattern = &quot;[:cntrl:]&quot;), text = str_remove_all(string = text, pattern = &quot;[:punct:]&quot;), text = str_remove_all(string = text, pattern = &quot;[:digit:]&quot;), text = str_to_lower(text), text = str_trim(text), text = str_squish(text) ) A glimpse funkció segítségével belepillanthatunk mind a két korpuszba és láthatjuk, hogy sikeres volt a tisztítása, valamint azt is, hogy jelenleg egyetlen metaadatunk a dokumentumok azonosítója. glimpse(mn) #&gt; Rows: 35,021 #&gt; Columns: 2 #&gt; $ doc_id &lt;chr&gt; &quot;mn_2002_05_27_00.txt&quot;, &quot;mn_2002_05_27_01.txt&quot;, &quot;mn_2~ #&gt; $ text &lt;chr&gt; &quot;Csere szerb módra\\nNagy vihart kavart Szerbiában a k~ glimpse(mn_clean) #&gt; Rows: 35,021 #&gt; Columns: 2 #&gt; $ doc_id &lt;chr&gt; &quot;mn_2002_05_27_00.txt&quot;, &quot;mn_2002_05_27_01.txt&quot;, &quot;mn_2~ #&gt; $ text &lt;chr&gt; &quot;csere szerb módranagy vihart kavart szerbiában a kor~ Fontos különbség, hogy az eddigi munkafolyamatokkal ellentétben a GloVe algoritmus nem egy dokumentum-kifejezés mátrixon dolgozik, hanem egy kifejezések együttes elfordulását tartalmazó mátrixot (feature co-occurence matrix) kell készíteni inputként. Ezt a quanteda fcm() függvényével tudjuk elállítani, ami a tokenekbl készíti el a mátrixot. A tokenek sorrendiségét úgy tudjuk megrizni, hogy egy dfm objektumból csak a kifejezéseket tartjuk meg a featnames() függvény segítségével, majd a teljes token halmazból a tokens_select() függvénnyel kiválasztjuk ket. mn_corpus &lt;- corpus(mn_clean) mn_tokens &lt;- tokens(mn_corpus) %&gt;% tokens_remove(stopwords(language = &quot;hungarian&quot;)) features &lt;- dfm(mn_tokens) %&gt;% dfm_trim(min_termfreq = 5) %&gt;% quanteda::featnames() mn_tokens &lt;- tokens_select(mn_tokens, features, padding = TRUE) Az fcm megalkotása során a célkifejezéstl való távolság függvényében súlyozzuk a tokeneket. mn_fcm &lt;- quanteda::fcm(mn_tokens, context = &quot;window&quot;, count = &quot;weighted&quot;, weights = 1 / (1:5), tri = TRUE) A tényleges szóbeágyazás a text2vec csomaggal történik. A GlobalVector egy új környezetet (environment) hoz létre. Itt adhatjuk meg az alapvet paramétereket. A rank a vektor dimenziót adja meg (a szakirodalomban a 300500 dimenzió a megszokott). A többi paraméterrel is lehet kísérletezni, hogy mennyire változtatja meg a kapott szóbeágyazásokat. A fit_transform pedig a tényleges becslést végzi. Itt az iterációk számát (a gépi tanulásos irodalomban epoch-nak is hívják a tanulási köröket) és a korai leállás (early stopping) kritériumát a convergence_tol megadásával állíthatjuk be. Minél több dimenziót szeretnénk és minél több iterációt, annál tovább fog tartani a szóbeágyazás futtatása. Az egyszerség és a gyorsaság miatt a lenti kód 10 körös tanulást ad meg, ami a relatíve kicsi Magyar Nemzet korpuszon ~3 perc alatt fut le.46 Természetesen minél nagyobb korpuszon, minél több iterációt futtatunk, annál pontosabb eredményt fogunk kapni. A text2vec csomag képes a számítások párhuzamosítására, így alapbeállításként a rendelkezésre álló összes CPU magot teljesen kihasználja a számításhoz. Ennek ellenére egy százezres, milliós korpusz esetén több óra is lehet a tanítás. glove &lt;- GlobalVectors$new(rank = 300, x_max = 10, learning_rate = 0.1) mn_main &lt;- glove$fit_transform(mn_fcm, n_iter = 10, convergence_tol = 0.1) #&gt; INFO [22:31:46.136] epoch 1, loss 0.2295 #&gt; INFO [22:32:08.468] epoch 2, loss 0.0963 #&gt; INFO [22:32:30.136] epoch 3, loss 0.0706 #&gt; INFO [22:32:53.723] epoch 4, loss 0.0490 #&gt; INFO [22:33:15.566] epoch 5, loss 0.0411 #&gt; INFO [22:33:40.096] epoch 6, loss 0.0362 #&gt; INFO [22:34:08.383] epoch 7, loss 0.0326 #&gt; INFO [22:34:31.161] epoch 8, loss 0.0298 #&gt; INFO [22:34:31.163] Success: early stopping. Improvement at iterartion 8 is less then convergence_tol A végleges szóvektorokat a becslés során elkészült két mátrix összegeként kapjuk. mn_context &lt;- glove$components mn_word_vectors &lt;- mn_main + t(mn_context) Az egyes szavakhoz legközelebb álló szavakat a koszinusz hasonlóság alapján kapjuk, a sim2() függvénnyel. A lenti példában l2 normalizálást alkalmazunk, majd a kapott hasonlósági vektort csökken sorrendbe rendezzük. Példaként a polgármester szónak a környezetét nézzük meg. Mivel a korpuszunk egy politikai napilap, ezért nem meglep, hogy a legközelebbi szavak a politikához kapcsolódnak. teszt &lt;- mn_word_vectors[&quot;polgármester&quot;, , drop = F] cos_sim_rom &lt;- text2vec::sim2(x = mn_word_vectors, y = teszt, method = &quot;cosine&quot;, norm = &quot;l2&quot;) head(sort(cos_sim_rom[, 1], decreasing = TRUE), 5) #&gt; polgármester mszps szocialista fideszes politikus #&gt; 1.000 0.518 0.509 0.463 0.407 A lenti show_vector() függvényt definiálva a kapott eredmény egy data frame lesz, és az n változtatásával a kapcsolódó szavak számát is könnyen változtathatjuk. show_vector &lt;- function(vectors, pattern, n = 5) { term &lt;- mn_word_vectors[pattern, , drop = F] cos_sim &lt;- sim2(x = vectors, y = term, method = &quot;cosine&quot;, norm = &quot;l2&quot;) cos_sim_head &lt;- head(sort(cos_sim[, 1], decreasing = TRUE), n) output &lt;- enframe(cos_sim_head, name = &quot;term&quot;, value = &quot;dist&quot;) return(output) } Példánkban láthatjuk, hogy a barack szó beágyazásának eredménye nem gyümölcsöt fog adni, hanem az Egyesült Államok elnökét és a hozzá kapcsolódó szavakat. show_vector(mn_word_vectors, &quot;barack&quot;, 10) #&gt; # A tibble: 10 x 2 #&gt; term dist #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 barack 1 #&gt; 2 obama 0.726 #&gt; 3 amerikai 0.427 #&gt; 4 elnök 0.394 #&gt; 5 demokrata 0.385 #&gt; 6 republikánus 0.280 #&gt; # ... with 4 more rows Ugyanez mködik magyar vezetkkel is. show_vector(mn_word_vectors, &quot;orbán&quot;, 10) #&gt; # A tibble: 10 x 2 #&gt; term dist #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 orbán 1 #&gt; 2 viktor 0.930 #&gt; 3 miniszterelnök 0.764 #&gt; 4 mondta 0.698 #&gt; 5 kormányfo 0.685 #&gt; 6 fidesz 0.679 #&gt; # ... with 4 more rows A szakirodalomban klasszikus vektormveletes példákat is reprokuálni tudjuk a Magyar Nemzet korpuszon készített szóbeágyazásainkkal. A budapest - magyarország + német + németország eredményét úgy kapjuk meg, hogy az egyes szavakhoz tartozó vektorokat kivonjuk egymásból, illetve hozzáadjuk ket, ezután pedig a kapott mátrixon a quanteda csomag textstat_simil() függvényével kiszámítjuk az új hasonlósági értékeket. budapest &lt;- mn_word_vectors[&quot;budapest&quot;, , drop = FALSE] - mn_word_vectors[&quot;magyarország&quot;, , drop = FALSE] + mn_word_vectors[&quot;német&quot;, , drop = FALSE] + + mn_word_vectors[&quot;németország&quot;, , drop = FALSE] cos_sim &lt;- textstat_simil(x = as.dfm(mn_word_vectors), y = as.dfm(budapest), method = &quot;cosine&quot;) head(sort(cos_sim[, 1], decreasing = TRUE), 5) #&gt; budapest németország német airport kancellár #&gt; 0.602 0.558 0.545 0.424 0.396 A szavak egymástól való távolságát vizuálisan is tudjuk ábrázolni. Az egyik ezzel kapcsolatban felmerül probléma, hogy egy 2 dimenziós ábrán akarunk egy 3500 dimenziós mátrixot ábrázolni. Több lehetséges megoldás is van, mi ezek közül a lehet legegyszerbbet mutatjuk be.47 Els lépésben egy data frame-et készítünk a szóbeágyazás eredményeként kapott mátrixból, megtartva a szavakat az els oszlopban a tibble csomag rownames_to_column() függvényével. Mivel csak 2 dimenziót tudunk ábrázolni egy tradícionális statikus ábrán, ezért a V1 és V2 oszlopokat tartjuk csak meg, amik az els és második dimenziót reprezentálják. mn_embedding_df &lt;- as.data.frame(mn_word_vectors[, c(1:2)]) %&gt;% tibble::rownames_to_column(var = &quot;words&quot;) Ezután pedig a ggplot() függvényt felhasználva definiálunk egy új, embedding_plot() nev, függvényt, ami az elkészült data frame alapján bármilyen kulcsszó kombinációt képes ábrázolni. embedding_plot &lt;- function(data, keywords) { data %&gt;% filter(words %in% keywords) %&gt;% ggplot(aes(V1, V2, label = words)) + labs( x = &quot;Els dimenzió&quot;, y = &quot;Második dimenzió&quot; ) + geom_text() + xlim(-1, 1) + ylim(-1, 1) } Példaként néhány településnevet megvizsgálva, azt látjuk, hogy a megadott szavak, jelen esetben budapest, debrecen, washington, moszkva milyen közel vagy távol vannak egymástól, vagyis milyen gyakorisággal fordulnak el ugyanazon szavak társaságában. A magyar városok közel helyezkednek el egymáshoz, ám washington és moszkva távolsága nagyobb. Ennek az oka az lehet hogy a két magyar nagyváros gyakrabban szerepel hasonló kontextusban a belföldi hírekben, míg a két külföldi fváros valószínleg eltér külpolitikai környezetben jelenik meg. words_selected &lt;- c(&quot;moszkva&quot;, &quot;debrecen&quot;, &quot;budapest&quot;, &quot;washington&quot;) embedded &lt;- embedding_plot(data = mn_embedding_df, keywords = words_selected) ggplotly(embedded) Ábra 8.1: Kiválasztott szavak két dimenzós térben Egy kiváló tanulmányban Spirling and Rodriguez (2021) (könyvünk írásakor még nem jelent meg) összehasonlítják a Word2Vec és GloVe módszereket, különböz paraméterekkel, adatbázisokkal. Azoknak, akiket komolyabban érdekelnek a szóbeágyazás gyakorlati alkalmazásának a részletei, mindenképp ajánljuk elolvasásra. A Magyar CAP Project által kezelt adatbázisok regisztrációt követen elérhetek az elábbi linken: https://cap.tk.hu/adatbazisok. A text2vec csomag dokumentációja: https://cran.r-project.org/web/packages/text2vec/vignettes/glove.html A futtatásra használt PC konfiguráció: CPU: Intel Core i5-4460 (3.2GHz); RAM: 16GB Az egyik legelterjedtebb dimenzionalitás csökkent eljárás a szakirodalomban a fkomponens-analízis (principal component analysis), illetve szintén gyakran használt az irodalomban az úgynevezett t-SNE (t-distributed stochastic neighbor embedding). "],["scaling.html", "9 Szövegskálázás 9.1 Fogalmi alapok 9.2 Wordfish 9.3 Wordscores", " 9 Szövegskálázás 9.1 Fogalmi alapok A politikatudomány egyik izgalmas kérdése a szereplk ideológiai skálákon való elhelyezése. Ezt jellemzen pártprogramok vagy különböz ügyekkel kapcsolatos álláspontpontok alapján szokták meghatározni, de a politikusok beszédei is alkalmasak arra, hogy meghatározzuk a beszél ideológiai hovatartozását. A szövegbányászat területén jellemzen a wordfish és a wordscores módszert alkalmazzák erre a feladatra. Míg elbbi a felügyelet nélküli módszerek sorába tartozik, utóbbi felügyelt módszerek közé. A wordscores a szótári módszerekhez hasonlóan a szövegeket a bennük található szavak alapján helyezi el a politikai térben oly módon, hogy az ún. referenciadokumentumok szövegét használja tanító halmazként. A wordscores kiindulópontja, hogy pozíció pontszámokat kell rendelni referencia szövegekhez. A modell számításba veszi a szövegek szavainak súlyozott gyakoriságát és a pozíciópontszám, valamint a szógyakoriság alapján becsüli meg a korpuszban lév többi dokumentum pozícióját (Laver, Benoit, and Garry 2003). A felügyelet nélküli wordfish módszer a skálázás során nem a referencia dokumentumokra támaszkodik, hanem olyan kifejezéseket keres a szövegben, amelyek megkülönböztetik egymástól a politikai spektrum különböz pontjain elhelyezked beszélket. Az IRT-n (item response theory) alapuló módszer azt feltételezi, hogy a politikusok egy kevés dimenziós politikai térben mozognak, amely tér leírható az i politikus \\(\\theta_1\\) paraméterével. Egy politikus (vagy párt) ebben a térben elfoglalt helyzete pedig befolyásolja a szavak szövegekben történ használatát. A módszer erssége, hogy kevés erforrás-befektetéssel megbízható becsléseket ad, ha a szövegek valóban az ideológiák mentén különböznek, tehát ha a szereplk ersen ideológiai tartalamú diskurzust folytatnak. Alkalmazásakor azonban tudnunk kell: a módszer nem képes kezelni, hogy a szövegek között nem csak ideológiai különbség lehet. Mivel a modell nem felügyelt, ezért nehéz garantálni, hogy valóban megbízhatóan azonosítja a szereplk elhelyezkedését a politikai térben, így az eredményeket mindenképpen körültekinten kell validálni (Grimmer and Stewart 2013; Hjorth et al. 2015; Slapin and Proksch 2008). library(readr) library(dplyr) library(stringr) library(ggplot2) library(ggrepel) library(quanteda) library(quanteda.textmodels) library(plotly) library(HunMineR) A skálázási algoritmusokat egy kis korpuszon mutatjuk be. A minta dokumentumok a 20142018-as parlamenti ciklusban az Országgylésben frakcióvezet politikusok egy-egy véletlenszeren kiválasztott napirend eltti felszólalásai. Ebben a ciklusban összesen 11 frakcióvezetje volt a két kormánypárti és öt ellenzéki frakciónak.48 A dokumentumokon elször elvégeztük a szokásos elkészítési lépéseket. parl_beszedek &lt;- HunMineR::data_parlspeakers_small beszedek_tiszta &lt;- parl_beszedek %&gt;% mutate( text = str_remove_all(string = text, pattern = &quot;[:cntrl:]&quot;), text = str_remove_all(string = text, pattern = &quot;[:punct:]&quot;), text = str_remove_all(string = text, pattern = &quot;[:digit:]&quot;), text = str_to_lower(text), text = str_trim(text), text = str_squish(text) ) A glimpse funkció segítségével ismét megtekinthetjük, mint az eredeti szöveget és a tisztított is, ezzel nem csak azt tudjuk ellenrizni, hogy a tisztítás sikeres volt-e, hanem a metaadatokat is megnézhetjük, amelyek jelenesetben a felszólalás azonosító száma a felszólaló neve, valamint a pártja. glimpse(parl_beszedek) #&gt; Rows: 10 #&gt; Columns: 4 #&gt; $ id &lt;chr&gt; &quot;20142018_024_0002_0002&quot;, &quot;20142018_055_0002_0002~ #&gt; $ text &lt;chr&gt; &quot;VONA GÁBOR (Jobbik): Tisztelt Elnök Úr! Tisztelt~ #&gt; $ felszolalo &lt;chr&gt; &quot;Vona Gábor (Jobbik)&quot;, &quot;Dr. Schiffer András (LMP)~ #&gt; $ part &lt;chr&gt; &quot;Jobbik&quot;, &quot;LMP&quot;, &quot;LMP&quot;, &quot;MSZP&quot;, &quot;LMP&quot;, &quot;MSZP&quot;, &quot;J~ glimpse(beszedek_tiszta) #&gt; Rows: 10 #&gt; Columns: 4 #&gt; $ id &lt;chr&gt; &quot;20142018_024_0002_0002&quot;, &quot;20142018_055_0002_0002~ #&gt; $ text &lt;chr&gt; &quot;vona gábor jobbik tisztelt elnök úr tisztelt ors~ #&gt; $ felszolalo &lt;chr&gt; &quot;Vona Gábor (Jobbik)&quot;, &quot;Dr. Schiffer András (LMP)~ #&gt; $ part &lt;chr&gt; &quot;Jobbik&quot;, &quot;LMP&quot;, &quot;LMP&quot;, &quot;MSZP&quot;, &quot;LMP&quot;, &quot;MSZP&quot;, &quot;J~ A wordfish és wordscores algoritmus is ugyanazt a kiinduló korpuszt és dfm objektumot használja, amit a szokásos módon a quanteda csomag corpus() függvényével hozunk létre. beszedek_corpus &lt;- corpus(beszedek_tiszta) summary(beszedek_corpus) #&gt; Corpus consisting of 10 documents, showing 10 documents: #&gt; #&gt; Text Types Tokens Sentences id #&gt; text1 442 819 1 20142018_024_0002_0002 #&gt; text2 354 607 1 20142018_055_0002_0002 #&gt; text3 426 736 1 20142018_064_0002_0002 #&gt; text4 314 538 1 20142018_115_0002_0002 #&gt; text5 354 589 1 20142018_158_0002_0002 #&gt; text6 333 538 1 20142018_172_0002_0002 #&gt; text7 344 559 1 20142018_206_0002_0002 #&gt; text8 352 628 1 20142018_212_0002_0002 #&gt; text9 317 492 1 20142018_236_0002_0002 #&gt; text10 343 600 1 20142018_249_0002_0002 #&gt; felszolalo part #&gt; Vona Gábor (Jobbik) Jobbik #&gt; Dr. Schiffer András (LMP) LMP #&gt; Dr. Szél Bernadett (LMP) LMP #&gt; Tóbiás József (MSZP) MSZP #&gt; Schmuck Erzsébet (LMP) LMP #&gt; Dr. Tóth Bertalan (MSZP) MSZP #&gt; Volner János (Jobbik) Jobbik #&gt; Kósa Lajos (Fidesz) Fidesz #&gt; Harrach Péter (KDNP) KDNP #&gt; Dr. Gulyás Gergely (Fidesz) Fidesz A leíró statisztikai táblázatban látszik, hogy a beszédek hosszúsága nem egységes, a leghosszabb 819, a legrövidebb pedig 492 szavas. Az átlagos dokumentum hossz az 611 szó. A korpusz szemléltet célú, alaposabb elemzéshez hosszabb és/vagy több dokumentummal érdemes dolgoznunk. A korpusz létrehozása után elkészítjük a dfm mátrixot, amelybl eltávolítjuk a magyar stopszvakat a quanteda beépített szótára segítségével. beszedek_dfm &lt;- beszedek_corpus %&gt;% tokens() %&gt;% tokens_remove(stopwords(&quot;hungarian&quot;)) %&gt;% dfm() 9.2 Wordfish A wordfish felügyelet nélküli skálázást a quanteda.textmodels csomagban implementált textmodel_wordfish() függvény fogja végezni. A megadott dir = c(1, 2) paraméterrel a két dokumentum relatív \\(\\theta\\) értékét tudjuk rögzíteni, mégpedig úgy hogy \\(\\theta_{dir1} &lt; \\theta_{dir2}\\). Alapbeállításként az algoritmus az els és az utolsó dokumentumot teszi be ide. A lenti példánál mi a pártpozíciók alapján a Jobbikos Vona Gábor és az LMP-s Schiffer András egy-egy beszédét használtuk. A summary() használható az illesztett modellel, és a dokumentumonkénti \\(\\theta\\) koefficienst tudjuk így megnézni. beszedek_wf &lt;- quanteda.textmodels::textmodel_wordfish(beszedek_dfm, dir = c(2, 1)) summary(beszedek_wf) #&gt; #&gt; Call: #&gt; textmodel_wordfish.dfm(x = beszedek_dfm, dir = c(2, 1)) #&gt; #&gt; Estimated Document Positions: #&gt; theta se #&gt; text1 1.7947 0.0422 #&gt; text2 0.0893 0.0400 #&gt; text3 1.0014 0.0391 #&gt; text4 -0.0999 0.0423 #&gt; text5 0.7360 0.0436 #&gt; text6 0.1857 0.0445 #&gt; text7 -0.7283 0.0359 #&gt; text8 -0.8059 0.0336 #&gt; text9 -0.5203 0.0400 #&gt; text10 -1.6527 0.0379 #&gt; #&gt; Estimated Feature Scores: #&gt; vona gábor jobbik tisztelt elnök úr országgyulés tegnapi #&gt; beta 3.68 2.32 1.971 0.239 -0.1115 0.0275 1.229 4.37 #&gt; psi -4.98 -2.73 -0.753 0.457 -0.0569 0.2872 -0.671 -5.31 #&gt; napon helyen tartottak idoközi önkormányzati választásokat #&gt; beta 2.99 3.10 3.68 3.68 3.68 3.68 #&gt; psi -3.01 -2.63 -4.98 -4.98 -4.98 -4.98 #&gt; két érdekelt recsken ózdon október nyertünk örömmel közlöm #&gt; beta 1.189 3.68 4.37 4.77 3.40 3.68 3.68 3.68 #&gt; psi -0.944 -4.98 -5.31 -5.54 -3.23 -4.98 -4.98 -4.98 #&gt; ország közvéleményével amúgy is tudnak mindkét jobbikos #&gt; beta 1.747 3.68 3.68 0.913 1.43 3.68 3.68 #&gt; psi -0.364 -4.98 -4.98 1.834 -1.74 -4.98 -4.98 #&gt; polgármester #&gt; beta 3.68 #&gt; psi -4.98 Amennyiben szeretnénk a szavak szintjén is megnézni a \\(\\beta\\) (a szavakhoz társított súly, ami a relatív fontosságát mutatja) és \\(\\psi\\) (a szó rögzített hatást (word fixed effects), ami az eltér szófrekvencia kezeléséért felels) koefficienseket, akkor a beszedek_wf objektumban tárolt értékeket egy data frame-be tudjuk bemásolni. A dokumentumok hosszát és a szófrekvenciát figyelembe véve, a negatív \\(\\beta\\) érték szavakat gyakrabban használják a negatív \\(\\theta\\) koefficienssel rendelkez politikusok. szavak_wf &lt;- data.frame( word = beszedek_wf$features, beta = beszedek_wf$beta, psi = beszedek_wf$psi ) szavak_wf %&gt;% arrange(beta) %&gt;% head(n = 15) #&gt; word beta psi #&gt; 1 czeglédy -5.90 -6.22 #&gt; 2 csaba -5.77 -6.15 #&gt; 3 human -5.44 -5.98 #&gt; 4 operator -5.44 -5.98 #&gt; 5 zrt -5.22 -5.86 #&gt; 6 fizette -4.93 -5.72 #&gt; 7 gyanú -4.93 -5.72 #&gt; 8 szocialista -4.93 -5.72 #&gt; 9 elkövetett -4.51 -5.52 #&gt; 10 tárgya -4.51 -5.52 #&gt; 11 céghálózat -4.51 -5.52 #&gt; 12 diákok -4.51 -5.52 #&gt; 13 májusi -4.51 -5.52 #&gt; 14 júniusi -4.51 -5.52 #&gt; 15 büntetoeljárás -4.51 -5.52 Ez a pozitív értékekre is igaz. szavak_wf %&gt;% arrange(desc(beta)) %&gt;% head(n = 15) #&gt; word beta psi #&gt; 1 nemzetközi 5.06 -5.72 #&gt; 2 önöknek 4.98 -4.78 #&gt; 3 ózdon 4.77 -5.54 #&gt; 4 kétharmados 4.77 -5.54 #&gt; 5 igenis 4.77 -5.54 #&gt; 6 választási 4.77 -5.54 #&gt; 7 geopolitikai 4.77 -5.54 #&gt; 8 ártatlanság 4.77 -5.54 #&gt; 9 vélelme 4.77 -5.54 #&gt; 10 tegnapi 4.37 -5.31 #&gt; 11 recsken 4.37 -5.31 #&gt; 12 lássuk 4.37 -5.31 #&gt; 13 tolünk 4.37 -5.31 #&gt; 14 janiczak 4.37 -5.31 #&gt; 15 szavazattal 4.37 -5.31 Az eredményeinket mind a szavak, mind a dokumentumok szintjén tudjuk vizualizálni. Elsként a klasszikus Eiffel-torony ábrát reprodukáljuk, ami a szavak gyakoriságának és a skálára gyakorolt befolyásának az illusztrálására szolgál. Ehhez a már elkészült szavak_wf data framet-et és a ggplot2 csomagot fogjuk használni. Mivel a korpuszunk nagyon kicsi, ezért csak 2410 kifejezést fogunk ábrázolni. Ennek ellenére a lényeg kirajzolódik a lenti ábrán is.49 Kihasználhatjuk, hogy a ggplot ábra definiálása közben a felhasznált bemeneti data frame-et különböz szempontok alapján lehet szrni. Így ábrázolni tudjuk a gyakran használt, ám semleges szavakat (magas \\(\\psi\\), alacsony \\(\\beta\\)), illetve a ritkább, de meghatározóbb szavakat (magas \\(\\beta\\), alacsony \\(\\psi\\)). ggplot(szavak_wf, aes(x = beta, y = psi)) + geom_point(color = &quot;grey&quot;) + geom_text_repel( data = filter(szavak_wf, beta &gt; 4.5 | beta &lt; -5 | psi &gt; 0), aes(beta, psi, label = word), alpha = 0.7 ) + labs( x = expression(beta), y = expression(psi) ) Ábra 9.1: A wordfish Eiffel-torony Az így kapott ábrán az egyes pontok mind egy szót reprezentálnak, láthatjuk, hogy tipikusan minél magasabb a \\(\\psi\\) értékük annál inkább középen helyezkednek el hiszen a leggyakoribb szavak azok, amelyeket mindenki használ politikai spektrumon való elhelyezkedésüktl függetlenül. Az ábra két szélén lév szavak azok, amelyek specifikusan a skála egy-egy végpontjához kötdnek. Jelen esetben ezek kevésbé beszédések, mivel a korpusz kifejezetten kis méret és láthatóan további stopszavazás is szükséges. A dokumentumok szintjén is érdemes megvizsgálni az eredményeket. Ehhez a dokumentum szint paramétereket fogjuk egy data frame-be gyjteni: a \\(\\theta\\) ideológiai pozíciót, illetve a beszél nevét. A vizualizáció kedvéért a párttagságot is hozzáadjuk. A data frame összerakása után az alsó és a fels határát is kiszámoljuk a konfidencia intervallumnak és azt is ábrázoljuk (ld. 9.1. ábra). dokumentumok_wf &lt;- data.frame( speaker = beszedek_wf$x@docvars$felszolalo, part = beszedek_wf$x@docvars$part, theta = beszedek_wf$theta, theta_se = beszedek_wf$se.theta ) %&gt;% mutate( lower = theta - 1.96 * theta_se, upper = theta + 1.96 * theta_se ) ggplot(dokumentumok_wf, aes(theta, reorder(speaker, theta))) + geom_point() + geom_errorbarh(aes(xmin = lower, xmax = upper), height = 0) + labs( y = NULL, x = expression(theta) ) Ábra 9.2: A beszédek egymáshoz viszonyított pozíciója A párt metaadattal összehasonlíthatjuk az egy párthoz tartozó frakcióvezetk értékeit a facet_wrap() használatával. Figyeljünk arra, hogy az y tengelyen szabadon változhasson az egyes rész ábrák között, a scales = \"free\" opcióval (ld. 9.2. ábra). speech_df &lt;- ggplot(dokumentumok_wf, aes(theta, reorder(speaker, theta))) + geom_point() + geom_errorbarh(aes(xmin = lower, xmax = upper), height = 0) + labs( y = NULL, x = &quot;wordscores&quot; ) + facet_wrap(~part, ncol = 1, scales = &quot;free_y&quot;) ggplotly(speech_df, height = 1000, tooltip = &quot;theta&quot;) Ábra 9.3: Párton belüli pozíciók 9.3 Wordscores A modell illesztést a wordfish-hez hasonlóan a quanteda.textmodels csomagban található textmodel_wordscores() függvény végzi. A kiinduló dfm ugyanaz, mint amit a fejezet elején elkészítettünk, a beszedek_dfm. A referencia pontokat dokumentumváltozóként hozzáadjuk a dfm-hez (a refrencia_pont oszlopot, ami NA értéket kap alapértelmezetten). A kiválasztott referencia dokumentumoknál pedig egyenként hozzáadjuk az értékeket. Erre több megoldás is van, az egyszerbb út, hogy az egyik és a másik végletet a -1; 1 intervallummal jelöljük. Ennek a lehetséges alternatívája, hogy egy küls, már validált forrást használunk. Pártok esetén ilyen lehet a Chapel Hill szakérti kérdívének a pontszámai, a Manifesto projekt által kódolt jobb-bal (rile) dimenzió. A lenti példánál mi maradunk az egyszerbb bináris kódolásnál (ld. 9.3. ábra). A wordfish eredményt alapul véve a két referencia pont Gulyás Gergely és Szél Bernadett beszédei lesznek.50 Ezek a 3. és a 10. dokumentumok. Miután a referencia pontokat hozzárendeltünk az adattáblához szintén a docvars funkcióval meg is tekinthetjük azt és láthatjuk, hogy a referenci_pont már a metaadatok között szerepel. docvars(beszedek_dfm, &quot;referencia_pont&quot;) &lt;- NA docvars(beszedek_dfm, &quot;referencia_pont&quot;)[3] &lt;- -1 docvars(beszedek_dfm, &quot;referencia_pont&quot;)[10] &lt;- 1 docvars(beszedek_dfm) #&gt; id felszolalo part #&gt; 1 20142018_024_0002_0002 Vona Gábor (Jobbik) Jobbik #&gt; 2 20142018_055_0002_0002 Dr. Schiffer András (LMP) LMP #&gt; 3 20142018_064_0002_0002 Dr. Szél Bernadett (LMP) LMP #&gt; 4 20142018_115_0002_0002 Tóbiás József (MSZP) MSZP #&gt; 5 20142018_158_0002_0002 Schmuck Erzsébet (LMP) LMP #&gt; 6 20142018_172_0002_0002 Dr. Tóth Bertalan (MSZP) MSZP #&gt; 7 20142018_206_0002_0002 Volner János (Jobbik) Jobbik #&gt; 8 20142018_212_0002_0002 Kósa Lajos (Fidesz) Fidesz #&gt; 9 20142018_236_0002_0002 Harrach Péter (KDNP) KDNP #&gt; 10 20142018_249_0002_0002 Dr. Gulyás Gergely (Fidesz) Fidesz #&gt; referencia_pont #&gt; 1 NA #&gt; 2 NA #&gt; 3 -1 #&gt; 4 NA #&gt; 5 NA #&gt; 6 NA #&gt; 7 NA #&gt; 8 NA #&gt; 9 NA #&gt; 10 1 A lenti wordscores-modell specifikáció követi a Laver, Benoit, and Garry (2003) tanulmányban leírtakat. beszedek_ws &lt;- textmodel_wordscores( x = beszedek_dfm, y = docvars(beszedek_dfm, &quot;referencia_pont&quot;), scale = &quot;linear&quot;, smooth = 0 ) summary(beszedek_ws, 10) #&gt; #&gt; Call: #&gt; textmodel_wordscores.dfm(x = beszedek_dfm, y = docvars(beszedek_dfm, #&gt; &quot;referencia_pont&quot;), scale = &quot;linear&quot;, smooth = 0) #&gt; #&gt; Reference Document Statistics: #&gt; score total min max mean median #&gt; text1 NA 486 0 18 0.202 0 #&gt; text2 NA 395 0 12 0.164 0 #&gt; text3 -1 439 0 12 0.182 0 #&gt; text4 NA 330 0 7 0.137 0 #&gt; text5 NA 360 0 8 0.149 0 #&gt; text6 NA 328 0 5 0.136 0 #&gt; text7 NA 349 0 5 0.145 0 #&gt; text8 NA 387 0 10 0.161 0 #&gt; text9 NA 307 0 13 0.127 0 #&gt; text10 1 383 0 8 0.159 0 #&gt; #&gt; Wordscores: #&gt; (showing first 10 elements) #&gt; tisztelt elnök úr országgyulés ország #&gt; -0.0755 0.3925 0.0681 0.0681 -1.0000 #&gt; is sot nemhogy tette fidesz #&gt; -0.1986 -1.0000 -1.0000 -1.0000 1.0000 Az illesztett wordscores modellünkkel ezek után már meg tudjuk becsülni a korpuszban lév többi dokumentum pozícióját. Ehhez a predict() függvény megoldását használjuk. A kiegészít opciókkal a konfidencia intervallum alsó és fels határát is meg tudjuk becsülni, ami jól jön akkor, ha szeretnénk ábrázolni az eredményt. beszedek_ws_pred &lt;- predict( beszedek_ws, newdata = beszedek_dfm, interval = &quot;confidence&quot;) beszedek_ws_pred &lt;- as.data.frame(beszedek_ws_pred$fit) beszedek_ws_pred #&gt; fit lwr upr #&gt; text1 -0.48986 -0.6214 -0.3583 #&gt; text2 -0.23461 -0.3966 -0.0726 #&gt; text3 -0.90905 -0.9351 -0.8830 #&gt; text4 -0.29653 -0.4754 -0.1177 #&gt; text5 -0.25907 -0.4495 -0.0687 #&gt; text6 0.00632 -0.2306 0.2432 #&gt; text7 0.16504 -0.0614 0.3915 #&gt; text8 -0.07774 -0.2765 0.1210 #&gt; text9 -0.12399 -0.3118 0.0638 #&gt; text10 0.90905 0.8793 0.9388 A kapott modellünket a wordfish-hez hasonlóan tudjuk ábrázolni, miután a beszedek_ws_pred objektumból adattáblát csinálunk és a ggplot2-vel elkészítjük a vizualizációt. A dokumentumok_ws két részbl áll össze. Elször a wordscores modell objektumunkból a frakcióvezetk neveit és pártjaikat emeljük ki (kicsit körülményes a dolog, mert egy komplexebb objektumban tárolja ket a quanteda, de az str() függvény tud segíteni ilyen esetekben). A dokumentumok becsült pontszámait pedig a beszedek_ws_pred objektumból készített data frame hozzácsatolásával adjuk hozzá a már elkészült data frame-hez. Ehhez a dplyr csomag bind_cols függvényét használjuk. Fontos, hogy itt teljesen biztosnak kell lennünk abban, hogy a sorok a két data frame esetében ugyanarra a dokumentumra vonatkoznak. dokumentumok_ws &lt;- data.frame( speaker = beszedek_ws$x@docvars$felszolalo, part = beszedek_ws$x@docvars$part ) dokumentumok_ws &lt;- bind_cols(dokumentumok_ws, beszedek_ws_pred) dokumentumok_ws #&gt; speaker part fit lwr upr #&gt; text1 Vona Gábor (Jobbik) Jobbik -0.48986 -0.6214 -0.3583 #&gt; text2 Dr. Schiffer András (LMP) LMP -0.23461 -0.3966 -0.0726 #&gt; text3 Dr. Szél Bernadett (LMP) LMP -0.90905 -0.9351 -0.8830 #&gt; text4 Tóbiás József (MSZP) MSZP -0.29653 -0.4754 -0.1177 #&gt; text5 Schmuck Erzsébet (LMP) LMP -0.25907 -0.4495 -0.0687 #&gt; text6 Dr. Tóth Bertalan (MSZP) MSZP 0.00632 -0.2306 0.2432 #&gt; text7 Volner János (Jobbik) Jobbik 0.16504 -0.0614 0.3915 #&gt; text8 Kósa Lajos (Fidesz) Fidesz -0.07774 -0.2765 0.1210 #&gt; text9 Harrach Péter (KDNP) KDNP -0.12399 -0.3118 0.0638 #&gt; text10 Dr. Gulyás Gergely (Fidesz) Fidesz 0.90905 0.8793 0.9388 A 9.4-es ábrán a párton belüli bontást illusztráljuk a facet_wrap() segítségével. party_df &lt;- ggplot(dokumentumok_ws, aes(fit, reorder(speaker, fit))) + geom_point() + geom_errorbarh(aes(xmin = lwr, xmax = upr), height = 0) + labs( y = NULL, x = &quot;wordscores&quot; ) + facet_wrap(~part, ncol = 1, scales = &quot;free_y&quot;) ggplotly(party_df, height = 1000, tooltip = &quot;fit&quot;) Ábra 9.4: A párton belüli wordscores-alapú skála A mintába nem került be Rogán Antal, akinek csak egy darab napirend eltti felszólalása volt. A quanteda.textplots csomag több megoldást is kínál az ábrák elkészítésére. Mivel ezek a megoldások kifejezetten a quanteda elemzések ábrázolására készültek, ezért rövid egysoros függvényekkel tudunk gyorsan ábrákat készíteni. A hátrányuk, hogy kevésbé tudjuk személyre szabni az ábráinkat, mint a ggplot2 példák esetében. A quanteda.textplots megoldásokat ezen a linken demonstrálják a csomag készíti: https://quanteda.io/articles/pkgdown/examples/plotting.html. Azért nem Vona Gábor beszédét választottuk, mert az gyaníthatóan egy kiugró érték, ami nem reprezentálja megfelelen a sokaságot. "],["similarity.html", "10 Szövegösszehasonlítás 10.1 A szövegösszehasonlítás különböz megközelítései 10.2 Lexikális hasonlóság 10.3 Szemantikai hasonlóság 10.4 Hasonlóságszámítás 10.5 Szövegtisztítás 10.6 A Jaccard-hasonlóság számítása 10.7 A koszinusz-hasonlóság számítása 10.8 Az eredmények vizualizációja", " 10 Szövegösszehasonlítás 10.1 A szövegösszehasonlítás különböz megközelítései A gépi szövegösszehasonlítás a mindennapi életünk számos területén megjelen szövegbányászati technika, bár az emberek többsége nincs ennek tudatában. Ezen a módszeren alapulnak a böngészk keres mechanizmusai, vagy a kérdés-felelet (Q&amp;A) fórumok algoritmusai, melyek ellenrzik, hogy szerepel-e már a feltenni kívánt kérdés a fórumon (Sieg 2018). Alkalmazzák továbbá a szövegösszehasonlítást a gépi szövegfordításban és az automatikus kérdésmegválaszolási feladatok esetén is (Wang and Dong 2020), de akár automatizált esszéértékelésre vagy plágiumellenrzésre is hasznosítható az eljárás (Bar, Zesch, and Gurevych 2011). A szövegösszehasonlítás hétköznapi életben elforduló rejtett alkalmazásain túl a társadalomtudományok mveli is számos esetben hasznosítják az eljárást. A politikatudomány területén többek között használhatjuk arra, hogy eldöntsük, mennyire különböznek egymástól a benyújtott törvényjavaslatok és az elfogadott törvények szövegei, ezzel fontos információhoz jutva arról, hogy milyen szerepe van a parlamenti vitának a végleges törvények kialakításában. Egy másik példa a szakpolitikai prioritásokban és alapelvekben végbemen változások elemzése, melyet például szakpolitikai javaslatok vagy ilyen témájú viták leiratainak elemzésével is megtehetünk. A könyv korábbi fejezeteiben bemutatott eljárások között több olyat találunk, melyek alkalmasak arra, hogy a szövegek hasonlóságából valamilyen információt nyerjünk. Ugyanakkor vannak módszerek, melyek segítségével számszersíthetjük a szövegek közötti különbségeket. Ez a fejezet ezekrl nyújt rövid áttekintést. Mindenekeltt azonban azt kell tisztáznunk, hogy miként értelmezzük a hasonlóságot. A hasonlóságelemzéseket jellemzen két nagy kategóriába szoktuk sorolni a mérni kívánt hasonlóság típusa szerint. Ez alapján beszélhetünk lexikális (formai) és szemantikai hasonlóságról. 10.2 Lexikális hasonlóság A lexikális hasonlóság a gépi szövegfeldolgozás egy egyszerbb megközelítése, amikor nem várjuk el az elemzésünktl, hogy értse a szöveget, csupán a formai hasonlóságot figyeljük. A megközelítés elnye, hogy számítási szempontból jelentsen egyszerbb, mint a szemantikai hasonlóságra irányuló elemzések, hátránya azonban, hogy az egyszerség könnyen tévútra vihet szofisztikáltabb elemzések esetén. Így például a lexikális hasonlóság szempontjából az alábbi két példamondat azonosnak tekinthet, hiszen formailag (kifejezések szintjén) megegyeznek. 1. A boszorkány megsüti Jancsit és Juliskát. 2. Jancsi és Juliska megsüti a boszorkányt. Két dokumentum közötti lexikális hasonlóságot a szöveg számos szintjén mérhetjük: karakterláncok (stringek), szóalakok (tokenek), n-gramok (n egységbl álló karakterláncok), szózsákok (bag of words) között, de akár a dokumentum nagyobb egységei, így szövegrészletek és dokumentumok között is. Bevett megközelítés továbbá a szókészlet összehasonlítása, melyet lexikális és szemantikai hasonlóság feltárására egyaránt használhatunk. A hasonlóság számítására számos metrika létezik. Ezek jelents része valamilyen távolságszámításon alapul, mint például a koszinus-távolság. Ez a metrika két szövegvektor (a két dokumentum-kifejezés mátrix) által bezárt szög alapján határozza meg a hasonlóságot (Wang and Dong 2020). Mindezt az alábbi képlet szerint: \\[ cos(X,Y)=\\frac{X \\cdot Y}{\\|X\\| \\|Y\\|} \\] vagyis kiszámoljuk a két vektor skaláris szorzatát, amelyet elosztunk a vektorok Euklidészi normáinak (gyakran hívják L2 normának is, és ennek segítségével kapjuk meg a vektorok hosszát) szorzatával. Vegyük az alábbi két példamondatot a koszinusz távolság számításának szemléltetésére: 1. Jancsi és Juliska megsüti a boszorkányt. 2. A pék megsüti a kenyeret. A két példamondat (vagyis a dokumentumaink) dokumentum-kifejezés mátrixsza az alábbi táblázat szerint fog kinézni. Az X vektor reprezentálja az 1. példamondatot, az Y vektor pedig a második példamondatot. Táblázat 10.1: Dokumentum-kifejezés mátrix két példamondattal Vektor_név jancsi és juliska megsüti a boszokrkányt pék kenyeret X 1 1 1 1 1 1 0 0 Y 0 0 0 1 2 0 1 1 A két mondat közötti távolság értékét a képlet szerint a következ módon számítjuk ki: \\[ \\frac{x_{1}*y_{1}+x_{2}*y_{2}+x_{3}*y_{3}+x_{4}*y_{4}+x_{5}*y_{5}+x_{6}*y_{6}+x_{7}*y_{7}+x_{8}*y_{8}} {\\sqrt{x_{1}^2+x_{2}^2+x_{3}^2+x_{4}^2+x_{5}^2+x_{6}^2+x_{7}^2+x_{8}^2}* \\sqrt{y_{1}^2+y_{2}^2+y_{3}^2+y_{4}^2+y_{5}^2+y_{6}^2+y_{7}^2+y_{8}^2}} \\] A két példamondat koszinusz-távolságának értéke ennek megfelelen 0,463. \\[ \\frac{1*0+1*0+1*0+1*1+1*2+1*0+0*1+0*1} {\\sqrt{1^2+1^2+1^2+1^2+1^2+1^2+0^2+0^2}* \\sqrt{0^2+0^2+0^2+1^2+2^2+0^2+1^2+1^2}} = \\frac{3}{\\sqrt{6}*\\sqrt{7}}\\approx 0,463 \\] A koszinusz-hasonlóság 0 és 1 közötti értékeket vehet fel. 0-ás értéket akkor kapunk, ha a dokumentumok egyáltalán nem hasonlítanak egymásra. Geometriai értelmeben ebben az esetben a két szövegvektor 90 fokos szöget zár be, hiszen cos(90) = 0 (Ladd 2020). Egy másik széles körben alkalmazott dokumentumhasonlósági metrika a Jaccard-hasonlóság, melynek számítása egy egyszer eljáráson alapul: a két dokumentumban egyez szavak számát elosztja a két dokumentumban szerepl szavak számának uniójával (vagyis a két dokumentumban szerepl szavak számának összegével, melybl kivonja az egyez szavak számának összegét). A Jaccard-hasonlóság tehát azt képes megmutatni, hogy a két dokumentum teljes szószámához képest mekkora az azonos kifejezések aránya (Niwattanakul et al. 2013, 2.o). Ahogy a koszinusz-hasonlóságnál is, itt is 0 és 1 közötti értéket kapunk, ahol a magasabb érték nagyobb hasonlóságra utal. \\[ Jaccard(doc_{1}, doc_{2}) = \\frac{|doc_{1}\\,\\cap \\, doc_{2}|}{|doc1 \\, \\cup \\, doc2|} = \\frac{|doc_{1} \\, \\cap \\, doc_{2}|}{|doc_{1}| + |doc_{2}| - |doc_{1} \\, \\cap \\, doc_{2} |} \\] 10.3 Szemantikai hasonlóság A szemantikai hasonlóság a lexikai hasonlósággal szemben egy komplexebb számítás, melynek során az algoritmus a szavak tartalmát is képes elemezni. Így például formai szempontból hiába nem azonos az alábbi két példamondat, a szemantikai hasonlóságvizsgálatnak észlelnie kell a tartalmi azonosságot. 1. A diákok jegyzetelnek, amíg a professzor eladást tart. 2. A nebulók írnak, amikor az oktató beszél. A jelentésbeli hasonlóság kimutatására számos megközelítés létezik. Többek között alkalmazható a témamodellezés (topikmodellezés), melyet a Felügyelet nélküli tanulás fejezetben tárgyaltunk bvebben, ezen belül pedig az LDA Látens Dirichlet-Allokáció (Latent Dirichlet Allocation), valamint az LSA látens érzelemelemzés (Latent Sentiment Analysis) is nagyszer lehetséget kínál arra, hogy az egyes dokumentumainkat tartalmi hasonlóságok alapján csoportosítsuk. Az LSA-nél és az LDA-nél azonban egy fokkal komplexebb megközelítés a szóbeágyazás, melyet a Szóbeágyazások cím fejezetben mutattunk be. Ez a módszertan a témamodellezéshez képest a szöveg mélyebb szemantikai tartalmait is képes feltárni, hiszen a beágyazásnak köszönheten képes formailag különböz, de jelentésükben azonos kifejezések azonosságát megmutatni. A jelentésbeli hasonlóság megállapítható a beágyazás során létrehozott vektorreprezentációkból (emlékezzünk: a hasonló vektorreprezentáció hasonló szemantikai tartalomra utal). Kimutathatjuk a szemantikai közelséget például a király  férfi  lovag kifejezések között, de olyan mesterségesen létrehozott jelentésbeli azonosságokat is feltárhatunk, mint az irányítószámok és az általuk jelölt városnevek kapcsolata. Abban az esetben, ha a szóbeágyazást kimondottan a szöveghasonlóság megállapítására szeretnénk használni, a WMD (Word Movers Distance) metrikát érdemes használni, mely a vektortérben elhelyezked szóvektorok közötti távolság által számszersíti a szövegek hasonlóságát (Kusner et al. 2015). 10.4 Hasonlóságszámítás 10.4.1 Adatbázis-importálás és elkészítés A fejezet második felében a lexikai hasonlóság vizsgálatára, ezen belül a Jaccard-hasonlóság és a koszinusz-hasonlóság számítására mutatunk be egy-egy példát a törvényjavaslatok és az elfogadott törvények szövegeinek összehasonlításával. Az alábbiakban bemutatott elemzés a (Sebk et al. 2021) megjelenés eltt álló tanulmányból meríti elemzési fókuszát. Az eredeti cikk által megvalósított elemzést a svájci korpusz elemzése nélkül, a magyar korpusz egy részhalmazán replikáljuk az alábbiakban. A kutatási kérdés arra irányul, hogy mennyiben változik meg a törvényjavaslatok szövege a parlamenti vita folyamán, amíg a javaslat elfogadásra kerül. Az elemzés során a különböz kormányzati ciklusok közötti eltérésekre világítunk rá. Az elemzés megkezdése eltt a már ismert módon betöltjük a szükséges csomagokat. library(stringr) library(dplyr) library(tidyr) library(quanteda) library(quanteda.textstats) library(readtext) library(ggplot2) library(plotly) library(HunMineR) Ezt követen betöltjük azokat az adatbázisokat, amelyeken a szövegösszehasonlítást fogjuk végezni: az elfogadott törvények szövegét tartalmazó korpuszt, a törvényjavaslatok szövegét tartalmazó korpuszt, valamint az ezek összekapcsolását segít adatbázist, melyben az összetartozó törvényjavaslatok és törvények azonosítóját (id-ját) tároltuk el. Ahogy behívjuk a három adattáblát, érdemes rögtön lekérni az oszlopneveket colnames() és a táblázat dimenzióit dim(), hogy lássuk, milyen adatok állnak a rendelkezésünkre, és mekkora táblákkal fogunk dolgozni. A dim() függvény els értéke a sorok száma, a második pedig az oszlopok száma lesz az adott táblázatban. torvenyek &lt;- HunMineR::data_lawtext_sample colnames(torvenyek) #&gt; [1] &quot;tv_id&quot; &quot;torveny_szoveg&quot; &quot;korm_ciklus&quot; #&gt; [4] &quot;ev&quot; &quot;korm_ell&quot; dim(torvenyek) #&gt; [1] 600 5 tv_javaslatok &lt;- HunMineR::data_lawprop_sample colnames(tv_javaslatok) #&gt; [1] &quot;tvjav_id&quot; &quot;tvjav_szoveg&quot; dim(tv_javaslatok) #&gt; [1] 600 2 parok &lt;- HunMineR::data_lawsample_match colnames(parok) #&gt; [1] &quot;tv_id&quot; &quot;tvjav_id&quot; dim(parok) #&gt; [1] 600 2 Az importált adatbázisok megfigyeléseinek száma egységesen 600. Ez a több mint háromezer megfigyelést tartalmazó eredeti korpusz egy részhalmaza, mely gyorsabb és egyszerbb elemzést tesz lehetvé. Az oszlopnevek lekérésével láthatjuk, hogy a törvénykorpuszban van néhány metaadat, amelyet az elemzés során felhasználhatunk: ezek a kormányzati ciklusra, a törvény elfogadásának évére, valamint a benyújtó kormánypárti vagy ellenzéki pártállására vonatkoznak. Ezenkívül rendelkezésre állnak a törvényeket és a törvényjavaslatokat azonosító kódok (tv_id és tvjav_id), melyek segítségével majd tudjuk párosítani az összetartozó törvényjavaslatok és törvények szövegeit. Ezt a left_join() függvénnyel tesszük meg. Elsként a törvényeket tartalmazó adatbázishoz kapcsoljuk hozzá a törvénytörvényjavaslat párokat tartalmazó adatbázist a törvények azonosítója (tv_id) alapján. A colnames() függvény használatával ellenrizhetjük, hogy sikeres volt-e a mvelet, és az új táblában szerepelnek-e a kívánt oszlopok. tv_tvjavid_osszekapcs &lt;- left_join(torvenyek, parok) colnames(tv_tvjavid_osszekapcs) #&gt; [1] &quot;tv_id&quot; &quot;torveny_szoveg&quot; &quot;korm_ciklus&quot; #&gt; [4] &quot;ev&quot; &quot;korm_ell&quot; &quot;tvjav_id&quot; dim(tv_tvjavid_osszekapcs) #&gt; [1] 600 6 Második lépésben a törvényjavaslatokat tartalmazó adatbázist rendeljük hozzá az elzekben már összekapcsolt két adatbázishoz. tv_tvjav_minta &lt;- left_join(tv_tvjavid_osszekapcs, tv_javaslatok) colnames(tv_tvjav_minta) #&gt; [1] &quot;tv_id&quot; &quot;torveny_szoveg&quot; &quot;korm_ciklus&quot; #&gt; [4] &quot;ev&quot; &quot;korm_ell&quot; &quot;tvjav_id&quot; #&gt; [7] &quot;tvjav_szoveg&quot; dim(tv_tvjav_minta) #&gt; [1] 600 7 Ha jól végeztük a dolgunkat az adatbázisok összekapcsolása során, az eljárás végére 7 oszlopunk és 600 sorunk van, vagyis az újonnan létrehozott adatbázisba bekerült az összes változó (oszlop). A korpuszaink egy adattáblában való kezelése azért hasznos, mert így nem kell párhuzamosan elvégezni az azonos mveleteket a két korpusz, a törvények és a törvényjavaslatok tisztításához, hanem párhuzamosan tudunk dolgozni a kettvel. Kicsit közelebbrl megvizsgálva az adatbázist, megtekinthetjük a metaadatainkat, valamit azt is láthatjuk a count funkció segítségével, hogy minden adatbázisunkban szerepl kormányzati ciklusra 100 megfigyelés áll rendelkezésünkre: tv_tvjav_minta %&gt;% count(korm_ciklus). summary(tv_tvjav_minta) #&gt; tv_id torveny_szoveg korm_ciklus #&gt; Length:600 Length:600 Length:600 #&gt; Class :character Class :character Class :character #&gt; Mode :character Mode :character Mode :character #&gt; #&gt; #&gt; #&gt; ev korm_ell tvjav_id tvjav_szoveg #&gt; Min. :1994 Min. : 0 Length:600 Length:600 #&gt; 1st Qu.:2000 1st Qu.:900 Class :character Class :character #&gt; Median :2006 Median :900 Mode :character Mode :character #&gt; Mean :2006 Mean :741 #&gt; 3rd Qu.:2012 3rd Qu.:900 #&gt; Max. :2018 Max. :901 tv_tvjav_minta %&gt;% count(korm_ciklus) #&gt; # A tibble: 6 x 2 #&gt; korm_ciklus n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 1994-1998 100 #&gt; 2 1998-2002 100 #&gt; 3 2002-2006 100 #&gt; 4 2006-2010 100 #&gt; 5 2010-2014 100 #&gt; 6 2014-2018 100 Hasonlóan ellenrizhetjük az egyes évekre es megfigyelések számát is. tv_tvjav_minta %&gt;% count(ev) #&gt; # A tibble: 24 x 2 #&gt; ev n #&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 1994 11 #&gt; 2 1995 31 #&gt; 3 1996 23 #&gt; 4 1997 31 #&gt; 5 1998 17 #&gt; 6 1999 20 #&gt; # ... with 18 more rows 10.5 Szövegtisztítás Mivel az elemzés során két különböz korpusszal dolgozunk  két oszlopnyi szöveggel , egyszerbb, ha a szövegtisztítás lépéseibl létrehozunk egy külön függvényt, amely magában foglalja a mvelet egyes lépéseit, és lehetvé teszi, hogy ne kelljen minden szövegtisztítási lépést külön definiálni az egyes korpuszok esetén. A függvény neve jelen esetben szovegtisztitas lesz, és a már ismert lépéseket foglalja magában: kontrollkarakterek szóközzé alakítása, központozás és a számok eltávolítása. Kisbetsítés, ismétld stringek és a stringek eltt található szóközök eltávolítása. Továbbá a str_remove_all() függvénnyel eltávolítjuk azokat az írásjeleket, amelyek elfordulnak a szövegben, de számunkra nem hasznosak. A függvény definiálását az alábbi szintaxissal tehetjük meg. fuggveny &lt;- function(bemenet) { elvegzendo_lepesek return(kimenet) } A bemenet helyen azt jelöljük, hogy milyen objektumon fogjuk végrehajtani a mveleteket, a kimenetet pedig a return() függvénnyel definiáljuk, ez lesz a függvényünk úgynevezett visszatérési értéke, vagyis az elvégzend lépések szerint átalakított objektum. A szövegtisztító függvény bemeneti és kimeneti értéke is text lesz, mivel ebbe a változóba mentettük az elvégzend változtatásokat. szovegtisztitas &lt;- function(text) { text = str_replace(text, &quot;[:cntrl:]&quot;, &quot; &quot;) text = str_remove_all(string = text, pattern = &quot;[:punct:]&quot;) text = str_remove_all(string = text, pattern = &quot;[:digit:]&quot;) text = str_to_lower(text) text = str_trim(text) text = str_squish(text) return(text) } Miután létrehoztuk a szövegtisztításra alkalmas függvényünket, az adatbázis két oszlopára fogjuk alkalmazni: a törvények szövegét és a törvényjavaslatok szövegét tartalmazó oszlopra, amiben a mapply() függvény lesz a segítségünkre. A mapply() függvényen belül megadjuk megadjuk az adattáblát és hivatkozunk annak releváns oszlopaira tv_tvjav_minta[ ,c(\"torveny_szoveg\",\"tvjav_szoveg\")]. Az alkalmazni kívánt függvényt a FUN argumentumaként adhatjuk meg  értelemszeren ez esetünkben az elzekben létrehozott szovegtisztitas függvény lesz. Végezetül pedig a függvényünk által megtisztított új oszlopokkal felülírjuk az elz adatbázisunk vonatkozó oszlopait, vagyis a torveny_szoveg és a tvjav_szoveg oszlopokat: tv_tvjav_minta[, c(\"torveny_szoveg\",\"tvjav_szoveg\")]. Amennyiben számítunk rá, hogy még változhatnak a szövegeket tartalmazó oszlopok, akkor érdemes elre definiálni a szöveges oszlopok neveit, hogy késbb csak egy helyen kelljen változtatni a kódon. szovegek &lt;- c(&quot;torveny_szoveg&quot;, &quot;tvjav_szoveg&quot;) tv_tvjav_minta[, szovegek] &lt;- mapply(tv_tvjav_minta[, szovegek], FUN = szovegtisztitas) A szövegtisztítás következ lépése a tiltólistás szavak meghatározása és kiszrése a szövegbl. Itt a quanteda csomagban elérhet magyar nyelv stopszavakat, valamint a 7. fejezetben meghatározott speciális jogi stopszavak listáját használjuk. legal_stopwords &lt;- HunMineR::data_legal_stopwords A stopszavak beimportálását követen korpusszá alakítjuk a szövegeinket és tokenizáljuk azokat. Ezt már külön-külön végezzük el a törvények és a törvényjavaslatok szövegeire, azonos lépésekben haladva. A létrehozott objektumokat itt is ellenrizhetjük, például a summary(torvenyek_coprus) paranccsal, vagy a torvenyek_tokens[1:3] paranccsal, mely az els 3 dokumentum tokenjeit fogja megmutatni. torvenyek_corpus &lt;- corpus(tv_tvjav_minta$torveny_szoveg) tv_javaslatok_corpus &lt;- corpus(tv_tvjav_minta$tvjav_szoveg) torvenyek_tokens &lt;- tokens(torvenyek_corpus) %&gt;% tokens_remove(stopwords(&quot;hungarian&quot;)) %&gt;% tokens_remove(legal_stopwords) %&gt;% tokens_wordstem(language = &quot;hun&quot;) tv_javaslatok_tokens &lt;- tokens(tv_javaslatok_corpus) %&gt;% tokens_remove(stopwords(&quot;hungarian&quot;)) %&gt;% tokens_remove(legal_stopwords) %&gt;% tokens_wordstem(language = &quot;hun&quot;) A szövegek tokenizálásával és a tiltólistás szavak eltávolításával a szövegtisztítás végére értünk, így megkezdhetjük az elemzést. 10.6 A Jaccard-hasonlóság számítása A Jaccard-hasonlóság kiszámításához a quanteda.textstats textstat_simil() függvényét fogjuk alkalmazni. Mivel a textstat_simil() függvény dokumentum-kifejezés mátrixot vár bemenetként, elsként alakítsuk át ennek megfelelen a korpuszainkat. Az elz fejezetekhez hasonlóan itt is a TF-IDF súlyozást választottuk a mátrix létrehozásakor. torvenyek_dfm &lt;- dfm(torvenyek_tokens) %&gt;% dfm_tfidf() tv_javaslatok_dfm &lt;- dfm(tv_javaslatok_tokens) %&gt;% dfm_tfidf() Miután létrehoztuk a dokumentum-kifejezés mátrixokat, érdemes a leggyakoribb tokeneket ellenrizni a textstat_frequency() függvénnyel, hogy biztosak lehessünk abban, hogy a megfelel eredményt értük el a szövegtisztítás során. (Amennyiben nem vagyunk elégedettek, érdemes visszatérni a stopszavakhoz és újabb kifejezéseket hozzárendelni a stopszólistához.) tv_toptokens &lt;- textstat_frequency(torvenyek_dfm, n = 10, force = TRUE) tv_toptokens #&gt; feature frequency rank docfreq group #&gt; 1 an 6992 1 131 all #&gt; 2 szerzodo 4795 2 150 all #&gt; 3 felhalmozás 3618 3 28 all #&gt; 4 articl 3426 4 74 all #&gt; 5 kiadás 3328 5 224 all #&gt; 6 for 3305 6 100 all #&gt; 7 contracting 3295 7 39 all #&gt; 8 költségvetés 3130 8 206 all #&gt; 9 befektetés 3130 9 73 all #&gt; 10 szanálás 2629 10 13 all tvjav_toptokens &lt;- textstat_frequency(tv_javaslatok_dfm, n = 10, force = TRUE) tvjav_toptokens #&gt; feature frequency rank docfreq group #&gt; 1 an 6051 1 160 all #&gt; 2 szerzodo 4533 2 148 all #&gt; 3 articl 3490 3 75 all #&gt; 4 befektetés 3447 4 90 all #&gt; 5 buncselekmény 3274 5 96 all #&gt; 6 contracting 3266 6 40 all #&gt; 7 szanálás 3235 7 12 all #&gt; 8 bíróság 3226 8 301 all #&gt; 9 egyezmény 3045 9 231 all #&gt; 10 for 3045 10 109 all A létrehozott dokumentum-kifejezés mátrixokon elvégezhetjük a dokumentumhasonlóság-vizsgálatot. A Jaccard-hasonlóság-metrika, illetve a quanteda textstat_simil() függvénye alkalmazható egy korpuszra is. Egy korpuszra végezve az elemzést, a függvény a korpusz dokumentumai közötti hasonlóságot számítja ki, míg két korpuszra mindkét korpusz összes dokumentuma közötti hasonlóságot. Érdemes továbbá azt is megjegyezni, hogy a textstat_simil() method argumentumaként megadható számos más hasonlósági metrika is, melyekkel további érdekes számítások végezhetk. Bvebben a textstat_simil() függény használatáról és argumentumairól a quanteda hivatalos honlapján olvashatunk.51 A textstat_simil() függvény kapcsán azt is érdemes figyelembe venni, hogy mivel nemcsak a dokumentum párokra, hanem az összes bemenetként megadott dokumentumra külön kiszámítja a Jaccard-indexet, a korpusz(ok) méretének növelésével a számítás kapacitás- és idigényessége exponenciálisan növekszik. Két 600 dokumentumból álló korpusz esetén kb. 45 perc a számítási id, míg 360 dokumentum esetén csupán 12 perc. jaccard_hasonlosag &lt;- textstat_simil(torvenyek_dfm, tv_javaslatok_dfm, method = &quot;jaccard&quot;) Mivel az eredménymátrixunk meglehetsen terjedelmes, nem érdemes az egészet egyben megtekinteni, egyszerbb az els néhány dokumentum közötti hasonlóságra szrni, melyet a szögletes zárójelben való indexeléssel tudunk megtenni. Az [1:5, 1:5] kifejezéssel specifikálhatjuk a sorokat és az oszlopkat az elstl az ötödikig. jaccard_hasonlosag[1:5, 1:5] #&gt; 5 x 5 Matrix of class &quot;dgeMatrix&quot; #&gt; text1 text2 text3 text4 text5 #&gt; text1 0.5541 0.126 0.1144 0.1250 0.1526 #&gt; text2 0.0983 0.572 0.1278 0.1649 0.0943 #&gt; text3 0.0880 0.124 0.7288 0.1022 0.0764 #&gt; text4 0.1089 0.189 0.1182 0.5720 0.1134 #&gt; text5 0.1504 0.094 0.0881 0.0984 0.6273 A mátrix fátlójában jelennek meg az összetartozó törvényekre és törvényszövegekre vonatkozó értékek, minden más érték nem összetartozó törvény- és törvényjavaslat-szövegek hasonlóságára vonatkozik, vagyis a vizsgálatunk szempontjából irreleváns. A mátrix fátlóját a diag() függvény segítségével nyerhetjük ki. Ha jól dolgoztunk, a létrehozott jaccard_diag els öt eleme (jaccard_diag[1:5]) megegyezik a fent megjelenített 5 × 5-ös mátrix fátlójában elhelyezked értékekkel, hossza pedig (length()) a mátrix bármelyik dimenziójával. jaccard_diag &lt;- diag(as.matrix(jaccard_hasonlosag)) jaccard_diag[1:5] #&gt; text1 text2 text3 text4 text5 #&gt; 0.554 0.572 0.729 0.572 0.627 Miután sikerült kinyerni az egyes törvénytörvényjavaslat párokra vonatkozó Jaccard-értéket, érdemes a számításainkat hozzárendelni az eredeti adattáblánkhoz, hogy a meglév metaadatok fényében tudjuk kiértékelni az egyes dokumentumok közötti hasonlóságot. A hozzárendeléshez egyszeren definiálunk egy új oszlopot a meglév adatbázisban tv_tvjav_minta$jaccard_index, melyhez hozzárendeljük a kinyert értékeket. tv_tvjav_minta$jaccard_index &lt;- jaccard_diag Érdemes megnézni a végeredményt, ellenrizni a Jaccard-hasonlóság legmagasabb vagy legalacsonyabb értékeit. A top_n() függvény használatával ki tudjuk válogatni a legmagasabb és a legalacsonyabb értékeket. A top_n() függvény els argumentuma a változó lesz, ami alapján a legalacsonyabb és a legmagasabb értékeket keressük, a második argumentum pedig azt specifikálja, hogy a legmagasabb és a legalacsonyabb értékek közül hányat szeretnénk látni. Az n=5 értékkel a legmagasabb, az n=-5 értékkel a legalacsonyabb 5 Jaccard-indexszel rendelkez sort tudjuk kiszrni. Emellett érdemes arra is odafigyelni, hogy a szövegeket tartalmazó oszlopainkat ne próbáljuk meg kiíratni, hiszen ez jelentsen lelassítja az RStudio mködését és csökkenti a kiírt eredmények áttekinthetségét. tvjav_oszlopok &lt;- c( &quot;tv_id&quot;, &quot;korm_ciklus&quot;, &quot;tvjav_id&quot;, &quot;jaccard_index&quot; ) tv_tvjav_minta[, tvjav_oszlopok] %&gt;% top_n(jaccard_index, n = 5) #&gt; # A tibble: 5 x 4 #&gt; tv_id korm_ciklus tvjav_id jaccard_index #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1994XCV 1994-1998 1994-1998_T0276 0.991 #&gt; 2 1995LXXXI 1994-1998 1994-1998_T1296 0.987 #&gt; 3 1999XXXV 1998-2002 1998-2002_T0807 0.985 #&gt; 4 2013XLII 2010-2014 2010-2014_T10219 0.981 #&gt; 5 2014VIII 2010-2014 2010-2014_T13631 0.980 tv_tvjav_minta[, tvjav_oszlopok] %&gt;% top_n(jaccard_index, n = -5) #&gt; # A tibble: 5 x 4 #&gt; tv_id korm_ciklus tvjav_id jaccard_index #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1998I 1994-1998 1994-1998_T4328 0.0513 #&gt; 2 2005LII 2002-2006 2002-2006_T16291 0.0532 #&gt; 3 2007CLXVIII 2006-2010 2006-2010_T04678 0.0270 #&gt; 4 2010CLV 2010-2014 2010-2014_T01809 0.0273 #&gt; 5 2012CXCV 2010-2014 2010-2014_T09103 0.00895 10.7 A koszinusz-hasonlóság számítása A Jaccard-hasonlóság számítása után a koszinusz-távolság számítása már nem jelent nagy kihívást, hiszen a textat_simil() függvénnyel ezt is kiszámíthatjuk, csupán a metrika paramétereként (method =) megadhatjuk a koszinuszt is. Ahogy az elbbiekben, itt is a dokumentum-kifejezés mátrixokat adjuk meg bemeneti értékként. koszinusz_hasonlosag &lt;- textstat_simil(x = torvenyek_dfm, y = tv_javaslatok_dfm, method = &quot;cosine&quot;) Érdemes itt is megtekinteni a mátrix els néhány sorába és oszlopába es értékeket. koszinusz_hasonlosag[0:5, 0:5] #&gt; 5 x 5 Matrix of class &quot;dgeMatrix&quot; #&gt; text1 text2 text3 text4 text5 #&gt; text1 0.6188 0.01583 0.01616 0.01436 0.05515 #&gt; text2 0.0105 0.92893 0.00619 0.04538 0.00704 #&gt; text3 0.0148 0.00763 0.98499 0.00566 0.00183 #&gt; text4 0.0185 0.05014 0.00557 0.96063 0.01834 #&gt; text5 0.0740 0.00687 0.00285 0.01748 0.75375 Ebben az esetben is csak a mátrix átlójára van szükségünk, melyet a fent ismertetett módon nyerünk ki a mátrixból. koszinusz_diag &lt;- diag(as.matrix(koszinusz_hasonlosag)) koszinusz_diag[1:5] #&gt; text1 text2 text3 text4 text5 #&gt; 0.619 0.929 0.985 0.961 0.754 Végezetül pedig az átlóból kinyert koszinusz értékeket is hozzárendeljük az adatbázisunkhoz. tv_tvjav_minta$koszinusz &lt;- koszinusz_diag A kibvített adatbázis oszlopneveit a colnames() függvénnyel ellenrizhetjük, hogy lássuk, valóban sikerült-e hozzárendelnünk a koszinusz értékeket a táblához. colnames(tv_tvjav_minta) #&gt; [1] &quot;tv_id&quot; &quot;torveny_szoveg&quot; &quot;korm_ciklus&quot; #&gt; [4] &quot;ev&quot; &quot;korm_ell&quot; &quot;tvjav_id&quot; #&gt; [7] &quot;tvjav_szoveg&quot; &quot;jaccard_index&quot; &quot;koszinusz&quot; 10.8 Az eredmények vizualizációja A hasonlósági metrikák vizulizációjára gyakran alkalmazott megoldás a htérkép (heatmap), mellyel korrelációs mátrixokat ábrázolhatunk. Ebben az esetben a mátrix értékeit egy színskálán vizualizáljuk, ahol a világosabb színek a magasabb, a sötétebb színek az alacsonyabb értékeket jelölik. A Jaccard-hasonlóság számításakor és a koszinusz-hasonlóság számításakor kapott mátrixok esetén is ábrázolhatjuk az értékeinket ilyen módon. Mivel azonban mindkét mátrix 600 × 600-as, nem érdemes a teljes mátrixot megjeleníteni, mert ilyen nagy mennyiség adatnál már értelmezhetetlenné válik az ábra, így csak az utolsó 100 elemet, vagyis a 20142018-as idszakra vonatkozó értékeket jelenítjük meg. Ezt a koszinusz_hasonlóság nev objektumunk feldarabolásával tesszük meg, szögletes zárójelben jelölve, hogy a mátrix mely sorait és mely oszlopait szeretnénk használni: koszinusz_hasonlosag[501:600, 501:600]. A koszinusz_hasonlosag objektumból egy data frame-et készítünk, ahol a dokumentumok közötti hasonlóság szerepel. A mátrix formátumból a tidyr csomag pivot_longer() függvényét használva tudjuk a kívánt formátumot elérni. koszinusz_df &lt;- as.matrix(koszinusz_hasonlosag[501:600, 501:600]) %&gt;% as.data.frame() %&gt;% rownames_to_column(&quot;docs1&quot;) %&gt;% pivot_longer( &quot;text501&quot;:&quot;text600&quot;, names_to = &quot;docs2&quot;, values_to = &quot;similarity&quot; ) glimpse(koszinusz_df) #&gt; Rows: 10,000 #&gt; Columns: 3 #&gt; $ docs1 &lt;chr&gt; &quot;text501&quot;, &quot;text501&quot;, &quot;text501&quot;, &quot;text501&quot;, &quot;text~ #&gt; $ docs2 &lt;chr&gt; &quot;text501&quot;, &quot;text502&quot;, &quot;text503&quot;, &quot;text504&quot;, &quot;text~ #&gt; $ similarity &lt;dbl&gt; 0.95601, 0.02297, 0.04258, 0.02809, 0.00906, 0.03~ Ezt követen pedig a ggplot függvényt használva a geom_tile segítségével tudjuk elkészíteni a htérképet, ami a hasonlósági mátrixot ábrázolja (ld. 10.1. ábra). koszinusz_plot &lt;- ggplot(koszinusz_df, aes(docs1, docs2, fill = similarity)) + geom_tile() + scale_fill_gradient(high = &quot;#2c3e50&quot;, low = &quot;#bdc3c7&quot;) + labs( x = NULL, y = NULL, fill = &quot;Koszinusz-hasonlóság&quot; ) + theme( axis.text.x=element_blank(), axis.ticks.x=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank() ) ggplotly(koszinusz_plot, tooltip = &quot;similarity&quot;) Ábra 10.1: Koszinusz-hasonlóság hotérképen ábrázolva A Jaccard-hasonlósági htérképet ugyanezzel a módszerrel tudjuk elkészíteni (ld. 10.2. ábra). # adatok átalakítása jaccard_df &lt;- as.matrix(jaccard_hasonlosag[501:600, 501:600]) %&gt;% as.data.frame() %&gt;% rownames_to_column(&quot;docs1&quot;) %&gt;% pivot_longer( &quot;text501&quot;:&quot;text600&quot;, names_to = &quot;docs2&quot;, values_to = &quot;similarity&quot; ) # a ggplot ábra jaccard_plot &lt;-ggplot(jaccard_df, aes(docs1, docs2, fill = similarity)) + geom_tile() + scale_fill_gradient(high = &quot;#2c3e50&quot;, low = &quot;#bdc3c7&quot;) + labs( x = NULL, y = NULL, fill = &quot;Jaccard hasonlóság&quot; ) + theme( axis.text.x=element_blank(), axis.ticks.x=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank() ) ggplotly(jaccard_plot, tooltip = &quot;similarity&quot;) Ábra 10.2: Jaccard hasonlóság hotérképen ábrázolva A két ábra összehasonlításánál láthatjuk, hogy a koszinusz-hasonlóság általában magasabb hasonlósági értékeket mutat. A mátrix fátlójában kiugró világos csík azt mutatja meg, hogy a legnagyobb hasonlóság az összetartozó törvény- és törvényjavaslat-szövegek között mutatkozik meg, eddig tehát az adataink az elvárásaink szerinti képet mutatják. Amennyiben a világos csíkot nem látnánk, az egyértelm visszajelzés volna arról, hogy elrontottunk valamit a szövegelkészítés eddigi lépései során, vagy a várakozásásaink voltak teljesen rosszak. A koszinusz- és a Jaccard-hasonlóság értékét ábrázolhatjuk közös pont-diagramon a geom_jitter() segítségével (ld. 10.3. ábra). Ehhez elször egy kicsit átalakítjuk a data frame-et a tidyr csomag pivot_longer() függvényével, hogy a két hasonlósági érték egy oszlopban legyen. Ez azért szükséges, hogy a ggplot ábránkat könnyebben tudjuk létrehozni. tv_tvjav_tidy &lt;- tv_tvjav_minta %&gt;% pivot_longer( &quot;jaccard_index&quot;:&quot;koszinusz&quot;, values_to = &quot;hasonlosag&quot;, names_to = &quot;hasonlosag_tipus&quot; ) glimpse(tv_tvjav_tidy) #&gt; Rows: 1,200 #&gt; Columns: 9 #&gt; $ tv_id &lt;chr&gt; &quot;1994LXXXII&quot;, &quot;1994LXXXII&quot;, &quot;1996CXXXI&quot;, &quot;1~ #&gt; $ torveny_szoveg &lt;chr&gt; &quot;évi lxxxii törvény a magánszemélyek jövede~ #&gt; $ korm_ciklus &lt;chr&gt; &quot;1994-1998&quot;, &quot;1994-1998&quot;, &quot;1994-1998&quot;, &quot;199~ #&gt; $ ev &lt;dbl&gt; 1994, 1994, 1996, 1996, 1995, 1995, 1995, 1~ #&gt; $ korm_ell &lt;dbl&gt; 900, 900, 900, 900, 900, 900, 900, 900, 900~ #&gt; $ tvjav_id &lt;chr&gt; &quot;1994-1998_T0233&quot;, &quot;1994-1998_T0233&quot;, &quot;1994~ #&gt; $ tvjav_szoveg &lt;chr&gt; &quot;magyar köztársaság kormánya t számú törvén~ #&gt; $ hasonlosag_tipus &lt;chr&gt; &quot;jaccard_index&quot;, &quot;koszinusz&quot;, &quot;jaccard_inde~ #&gt; $ hasonlosag &lt;dbl&gt; 0.554, 0.619, 0.572, 0.929, 0.729, 0.985, 0~ jaccard_koszinusz_plot &lt;- ggplot(tv_tvjav_tidy, aes(ev, hasonlosag)) + geom_jitter(aes(shape = hasonlosag_tipus,color = hasonlosag_tipus), width = 0.1, alpha = 0.45) + scale_x_continuous(breaks = seq(1994, 2018, by = 2)) + labs( y = &quot;Jaccard és koszinusz-hasonlóság&quot;, shape = NULL, color = NULL, x = NULL ) + theme(legend.position = &quot;bottom&quot;, panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank() ) ggplotly(jaccard_koszinusz_plot) Ábra 10.3: Koszinusz- és Jaccard hasonlóság értékeinek alakulása A pont-diagramm ebben a formájában keveset árul el a számunkra, láthatjuk, hogy a koszinusz értékek általában magasabbak, de azt nem tudjuk meg mondani az egyes pontok alapján, hogy évrl évre hogyan alakultak a jaccard és koszinusz értékek. Ezért a hasonlósági értékek évenkénti alakulásának megértése érdekében érdemes átlagot számolni a mutatókra. Ezt a group_by() és a summarize() függvények együttes alkalmazásával tehetjük meg. Megadjuk, hogy évenkénti bontásban szeretnénk a számításainkat elvégezni group_by(ev), és azt, hogy átlagszámítást szeretnénk végezni mean(). evenkenti_atlag &lt;- tv_tvjav_tidy %&gt;% group_by(ev, hasonlosag_tipus) %&gt;% summarize(atl_hasonlosag = mean(hasonlosag)) head(evenkenti_atlag) #&gt; # A tibble: 6 x 3 #&gt; # Groups: ev [3] #&gt; ev hasonlosag_tipus atl_hasonlosag #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1994 jaccard_index 0.556 #&gt; 2 1994 koszinusz 0.757 #&gt; 3 1995 jaccard_index 0.499 #&gt; 4 1995 koszinusz 0.776 #&gt; 5 1996 jaccard_index 0.675 #&gt; 6 1996 koszinusz 0.880 Az évenkénti átlagot tartalmazó adattáblánkra ezt követen vonal diagramot illesztünk. A 10.4.-es ábrán az látható, hogy a Jaccard- és koszinusz-hasonlóság értéke nagyjából együtt mozog, leszámítva a 2002-es, 2003-as éveket, mivel azonban itt csak néhány adatpont áll rendelkezésre, ettl az inkonzisztenciától eltekinthetünk. jac_cos_ave_df &lt;- ggplot(evenkenti_atlag, aes(ev, atl_hasonlosag)) + geom_line(aes(linetype = hasonlosag_tipus)) + labs(y = &quot;Átlagos Jaccard- és koszinusz-hasonlóság&quot;, linetype = NULL, x = NULL) + theme( legend.position = &quot;bottom&quot;, panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank() ) ggplotly(jac_cos_ave_df) Ábra 10.4: Koszinusz- és Jaccard hasonlóság vonal diagramon ábrázolva Ahhoz, hogy valamivel pontosabb képet kapjunk a Jaccard-index értékének alakulásáról, az eredményeinket érdemes vizualizálni a ggplot2 segítségével. Elsként évenkénti bontásban, boxplotokkal ábrázoljuk a Jaccard-index értékének alakulását (ld. 10.5. ábra). Így már nem csak azt láthatjuk, hogy évente mennyi volt az átlagos jaccard érték, hanem azt is, hogy az átlagtól mekkora eltérések voltak jelen, ez utóbbit természetesen azt is befolyásolja, hogy hány törvényt fogadtak el az adott évben. jac_cos_box_df &lt;- ggplot(tv_tvjav_minta, aes(as.factor(ev), jaccard_index)) + geom_boxplot() + labs( x = NULL, y = &quot;Jaccard-hasonlóság&quot; ) + theme( axis.text.x = element_text(angle = 45), legend.position = &quot;none&quot; ) ggplotly(jac_cos_box_df) Ábra 10.5: Jaccard hasonlóság boxplotokkal ábrázolva A legalacsonyabb Jaccard-hasonlóság talán az 19941998-as idszakra jellemz, míg a 20142018-as idszakra szembetnen magas Jaccard-értékeket látunk az els ábrázolt ciklushoz képest. Összességében nehéz trendet látni az ábrán, de érdemes azt is megjegyezni, hogy a negatív irányba kiugró adatpontok a 20142018-as ciklusban jelentsen nagyobb arányban tnnek fel, mint a korábbi kormányzati ciklusok alatt. Végezetül pedig ábrázolhatjuk a Jaccard-hasonlóságot a benyújtó személye alapján is a korm_ell változónk alapján. A változók értékei a következk a CAP kódkönyve alapján: 0 - Ellenzéki benyújtó 1 - Kormánypárti benyújtó 2 - Kormánypárti és ellenzéki benyújtó közösen 3 - Benyújtók legalább két ellenzéki pártból 4 - Benyújtók legalább két kormánypártból 900 - Nem releváns  a benyújtó a kabinet tagja 901 - Nem releváns  a benyújtó a bizottság tagja 902 - Nem releváns  a benyújtó sem a parlamentnek, sem a kabinetnek, sem a bizottságnak nem tagja Mivel nincs túl sok adatpontunk, és ezek többsége a 900-as adatpont alá esik (lásd tv_tvjav_minta %&gt;% count(korm_ell)), érdemes összevonni a 0-ás és a 3-as változót, valamint az 1-es és a 4-es változót egy-egy értékbe, hogy jobban elemezhetek legyenek az eredményeink. Ehhez a korm_ell változó értékei alapján definiálunk egy új korm_ell2 változót. Az új változó definiálását és az értékadásokat a dplyr case_when() függvényével fogjuk megtenni. A függvényen belül a bal oldalra kerül, hogy milyen értékek alapján szeretnénk az új értéket meghatározni, a tilde (~) után pedig az, hogy mi legyen az újonnan létrehozott oszlop értéke. Tehát a case_when()-en belül lév els sor azt fejezi ki, hogy amennyiben a korm_ell egyenl 0-val, vagy (|) a korm_ell egyenl 3-mal, legyen a korm_ell2 értéke 0. tv_tvjav_minta &lt;- tv_tvjav_minta %&gt;% mutate( korm_ell2 = case_when( korm_ell == 0 | korm_ell == 3 ~ &quot;Ellenzéki képvisel&quot;, korm_ell == 1 | korm_ell == 4 ~ &quot;Kormánypárti képvisel&quot;, korm_ell == 2 ~ &quot;Kormánypárti és ellenzéki képviselk közösen&quot;, korm_ell == 900 ~ &quot;Kabinet tagja&quot;, korm_ell == 901 ~ &quot;Parlamenti bizottság&quot;, korm_ell == 902 ~ &quot;Egyik sem&quot; ), korm_ell2 = as.factor(korm_ell2) ) Miután létrehoztuk az új oszlopot, létrehozhatjuk a vizualizációt is annak alapján. Itt egy speciális pontdiagramot fogunk használni: geom_jitter(). Ez annyiban különbözik a pontdiagramtól, hogy kicsit szórtabban ábrázolja a diszkrét értékekre (évekre) es pontokat, hogy az egy helyen srsöd értékek ne takarják ki egymást. A facet_wrap segítségével tudjuk kategóriánként ábrázolni az évenkénti hasonlóságot (ld. 10.6. ábra). jaccard_be_plot &lt;- ggplot(tv_tvjav_minta, aes(ev, jaccard_index)) + geom_jitter(width = 0.1, alpha = 0.5) + scale_x_continuous(breaks = seq(1994, 2018, by = 2)) + facet_wrap(~ korm_ell2, ncol = 1) + theme(panel.spacing = unit(2, &quot;lines&quot;))+ labs( y = NULL, x = NULL) ggplotly(jaccard_be_plot, height = 1000) Ábra 10.6: Jaccard hasonlóság a benyújtó személye alapján Mivel a törvényjavaslatok túlnyomó többségét a kabinet tagjai nyújtják be, nem igazán tudunk érdemi következtetéseket levonni arra vonatkozóan, hogy az ellenzéki vagy a kormánypárti képviselk által benyújtott javaslatok módosulnak-e többet a vita folyamán. Amennyiben ezzel a kérdéssel alaposabban is szeretnénk foglalkozni, érdemes csak azokat a sorokat kiválasztani a hasonlóságszámításhoz, amelyekben a számunkra releváns megfigyelések szerepelnek. Ha azonban ezt az eljárást választjuk, mindenképpen fontos odafigyelni arra is, hogy az elemzésben használandó megfigyelések kiválogatása nehogy szelektív legyen valamely nem megfigyelt változó szempontjából, ezzel befolyásolva a kutatás eredményeit. https://quanteda.io/reference/textstat_simil.html "],["nlp_ch.html", "11 NLP és névelemfelismerés 11.1 Fogalmi alapok 11.2 A magyarlanc 11.3 A szeged ner 11.4 Angol nyelv szövegek névelemfelismerése", " 11 NLP és névelemfelismerés 11.1 Fogalmi alapok A természetes-nyelv feldolgozása (Natural Language Processing  NLP) a nyelvészet és a mesterséges intelligencia közös területe, amely a számítógépes módszerek segítségével elemzi az emberek által használt (természetes) nyelveket. Azaz képes feldolgozni különböz szöveges dokumentumok tartalmát, kinyerni a bennük található információkat, kategorizálni és rendszerezni azokat. Angol nyelv szövegek NLP elemzésére több R csomag is rendelkezésünre áll, ezek közül kettt mutatunk be röviden. Mivel magyar nyelv szövegek NLP elemzésére ezek a csomagok jelenleg nem alkalmasak, azt mutatjuk be, hogyan végezhetjük el a magyar nyelv szövegek mondatra és szavakra bontását, szófaji egyértelmsítését, morfológiai és szintaktikai elemzését az R program használata nélkül és azután a kapott fájlokkal hogyan végezhetünk az R program segítségével további elemzéseket.52 A fejezetben részletesen foglalkozunk a névelem-felismeréssel (Named Entity Recognition  NER). Névelemnek azokat a tokensorozatokat nevezzük, amelyek valamely entitást egyedi módon jelölnek. A névelem-felismerés az infomációkinyerés részterülete, melynek lényege, hogy automatikusan felismerjük a strukturálátlan szövegben szerepl tulajdonneveket, majd azokat kigyjtsük, és típusonként (például személynév, földrajzi név, márkanév, stb.) csoportosítsuk. Bár a tulajdonnevek mellett névelemnek tekinthetk még például a telefonszámok vagy az e-mail címek is, a névelem-felismerés leginkább mégis a tulajdonnevek felismerésére irányul. A névelem-felismerés a számítógépes nyelvészetben a korai 1990-es évektl kezdve fontos feladatnak és megoldandó problémának számít. A névelem-felismerés többféle módon is megoldható, így például felügyelt tanulással, szótár alapú módszerekkel vagy kollokációk elemzésével. A névelem-felismerés körében két alapvet módszer alkalmazására van lehetség. A szabályalapú módszer alkalmazása során elre megadott adatok alapján kerül kinyerésre az információ (ilyen szabály például a mondatközi nagybet mint a tulajdonnév kezdete). A másik módszer a statisztikai tanulás, amikor a gép alkot szabályokat a kutató elzetes mintakódolása alapján. A névelemfelismerés során nehézséget okozhat a különböz névelemosztályok közötti gyakori átfedés, így például ha egy adott szó településnév és vezetéknév is lehet. A magyar nyelv szövegekben a tulajdonnevek automatikus annotációjára jelenleg három módon van lehetség: tulajdonnév-felismer algoritmussal, szófaji címke szintjén történ megkülönböztetéssel, valamint szintaktikai szint címkézéssel. Utóbbi kettre példa a fejezetben is bemutatásra kerül magyarlanc elemz, ami szófaji szinten megkülönbözteti a tulajdonneveket, a szintaxis szintjén pedig jelöli a többtagúakat.(Zsibrita, Vincze, and Farkas 2013) A tulajdonnév-felismer algoritmusok megkeresik az adott szövegben a tulajdonneveket, majd azokat valamilyen kategóriába sorolják, ilyen magyar nyelv algoritmus a szeged ner, melynek alkalmazását szintén bemutatjuk.(Szarvas, Farkas, and Kocsor 2006) Fontos különbséget tenni a névelem-felismerés és a tulajdonnév-felismerés között. A névelem-felismerésbe beletartozik minden olyan kifejezés, amely a világ valamely entitására egyedi módon (unikálisan) referál. Ezzel szemben a tulajdonnév-felismerés, kizárólag a tulajdonnevekre koncentrál.(Üveges 2019; Vincze 2019) A magyarlancnyelvi elfeldolgozó eszköz a Szegedi Tudományegyetem fejlesztése,(Zsibrita, Vincze, and Farkas 2013) ami magyar nyelv txt formátumú fájlokat feldolgozva képes egy szöveg mondatokra és szavakra bontására, a szavak morfológiai elemzésére, szófaji egyértelmsítésére, emellett kétféle szintaktikai elemzést is képes hozzárendelni a mondatokhoz.53 A magyarlanchoz hasonlóan az UDPipe nev elemz szintén képes magyar nyelv nyers szövegek mondatra és szavakra bontására és szófaji elemzésére, azaz POS-taggelésére (Part of Speech-tagging) továbbá a mondatok függségi elemzésére. Ez az elemz a nemzetközileg elismert Universal Dependencies annotációs sémán alapul. (Straka and Straková 2017) A két nyelvi elemz hasonló funkcionalitásokkal rendelkezik, ugyanakkor az UDPipe technikailag könnyebben kezelhet, azonban kevésbé pontos elemzési eredményt ad, mivel jóval kisebb tanító anyagon lett betanítva, mint a magyarlanc.54 Az alábbiakban a magyarlanc és a szeged ner mködését és az általuk létrehozott fájlokkal R-ben végezhet elemzésekre mutatunk példákat. 11.2 A magyarlanc Az elemz használatának részletes leírás megtalálható a már jelzett honlapon, itt most csak vázlatosan ismeretetjük. Fontos kiemelni, hogy a magyarlanc JAVA modulokból áll, így használatához szükséges, hogy a számítógépen megfelel JAVA környezet legyen telepítve. Elször fenti oldalról le kell töltenünk a magyarlanc-3.0.jar fájlt, majd bemásolni azt abba a mappába, ahol az elemezni kívánt txt található. A parancssort Windows operációs rendszer alatt a számítógép keres mezjébe a cmd parancs beírásával tudjuk megnyitni. Ezután a parancsorban belépve abba a könyvtárba, ahol az elemezni kíván txt és a magyarlanc-3.0.jar elemz van, az alábbi parancs segítségével végezhetjük el az elemzést: java -Xmx1G -jar magyarlanc-3.0.jar -mode morphparse -input in.txt -output out.txt, ahol az in.txt helyébe az elemezni kívánt txt nevét, az out.txt helyébe, pedig az elemzés eredményeként létrejöv fájl nevét kell megadni. Példánkban az Országgylésben 2014 és 2018 között elhangzott véletlenszeren kiválasztott 25 napirend eltti felszólalás korpuszán szemléltetjük az elemz mködését.55 A 25 fájlt elemezhetjük egyesével, de ha ez a késbbi elemzéshez nem szükséges, a parancsorban a copy *.txt eredmeny.txt paranccsal egyesíthetjük azokat egy fájlba. Majd ezen az eredmeny.txt-n végezzük el az elemzést az alábbi paranccsal: java -Xmx1G -jar magyarlanc-3.0.jar -mode morphparse -input eredmeny.txt -output eredmeny_out.txt Az elemzés eredményéül kapott txt fájlban láthatjuk, hogy az elemz elvégezte a szövegek mondatokra bontását, tokenizálását, szótári alakra hozását és POS-taggelését, azaz meghatározta a szavak szófaját. Ábra 11.1: A magyarlánc elemzo nyers eredménye Ezt követen célszer a txt fájlt excelbe beolvasva oszlopokra tagolni, az oszlopokat fejléccel ellátni, majd csv fájlként elmenteni. Ábra 11.2: Az Excelben megnyitott magyarlánc eredmény Az így létrehozott .csv fájllal megyegyez adattáblát be tudjuk tölteni a HunMineR segítségével. library(dplyr) library(HunMineR) napirend_elotti &lt;- HunMineR::data_parlspeech_magyarlanc Az így létrehozott objektummal, mely esetünkben 17870 megfigyelést és 4 változót tartalmaz, ezután különböz mveleteket végezhetünk, a korábban már bemutatottak szerint, például dplyr csomag filter függvénye segítségével kiválogathatjuk az igéket, és elmenthetjük azokat egy újabb 1769 megfigyelést és 4 változót tartalmazó objektumba. verb_napirend_elotti &lt;- napirend_elotti %&gt;% filter(POS_tag == &quot;VERB&quot;) 11.3 A szeged ner A magyarlanc nyelvi elemzhöz hasonlóan használhatjuk a szeged ner elemzt is, melynek részletes leírása és maga a ner.jar elemz is megtalálható az alábbi oldalon: https://rgai.inf.u-szeged.hu/node/109. Az elemz a fent bemutatott módon szintén parancssorból indítható az alábbi parancs használatával: java -Xmx3G -jar ner.jar -mode predicate -input input.txt -output output.txt. Az elemz PER (személynév), LOC (hely(szín)), ORG (szervezet) és MISC (vegyes) címkét ad az egyes névelemeknek. Ábra 11.3: A szeged ner elemzo eredménye A fentiekhez hasonlóan ezt a txt-t is átalakíthatjuk táblázattá, majd ezt a csv fájlt beolvashatjuk (a HunMineR csomag szintén tartalmazza ezt a data frame-et). napirend_elotti_ner &lt;- HunMineR::data_parlspeech_szner Ezután tetszlegesen kiválogathatjuk például a helyek neveit. A filterezés eredményeként láthatjuk, hogy az elemz korpuszunkban 175 szót azonosított és címkézett fel helynévként. loc_napirend_elotti &lt;- napirend_elotti_ner %&gt;% filter(ner == &quot;I-LOC&quot;) De ugyanígy kiváogathatjuk a személyneveket is, azonban itt figyelembe kell vennünk, hogy az elemz külön névelemként jelöli a vezeték- és keresztneveket, a további elemzés szükségletei szerint ezeket utólag kell összevonnunk. pers_napirend_elotti &lt;- napirend_elotti_ner %&gt;% filter(ner == &quot;I-PER&quot;) Az így kiválogatott különböz névelemekkel azután további elemzéseket végezhetünk. 11.4 Angol nyelv szövegek névelemfelismerése Amennyiben angol nyelv korpusszal dolgozunk több lehetség is a rendelkezésünkre áll a névelemfelismerés elvégzésére.56 Ezek közül most röviden a spacyr használatát mutatjuk be.57 A spaCy nem egy R, hanem egy Phyton csomag,58 amely azonban az R reticulate csomag segítségével nagyon jól együttmködik a kötetben rendszeresen használt quanteda csomaggal. Használatához a már megszokott módon installálnunk kell a spacyr csomagot, majd beolvasnunk és telepítenünk az angol nyelvi modellt. A Pythonban készült spacy-t a spacyr::spacy_install() paranccsal kell telepíteni (ezt elég egyszer megtenni, amikor elször használjuk a csomagot). library(spacyr) library(quanteda) library(ggplot2) spacy_initialize(model = &quot;en_core_web_sm&quot;) A spacy_parse() függvény segítségével lehetségünk van a szövegek tokenizálására, szótári alakra hozására és szófaji egyértelmsítésére. txt &lt;- c(d1 = &quot;spaCy is great at fast natural language processing.&quot;, d2 = &quot;Mr. Smith spent two years in North Carolina.&quot;) # process documents and obtain a data.table parsedtxt &lt;- spacy_parse(txt) parsedtxt #&gt; doc_id sentence_id token_id token lemma pos entity #&gt; 1 d1 1 1 spaCy spacy INTJ #&gt; 2 d1 1 2 is be AUX #&gt; 3 d1 1 3 great great ADJ #&gt; 4 d1 1 4 at at ADP #&gt; 5 d1 1 5 fast fast ADJ #&gt; 6 d1 1 6 natural natural ADJ #&gt; 7 d1 1 7 language language NOUN #&gt; 8 d1 1 8 processing processing NOUN #&gt; 9 d1 1 9 . . PUNCT #&gt; 10 d2 1 1 Mr. Mr. PROPN #&gt; 11 d2 1 2 Smith Smith PROPN PERSON_B #&gt; 12 d2 1 3 spent spend VERB #&gt; 13 d2 1 4 two two NUM DATE_B #&gt; 14 d2 1 5 years year NOUN DATE_I #&gt; 15 d2 1 6 in in ADP #&gt; 16 d2 1 7 North North PROPN GPE_B #&gt; 17 d2 1 8 Carolina Carolina PROPN GPE_I #&gt; 18 d2 1 9 . . PUNCT Az elvégzett tokenizálás eredményébl adattáblát készíthetünk. spacy_tokenize(txt, remove_punct = TRUE, output = &quot;data.frame&quot;) %&gt;% tail() #&gt; doc_id token #&gt; 11 d2 spent #&gt; 12 d2 two #&gt; 13 d2 years #&gt; 14 d2 in #&gt; 15 d2 North #&gt; 16 d2 Carolina Ugyancsak lehetségünk van a különböz entitások, így például a tulajdonnevek kinyerésére. parsedtxt &lt;- spacy_parse(txt, lemma = FALSE, entity = TRUE, nounphrase = TRUE) entity_extract(parsedtxt) #&gt; doc_id sentence_id entity entity_type #&gt; 1 d2 1 Smith PERSON #&gt; 2 d2 1 North_Carolina GPE A tulajdonneveken túl felcímkézhetjük a dátumokat, eseményeket is. entity_extract(parsedtxt, type = &quot;all&quot;) #&gt; doc_id sentence_id entity entity_type #&gt; 1 d2 1 Smith PERSON #&gt; 2 d2 1 two_years DATE #&gt; 3 d2 1 North_Carolina GPE Az entity_consolidate() függvény segítségével arra is lehetségünk van, hogy a több szóból álló entitásokat egy tokenként kezeljük. entity_consolidate(parsedtxt) %&gt;% tail() #&gt; doc_id sentence_id token_id token pos entity_type #&gt; 11 d2 1 2 Smith ENTITY PERSON #&gt; 12 d2 1 3 spent VERB #&gt; 13 d2 1 4 two_years ENTITY DATE #&gt; 14 d2 1 5 in ADP #&gt; 15 d2 1 6 North_Carolina ENTITY GPE #&gt; 16 d2 1 7 . PUNCT A nounphrase_extract() függvény lehetséget ad az összetartozó kifejezések összefzésére. nounphrase_extract(parsedtxt) #&gt; doc_id sentence_id nounphrase #&gt; 1 d1 1 fast_natural_language_processing #&gt; 2 d2 1 Mr._Smith #&gt; 3 d2 1 two_years #&gt; 4 d2 1 North_Carolina Majd képes arra, hogy ezeket az összetartozó kifejezéseket egyben kezelje. nounphrase_consolidate(parsedtxt) #&gt; doc_id sentence_id token_id token #&gt; 1 d1 1 1 spaCy #&gt; 2 d1 1 2 is #&gt; 3 d1 1 3 great #&gt; 4 d1 1 4 at #&gt; 5 d1 1 5 fast_natural_language_processing #&gt; 6 d1 1 6 . #&gt; 7 d2 1 1 Mr._Smith #&gt; 8 d2 1 2 spent #&gt; 9 d2 1 3 two_years #&gt; 10 d2 1 4 in #&gt; 11 d2 1 5 North_Carolina #&gt; 12 d2 1 6 . #&gt; pos #&gt; 1 INTJ #&gt; 2 AUX #&gt; 3 ADJ #&gt; 4 ADP #&gt; 5 nounphrase #&gt; 6 PUNCT #&gt; 7 nounphrase #&gt; 8 VERB #&gt; 9 nounphrase #&gt; 10 ADP #&gt; 11 nounphrase #&gt; 12 PUNCT Arra is lehetség van, hogy az egyes kifejezések közötti függségeket vizsgáljuk. spacy_parse(txt, dependency = TRUE, lemma = FALSE, pos = FALSE) #&gt; doc_id sentence_id token_id token head_token_id dep_rel #&gt; 1 d1 1 1 spaCy 2 intj #&gt; 2 d1 1 2 is 2 ROOT #&gt; 3 d1 1 3 great 2 acomp #&gt; 4 d1 1 4 at 2 prep #&gt; 5 d1 1 5 fast 8 amod #&gt; 6 d1 1 6 natural 7 amod #&gt; 7 d1 1 7 language 8 compound #&gt; 8 d1 1 8 processing 4 pobj #&gt; 9 d1 1 9 . 2 punct #&gt; 10 d2 1 1 Mr. 2 compound #&gt; 11 d2 1 2 Smith 3 nsubj #&gt; 12 d2 1 3 spent 3 ROOT #&gt; 13 d2 1 4 two 5 nummod #&gt; 14 d2 1 5 years 3 dobj #&gt; 15 d2 1 6 in 3 prep #&gt; 16 d2 1 7 North 8 compound #&gt; 17 d2 1 8 Carolina 6 pobj #&gt; 18 d2 1 9 . 3 punct #&gt; entity #&gt; 1 #&gt; 2 #&gt; 3 #&gt; 4 #&gt; 5 #&gt; 6 #&gt; 7 #&gt; 8 #&gt; 9 #&gt; 10 #&gt; 11 PERSON_B #&gt; 12 #&gt; 13 DATE_B #&gt; 14 DATE_I #&gt; 15 #&gt; 16 GPE_B #&gt; 17 GPE_I #&gt; 18 A következkben a Szótáralapú elemzések, érzelem-elemzés fejezetben is használt Magyar Nemzeti Bank kamatdöntéseit kísér nemzetközi sajtóközleményei korpuszán mutatunk be egy példát a névelemfelismerésre és az eredmények vizualizálására. Els lépésként beolvassuk a szövegeket, majd a már megismert quanteda csomag segítségével korpuszt készítünk bellük. mnb_df &lt;- HunMineR::data_mnb_pr corpus_mnb &lt;-corpus(mnb_df) Ezután a spacy_extract_entity() függvénye segítségévek elvégezzük a névelemfelismerést, a függvény argumentumában megadva, hogy milyen tipusú névelemeket szeretnénk kigyjteni a korpuszból. A lehetséges típusok a named, extended, vagy all.59 Az elemzés eredménye pedig készülhet listában, vagy készülhet belle adattábla. Példánkban mi kimenetként listát állítottunk be. A névelemek tokenjeit ezután megritkítottuk, és csak azokat hagytuk meg, amelyek legalább nyolc alkalommal szerepeltek a korpuszban. mnb_ner &lt;- spacy_extract_entity( corpus_mnb, output = c(&quot;list&quot;), type = (&quot;named&quot;), multithread = TRUE) mnb_tokens &lt;- tokens(mnb_ner) features &lt;- dfm(mnb_tokens) %&gt;% dfm_trim(min_termfreq = 8) %&gt;% featnames() mnb_tokens &lt;- tokens_select(mnb_tokens, features, padding = TRUE) Ezután a különböz alakban elforduló, de ugyanarra az entitásra vonatkozó névelemeket összevontuk. mnb &lt;- c(&quot;Magyar Nemzeti Bank&quot;, &quot;MAGYAR NEMZETI BANK&quot;, &quot;The Magyar Nemzeti Bank &quot;, &quot;the Magyar Nemzeti Bank&quot;, &quot;MNB&quot;, &quot;the Magyar Nemzeti Banks&quot;, &quot;Nemzeti Bank&quot;, &quot;The Magyar Nemzeti Banks MNB&quot; ) lemma &lt;- rep(&quot;Magyar Nemzeti Bank&quot;, length(mnb)) mnb_tokens &lt;- tokens_replace(mnb_tokens, mnb, lemma, valuetype = &quot;fixed&quot;) mc &lt;- c(&quot;Monetary Council&quot;, &quot;MONETARY COUNCIL&quot;, &quot;Magyar Nemzeti Bank Monetary Council&quot;, &quot;MAGYAR NEMZETI BANK Monetary Council&quot;, &quot;NEMZETI BANK Monetary Council&quot; ,&quot;The Monetary Council&quot;, &quot;Council&quot;, &quot;The Council&quot;, &quot;Councils&quot;, &quot;the Monetary Council&quot;, &quot;Monetary Councils&quot;, &quot;the Monetary Councils&quot;, &quot;The Monetary Councils&quot;, &quot;Monetray Council&quot;, &quot;May the Monetary Council&quot;) lemma &lt;- rep(&quot;Monetary Council&quot;, length(mc)) mnb_tokens &lt;- tokens_replace(mnb_tokens, mc, lemma, valuetype = &quot;fixed&quot;) Majd elkészítettük a szóbeágyazás fejezetben már megismert fcm-et, végezetül pedig egy együttes elfordulási mátrixot készítettünk a kinyert entitásokból és a ggplot() segítségével ábrázoltuk (ld 11.4. ábra).60 Az így kapott ábránk láthatóvá teszi, hogy mely szavak fordulnak el jellemzen együtt, valamint a vonalvastagsággal azt is egmutatja, hogy ez relatív értelemben milyen gyakran történik. mnb_fcm &lt;- fcm(mnb_tokens, context = &quot;window&quot;, count = &quot;weighted&quot;, weights = 1 / (1:5), tri = TRUE) feat &lt;- names(topfeatures(mnb_fcm, 80)) mnb_fcm_select &lt;- fcm_select(mnb_fcm, pattern = feat, selection = &quot;keep&quot;) dim(mnb_fcm_select) #&gt; [1] 41 41 size &lt;- log(colSums(dfm_select(mnb_fcm, feat, selection = &quot;keep&quot;))) set.seed(144) textplot_network(mnb_fcm_select, min_freq = 0.7, vertex_size = size / max(size) * 3) Ábra 11.4: A Magyar Nemzeti Bank korpusz névelemeinek együttelofordulási mátrixa A spacyr alapveten az angol nyelvi modellel mködik, de arra is van lehetség, hogy a spaCy egyéb beépített nyelvi modelljeit (német, spanyol, portugál, stb.) használjuk. Létezik magyar nyelvi modell is, ez azonban jelenleg még nincs integrálva a spaCy-be, hanem egy GitHub repozitoriumból tölthet le. Ennek R-ben történ megvalósításához azonban haladó R ismeretek szükségesek, azért ennek leírásától itt eltekintünk.61 Magyar nyelv szövegek NLP elemzésére használható eszközök részletes listája: https://github.com/oroszgy/awesome-hungarian-nlp A magyarlanc elérhet a https://rgai.inf.u-szeged.hu/magyarlanc oldalon, az innen letölthet jar fájl segítségével a txt formátumú szövegfájlok elemzése parancssorból lehetséges. Az UDPipe elérhetsége: http://lindat.mff.cuni.cz/services/udpipe A napirend eltti felszólalásokat tartalmazó korpusz a Hungarian Comparative Agendas Project keretében készült: https://cap.tk.hu/hu További lehetségekhez lásd például: https://analyticsindiamag.com/top-10-r-packages-for-natural-language-processing-nlp/ Részletes leírása: https://spacyr.quanteda.io/articles/using_spacyr.html Részletes leírása: https://spacy.io/usage/linguistic-features#named-entities Részletes leírását lásd: https://towardsdatascience.com/extend-named-entity-recogniser-ner-to-label-new-entities-with-spacy-339ee5979044 Részletes leírását lásd: https://tutorials.quanteda.io/basic-operations/fcm/fcm/ A magyar nyelvi modell és leírása elérhet az alábbi linken: https://github.com/oroszgy/spacy-hungarian-models "],["felugyelt.html", "12 Osztályozás és felügyelt tanulás 12.1 Fogalmi alapok 12.2 Osztályozás felügyelt tanulással", " 12 Osztályozás és felügyelt tanulás 12.1 Fogalmi alapok A mesterséges intelligencia két fontos társadalomtudományi alkalmazási területe a felügyelet nélküli és a felügyelt tanulás. Míg az els esetben  ahogy azt a Felügyelet nélküli tanulás fejezetben bemutattuk  az emberi beavatkozás néhány kulcsparaméter megadására (így pl. a kívánt topikok számának meghatározására) szorítkozik, addig a felügyelt tanulás esetében a kutatónak nagyobb mozgástere van tanítani a gépet. Ennyiben a felügyelt tanulás alkalmasabb hipotézisek tesztelésére, mint az adatok rejtett mintázatait felfedez felügyelet nélküli tanulás. A felügyelt tanulási feladat megoldása egy úgynevezett tanító halmaz (training set) meghatározásával kezddik, melynek során a kutatók saját maguk végzik el kézzel azt a feladatot melyet a továbbiakban gépi közremködéssel szeretnének nagyobb nagyságrendben, de egyben érvényesen (validity) és megbízhatóan (reliability) kivitelezni. Eredményeinket az ugyanúgy eredetileg kézzel lekódolt, de a modell-építés során félretett teszthalmazunkon (test set) értékelhetjük. Ennek során négy kategóriába rendezzük modellünk elrejelzéseit. Egy, a politikusi beszédeket a pozitív hangulatuk alapján osztályozó példát véve ezek a következk: azok a beszédek amelyeket a modell helyesen sorolt be pozitívba (valódi pozitív), vagy negatívba (valódi negatív), illetve azok, amelyek hibásan szerepelnek a pozitív (ál-pozitív), vagy a negatív kategóriában (ál-negatív). Mindezek együttesen egy ún. tévesztési táblát (confusion matrix) adnak, melynek további elemzésével ítéletet alkothatunk modellépítésünk eredményességérl. A felügyelt tanulás számos kutatási feladat megoldására alkalmazhatjuk, melyek közül a leggyakoribbak a különböz osztályozási (classification) feladatok. Miközben ezek  így pl. a véleményelemzés  szótáralapú módszertannal is megoldhatóak (lásd a Szótárak és érzelemelemzés fejezetet), a felügyelt tanulás a nagyobb élmunkaigényt rendszerint jobb eredményekkel és rugalmasabb felhasználhatósággal hálálja meg (gondoljunk csak a szótárak domain-függségére). A felügyelt tanulás egyben a mesterséges intelligencia kutatásának gyorsan fejld területe, mely az e fejezetben tárgyalt algoritmus-központú gépi tanuláson túl az ún. mélytanulás (deep learning) és a neurális hálók területén is zajlik egyre látványosabb sikerekkel. Jelen fejezetben két modell fajtát mutatunk be egyazon korpuszon az Support Vector Machine (SVM) és a Naïve Bayes (NB). Mindkett a fentiekben leírt klasszifikációs feladatot végzi el, viszont eltér módon mködnek. Az SVM a tanító halmazunk dokumentumait vektorként reprezentálja, ami annyit jelent, hogy hozzájuk rendel egy számsort, amely egy közös térben betöltött pozíciójukat reprezentálja. A teszthalmaz dokumentumai egy közös térben reprezentálva Ezt követen pedig a különféle képpen képpen felcímkézett dokumentumok között egy vonalat (hyperplane) húz meg, amely a lehet legnagyobb távolságra van minden egyes dokumentumtól. A minta osztályozása Innentl kezdve pedig a modellnek nincs más dolga, mint a tanító halmazon kívül es dokumentumok vektor értékeit is megállapítani, elhelyezni ket ebben a közös térben, és attól függen, hogy a hyperplane mely oldalára kerülnek besorolni ket. Ezzel szemben az NB a felcímkézett tanító halmazunk szavaihoz egy valószínségi értéket rendel annak függvényében, hogy az adott szó adott kategóriába tartozó dokumentumokban hányszor jelenik meg, az adott kategória dokumentumainak teljes szószámához képest. Miután a teszt halmazunk dokumentumaiban minden szóhoz hozzárendeltük ezeket a valószínségi értékeket nincs más dolgunk, mint a teszt halmazunkon kívül es dokumentumokban felkeresni ugyanezen szavakat, a hozzájuk rendelt valószínségi értékeket aggregálni és ez alapján minden dokumentumhoz tudunk rendelni több valószínségi értéket is, amelyek megadják, hogy mekkora eséllyel tartozik a dokumentum a teszt halmazunk egyes kategóriáiba. 12.2 Osztályozás felügyelt tanulással Az alábbi fejezetben a CAP magyar média gyjteményébl a napilap címlapokat tartalmazó modult használjuk.62 Az induló adatbázis 61 835 cikk szövegét és metaadatait (összesen öt változót: sorszám, fájlnév, a közpolitikai osztály kódja, szöveg, illetve a korpusz forrása  Magyar Nemzet vagy Népszabadság) tartalmazza. Az a célunk, hogy az egyes cikkekhez kézzel, jó minségben (két, egymástól függetlenül dolgozó kódoló által) kiosztott és egyeztetett közpolitikai kódokat  ez a tanítóhalmaz  arra használjuk, hogy meghatározzuk egy kiválasztott cikkcsoport hasonló kódjait. Az osztályozási feladathoz a CAP közpolitikai kódrendszerét használjuk, mely 21 közpolitikai kategóriát határoz meg az oktatástól az egészségügyön át a honvédelemig.63 Annak érdekében, hogy egyértelmen értékelhessük a gépi tanulás hatékonyságát, a kiválasztott cikkcsoport (azaz a teszthalmaz) esetében is ismerjük a kézi kódolás eredményét (éles kutatási helyzetben, ismeretlen kódok esetében ugyanakkor ezt gyakran szintén csak egy kisebb mintán tudjuk kézzel validálni). További fontos lépés, hogy az észszer futási id érdekében a gyakorlat során a teljes adatbázisból  és ezen belül is csak a Népszabadság részhalmazból  fogunk venni egy 4500 darabos mintát. Ezen a mintán pedigg a már korábban említett kétféle modellt fogjuk futtatni a NB-t és az SVM-t. Az ezekkel a módszerekkel létrehozott két modellünk hatékonyságát fogjuk összehasonlítani, valamint azt is megfogjuk nézni, hogy az eredmények megbízhatósága mennyiben közelíti meg a kézikódolási módszerre jellemz 80-90%-os pontosságot. Elször behívjuk a szükséges csomagokat. Majd a felügyelet nélküli tanulással foglalkozó fejezethez hasonlóan itt is alkalmazzuk a set.seed() funkciót, mivel anélkül nem egyeznének az eredményeink teljes mértékben egy a kódunk egy késöbbi újrafuttatása esetén. library(stringr) library(dplyr) library(quanteda) library(caret) library(quanteda.textmodels) library(HunMineR) set.seed(1234) Ezt követen betöltjük a HunMineR-bl az adatainkat. Jelen esetben is vethetünk egy pillantást az objektum tartalmára a glimpse funkció segítségével, és láthatjuk, hogy az öt változónk a cikkek tényleges szövege, a sorok száma, a fájlok neve, az újság, ahol a cikk megjelent, valamint a cikk adott témába való besorolása, amely kézi kódolással került hozzárendelésre. Data_NOL_MNO &lt;- HunMineR::data_nol_mno_clean glimpse(Data_NOL_MNO) #&gt; Rows: 71,875 #&gt; Columns: 5 #&gt; $ row_number &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1~ #&gt; $ filename &lt;chr&gt; &quot;nol_1990_01_02_01.txt&quot;, &quot;nol_1990_01_02_02.~ #&gt; $ majortopic_code &lt;dbl&gt; 19, 19, 19, 1, 1, 19, 19, 1, 1, 19, 19, 1, 1~ #&gt; $ text &lt;chr&gt; &quot;változás év választás év világ ünneplés köz~ #&gt; $ corpus &lt;chr&gt; &quot;NOL&quot;, &quot;NOL&quot;, &quot;NOL&quot;, &quot;NOL&quot;, &quot;NOL&quot;, &quot;NOL&quot;, &quot;N~ Majd pár kissebb átalakítást hajtunk végre az adatainkon. Elször is ehhez a modellhez most csak a Népszabadság cikkeket fogjuk alkalmazni a Magyar Nemzet cikkeit kiszedjük az adataink közül. Ezután leegyszersítjük a kódolást. Összesen 21 témakört lefed a cikkek kézi kódolása most viszont ezekbl egy bináris változót készítünk, amelynek 1 lesz az értéke, ha a cikk makrogazdaságról szól, ha pedig bármilyne más témakörrl, akkor 0 lesz az értéke. Így két hasonló méret csoportot hozunk létre. A modellek pedig az ide történ besorolálst próbálják majd megállapítani a cikkek szövege alapján vagyis, hogy makrogazdasági témáról szól-e az adott cikk, vagy valamilyen más témáról. Majd kiválasztjuk a táblából a két változó, amelyekre szükségünk lesz a cikkek szövegét text és a cikkekhez kézzel hozzárendelt témt label a többit pedig elvetjük. Data_NOL &lt;- Data_NOL_MNO %&gt;% filter(text != &quot; &quot; | text != &quot;&quot;, corpus == &quot;NOL&quot;) %&gt;% mutate(label = if_else(majortopic_code == 1, 1, 0)) %&gt;% group_by(label) %&gt;% sample_n(2250) %&gt;% ungroup() %&gt;% select(text, label) Ezt követen egy korpusszá alakítjuk az objektumunkat. nol_corpus &lt;- corpus(Data_NOL) Majd létrehozunk egy új objektumot id_train néven amely 1 és 4500 között 3600 különböz a sample() funkció segítségével véletlenszeren kiválogatott számot tartalmaz. Utána pedig a korpuszunkhoz hozzá adunk egy új id_numeric elnevezés változót, amely csupán a megszámozza az egyes cikkeket. Ezeket a késöbbiekben arra fogjuk felhasználni, hogy kialakítsuk a tanító és teszt halmazokat id_train &lt;- sample(1:4500, 3600, replace = FALSE) nol_corpus$id_numeric &lt;- 1:ndoc(nol_corpus) Ezt követen a korpuszunkat egy tokenekké alakítjuk és ezzel egyidejleg eltávolítjuk a quanteda beépített magyar stopszó szótárát, valamint sztemmelést is végrehajtunk. nol_toks &lt;- tokens(nol_corpus, remove_punct = TRUE, remove_number = TRUE) %&gt;% tokens_remove(pattern = stopwords(&quot;hungarian&quot;)) %&gt;% tokens_wordstem() A tokeneket pedig egy dfm-é alakítjuk. nol_dfm &lt;- dfm(nol_toks) Ezt követen kialakítjunk a tanító és teszt halmazokat a korábban létrehozott változók segítségével. A lenti kód elssora egy subsetet alakít ki az adattáblánk mindazon soraiból, amelyek id_numeric változója szerepel az id_train számsorban ez lesz a tanító alhalmaz. Majd ennek megfelelen egy másik alhalmazt alakítunk ki a teszteléshez, amely minden olyan sorát fogja tartalmazni adattáblánknak, amely id_numeric változója nem szerepel az id_train számsorban. train_dfm &lt;- dfm_subset(nol_dfm, id_numeric %in% id_train) test_dfm &lt;- dfm_subset(nol_dfm, !id_numeric %in% id_train) Azáltal pedig, hogy kialakítottuk a két alhalmazt el is végeztük az utolsó elkészítési folyamatot is, az így elkészített adatokon tudjuk majd futtatni a következkben, mind az NB és az SVM modellünket. 12.2.1 Naïve Bayes A fejezet Naïve Bayesre vonatkozó kódjai quanteda tutoriál menetét és kódjait követik.64 Elször létrehozzuk a modellünket, amely számára meghatározzuk, hogy a label változóhoz kell egy predikciót adnia, majd ezt alakalmazzuk az adatainkra. A dfm_match parancs segítségével eltávolítjuk a vizsgált dokumentumaink szavai közül mindazokat, amelyek nem szerepeltek a teszt halmazunkba. Erre azért van szükség, mivel az NB csak azokat a szavakat képes kezelni, amelyekhez már hozzárendelt egy valószínségi értéket, tehát csak azokat, amelyek a teszt halmazban is megtalálhatóak. nol_nb &lt;- textmodel_nb(train_dfm, train_dfm$label) dfm_kozos &lt;- dfm_match(test_dfm, features = featnames(train_dfm)) A következ objektumban eltároljuk a kézikódolás eredményeit, amelyeket már ismerünk. tenyleges_osztaly &lt;- dfm_kozos$label Majd eltároljuk egy másik objektumban azokat az eredményeket, amelyeket a modellünk generált. becsult_osztaly &lt;- predict(nol_nb, newdata = dfm_kozos) A két fenti adat segítségével pedig létrehozhatjuk a korábban is említett tévesztési táblát. eredmeny_tabla &lt;- table(tenyleges_osztaly, becsult_osztaly) eredmeny_tabla #&gt; becsult_osztaly #&gt; tenyleges_osztaly 0 1 #&gt; 0 391 65 #&gt; 1 72 372 Ezt létrehozhatjuk a caret csomag funkciójával is, amely a tévesztési tábla mellett sok más hasznos adatot is megad a számunkra. confusionMatrix(eredmeny_tabla, mode = &quot;everything&quot;) #&gt; Confusion Matrix and Statistics #&gt; #&gt; becsult_osztaly #&gt; tenyleges_osztaly 0 1 #&gt; 0 391 65 #&gt; 1 72 372 #&gt; #&gt; Accuracy : 0.848 #&gt; 95% CI : (0.823, 0.871) #&gt; No Information Rate : 0.514 #&gt; P-Value [Acc &gt; NIR] : &lt;2e-16 #&gt; #&gt; Kappa : 0.695 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 0.608 #&gt; #&gt; Sensitivity : 0.844 #&gt; Specificity : 0.851 #&gt; Pos Pred Value : 0.857 #&gt; Neg Pred Value : 0.838 #&gt; Precision : 0.857 #&gt; Recall : 0.844 #&gt; F1 : 0.851 #&gt; Prevalence : 0.514 #&gt; Detection Rate : 0.434 #&gt; Detection Prevalence : 0.507 #&gt; Balanced Accuracy : 0.848 #&gt; #&gt; &#39;Positive&#39; Class : 0 #&gt; Az eredményeken pedig láthatjuk, hogy még az egyszerség kedvéért lecsökkentett méret tanítási halmaz ellenére is egy kifejezett magas 84.78%-os pontossági értéket kapunk, amely többé-kevésbé megfeletethet egy kizárólag kézikódolással végzett vizsgálat pontosságának 12.2.2 Support-vector machine A következ kódsorral alkalmazzuk az SVM modellünket, hogy az adatainkon belül a label változóra vonatkozóan készítsen predikciót. nol_svm &lt;- textmodel_svm(train_dfm, y = train_dfm$label) Ismét eltároljuk a kézikódolás eredményeit, amelyeket már ismerünk. Valamint az eredményeket, amelyeket az SVM modellünk generált szintén eltároljuk egy objektumba. tenyleges_osztaly_svm &lt;- dfm_kozos$label becsult_osztaly_svm &lt;- predict(nol_svm, newdata = dfm_kozos) Ismét létrehozhatunk egy egyszer tévesztési táblát. eredmeny_tabla_svm &lt;- table(tenyleges_osztaly_svm, becsult_osztaly_svm) eredmeny_tabla_svm #&gt; becsult_osztaly_svm #&gt; tenyleges_osztaly_svm 0 1 #&gt; 0 369 87 #&gt; 1 45 399 Valamint jelen esetben is használhatjuk a caret csomagban található funkciót, hogy még több információt nyerjünk ki a modellünk mködésérl. confusionMatrix(eredmeny_tabla_svm, mode = &quot;everything&quot;) #&gt; Confusion Matrix and Statistics #&gt; #&gt; becsult_osztaly_svm #&gt; tenyleges_osztaly_svm 0 1 #&gt; 0 369 87 #&gt; 1 45 399 #&gt; #&gt; Accuracy : 0.853 #&gt; 95% CI : (0.829, 0.876) #&gt; No Information Rate : 0.54 #&gt; P-Value [Acc &gt; NIR] : &lt; 2e-16 #&gt; #&gt; Kappa : 0.707 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 0.000359 #&gt; #&gt; Sensitivity : 0.891 #&gt; Specificity : 0.821 #&gt; Pos Pred Value : 0.809 #&gt; Neg Pred Value : 0.899 #&gt; Precision : 0.809 #&gt; Recall : 0.891 #&gt; F1 : 0.848 #&gt; Prevalence : 0.460 #&gt; Detection Rate : 0.410 #&gt; Detection Prevalence : 0.507 #&gt; Balanced Accuracy : 0.856 #&gt; #&gt; &#39;Positive&#39; Class : 0 #&gt; Itt ismét azt találjuk, hogy a csökkentett méret korpusz ellenére is egy kifejezetten magas pontossági értéket 85.33%-ot kapunk. A korpusz regisztációt követen elérhet az alábbi linken: https://cap.tk.hu/a-media-es-a-kozvelemeny-napirendje A kódkönyv regisztrációt követen elérhet az alábbi linken: https://cap.tk.hu/kozpolitikai-cap https://tutorials.quanteda.io/machine-learning/nb/ "],["függelék.html", "13 Függelék 13.1 Az R és az RStudio használata 13.2 Munka saját adatokkal 13.3 Vizualizáció", " 13 Függelék 13.1 Az R és az RStudio használata Az R egy programozási nyelv, amely alkalmas statisztikai számítások elvégzésére és ezek eredményeinek grafikus megjelenítésére. Az R ingyenes, nyílt forráskódú szoftver, mely telepíthet mind Windows, mind Linux, mind MacOS operációs rendszerek alatt az alábbi oldalról: https://cran.r-project.org/ Az RStudio az R integrált fejleszti környezete (integrated development environment, IDE), mely egy olyan felhasználóbarát felületet biztosít, ami egyszerbb és átláthatóbb munkát tesz lehetvé. Az RStudio az alábbi oldalról tölthet le: https://rstudio.com/products/rstudio/download/ A point and click szoftverekkel szemben az R használata során scripteket kell írni, ami bizonyos programozási jártasságot feltételez, de a késbbiekben lehetvé teszi azt adott kutatási kérdéshez maximálisan illeszked kódok összeállítását, melyek segítségével az elemzések mások számára is megbízhatóan reprodukálhatók lesznek. Ugyancsak az R használata mellett szól, hogy komoly fejleszti és felhasználói közösséggel rendelkezik, így a használat során felmerül problémákra általában gyorsan megoldást találhatunk. 13.1.1 Az RStudio kezdfelülete Az RStudio kezdfelülete négy panelbl, eszközsorból és menüsorból áll: Ábra 13.1: RStudio felhasználói felület Az (1) editor ablak szolgál a kód beírására, futtatására és mentésére. A (2) console ablakban jelenik meg a lefuttatott kód és az eredmények. A jobb fels ablak (3) environment fülén láthatóak a memóriában tárolt adatállományok, változók és felhasználói függvények. A history fül mutatja a korábban lefuttatott utasításokat. A jobb alsó ablak (4) files fülén az aktuális munkakönyvtárban tárolt mappákat és fájlok találjuk, míg a plot fülön az elemzéseink során elkészített ábrák jelennek meg. A packages fülön frissíthetjük a meglév r csomagokat és telepíthetünk újakat. A help fülön a különböz függvények, parancsok leírását, és használatát találjuk meg. A Tools -&gt; Global Options menüpont végezhetjük el az RStudio testreszabását. Így például beállíthatjuk az ablaktér elrendezését (Pane layout), vagy a színvilágot (Appearance), illetve azt hogy a kódok ne fussanak ki az ablakból (Code -&gt; Editing -&gt; Soft wrap R source files). 13.1.2 A projektalapú munka Bár nem kötelez, de javasolt, hogy az RStudio-ban projekt alapon dolgozzunk, mivel így az összes  az adott projekttel kapcsolatos fájlt  egy mappában tárolhatjuk. Új projekt beállítását a File-&gt;New Project menüben tehetjük meg, ahol a saját gépünk egy könyvtárát kell kiválasztani, ahová az R a scripteket, az adat- és elzményfájlokat menti. Ezenkívül a Tools-&gt;Global Options-&gt;General menüpont alatt le kell tiltani a Restore most recently opened project at startup és a Restore .RData ino workspace at startup beállítást, valamint Save workspace to .RData on exit legördül menüjében be kell állítani a Never értéket. Ábra 13.2: RStudio projekt beállítások A szükséges beállítások után a File -&gt; New Project menüben hozhatjuk létre a projektet. Itt arra is lehetségünk van, hogy kiválasszuk, hogy a projektünket egy teljesen új könyvtárba, vagy egy meglévbe kívánjuk menteni, esetleg egy meglév projekt új verzióját szeretnénk létrehozni. Ha sikeresen létrehoztuk a projektet, az RStudio jobb fels sarkában látnunk kell annak nevét. 13.1.3 Scriptek szerkesztése, függvények használata Új script a File -&gt; New -&gt; File -&gt; R Script menüpontban hozható létre, mentésére a File-&gt;Save menüpontban egy korábbi script megnyitására File -&gt; Open menüpontban van lehetségünk. Script bármilyen szövegszerkesztvel írható, majd beilleszthet az editor ablakba. A scripteket érdemes magyarázatokkal (kommentekkel) ellátni, hogy a késbbiekben pontosan követhet legyen, hogy melyik parancs segítségével pontosan milyen lépéseket hajtottunk végre. A magyarázatokat vagy más néven kommenteket kettskereszt (#) karakterrel vezetjük be. A scriptbeli utasítások az azokat tartalmazó sorokra állva vagy több sort kijelölve a Run feliratra kattintva vagy a Ctrl+Enter billentyparanccsal futtathatók le. A lefuttatott parancsok és azok eredményei ezután a bal alsó sarokban lév console ablakban jelennek meg és ugyanitt kapunk hibaüzenetet is, ha valamilyen hibát vétettünk a script írása közben. A munkafolyamat során létrehozott állományok (ábrák, fájlok) az ún. munkakönyvtárba (working directory) mentdnek. Az aktuális munkakönyvtár neve, elérési útja a getwd() utasítással jeleníthet meg. A könyvtárban található állományok listázására a list.files() utasítással van lehetségünk. Ha a korábbiaktól eltér munkakönyvtárat akarunk megadni, azt a setwd() függvénnyel tehetjük meg, ahol a ()-ben az adott mappa elérési útját kell megadnunk. Az elérési útban a meghajtó azonosítóját, majd a mappák, almappák nevét vagy egy normál irányú perjel (/), vagy két fordított perjel (\\\\) választja el, mivel az elérési út karakterlánc, ezért azt idézjelek vagy aposztrófok közé kell tennünk. Az aktuális munkakönyvtárba beléphetünk a jobb alsó ablak file lapján a More -&gt; Go To Working Directory segítségével. Ugyanitt a Set Working Directory-val munkakönyvtárnak állíthatjuk be az a mappát, amelyben épp benne vagyunk. Ábra 13.3: Working directory beállítások A munkafolyamat befejezésére a q() vagy quit() függvénnyel van lehetségünk. Az R-ben objektumokkal dolgozunk, amik a teljesség igénye nélkül lehetnek például egyszer szám vektortok, vagy akár komplex listák, illetve függvények, ábrák. A munkafolyamat során létrehozott objektumokat az RStudio jobb fels ablakának environment fülén jelennek meg. A mentett objektumokat a fent látható sepr ikonra kattintva törölhetjük a memóriából. Az environment ablakra érdemes úgy gondolni hogy ott jelennek meg a memóriában tárolt értékek. Az RStudio jobb alsó ablakának plots fülén láthatjuk azon parancsok eredményét, melyek kimenete valamilyen ábra. A packages fülnél a már telepített és a letölthet kiegészít csomagokat jeleníthetjük meg. A help fülön a korábban említettek szerint a súgó érhet el. Az RStudio-ban használható billentyparancsok teljes listáját Alt+Shift+K billentykombinációval tekinthetjük meg. Néhány gyakrabban használt, hasznos billentyparancs: Ctrl+Enter: futtassa a kódot az aktuális sorban Ctrl+Alt+B: futtassa a kódot az elejétl az aktuális sorig Ctrl+Alt+E: futtassa a kódot az aktuális sortól a forrásfájl végéig Ctrl+D: törölje az aktuális sort Az R-ben beépített függvények (function) állnak rendelkezésünkre a számítások végrehajtására, emellett több csomag (package) is letölthet, amelyek különböz függvényeket tartalmaznak. A függvények a következképpen épülnek fel: függvénynév(paraméter). Például tartalom képernyre való kiíratását a print() függvénnyel tehetjük, amelynek gömböly zárójelekkel határolt részébe írhatjuk a megjelenítend szöveget. A citation() függvénnyel lekérdezhetjük az egyes beépített csomagokra való hivatkozást is: a citation(quanteda) függvény a quanteda csomag hivatkozását adja meg. Az R súgórendszere a help.start() utasítással indítható el. Egy adott függvényre vonatkozó súgórészlet a függvények neve elé kérdjel írásával, vagy a help() argumentumába a kérdéses függvény nevének beírásával jeleníthet meg (pl.: help(sum)). 13.1.4 R csomagok Az R-ben telepíthetk kiegészít csomagok (packages), amelyek alapértelmezetten el nem érhet algoritmusokat, függvényeket tartalmaznak. A csomagok saját dokumentációval rendelkeznek, amelyeket fel kell tüntetni a használatukkal készült publikációink hivatkozáslistájában. A csomagok telepítésre több lehetségünk is van: használhatjuk a menüsor Tools -&gt; Install Packages menüpontját, vagy a jobb alsó ablak packages fül Install menüpontját, illetve az editor ablakban az install.packages() parancsot futtatva, ahol a ()-be a telepíteni kívánt csomag nevét kell beírnunk (pl.: install.packages(\"dplyr\")). Ahhoz, hogy egy csomag funkcióit használjuk azt be kell töltetnünk a library() parancs segítségével, itt megintcsak a használni kívánt csomag nevét kell a zárójelek közé helyeznünk, viszont ebben az esetben nem szükséges idézjelek közé helyeznünk zat (pl.: library(dplyr)) ameddig ezt a parancsot nem futattjuk le az adott csomag funkció nem lesznek elérhetek számunkra. Ábra 13.4: Packages fül 13.1.5 Objektumok tárolása, értékadás {object} Az objektumok lehetnek például vektorok, mátrixok, tömbök (array), adat táblák (data frame). Értékadás nélkül az R csak megjeleníti a mveletek eredményét, de nem tárolja el azokat. Az eredmények eltárolásához azokat egy objektumba kell elmentenünk. Ehhez meg kell adnunk az objektum nevét majd az &lt;- után adjuk meg annak értékét: a &lt;- 12 + 3.Futtatás után az environments fülön megjelenik az a objektum, melynek értéke 15. Az objektumok elnevezésénél figyelnünk kell arra, hogy az R különbséget tesz a kis és nagybetk között, valamint, hogy az ugyanolyan nev objektumokat kérdés nélkül felülírja és ezt a felülírást nem lehet visszavonni. 13.1.6 Vektorok {vector} Az R-ben kétféle típusú vektort különböztetünk meg: atomi vektor (atomic vector) lista (list) Az atomi vektornak hat típusa van, logikai (logical), egész szám (integer), természetes szám (double), karakter (character), komplex szám (complex) és nyers adat (raw). A leggyakrabban valamilyen numerikus, logikai vagy karakter vektorral használjuk. Az egyedüli vektorok onnan kapták a nevüket hogy csak egy féle adattípust tudnak tárolni. A listák ezzel szemben gyakorlatilag bármit tudnak tárolni, akár több listát is egybeágyazhatunk. A vektorok és listák azok az építelemek amikbl felépülnek az R objektumaink. Több érték vagy azonos típusú objektum összefzését a c() függvénnyel végezhetjük el. A lenti példában három különböz objektumot kreálunk, egy numerikusat, egy karaktert és egy logikait. A karakter vektorban az elemeket idzjellel és vesszvel szeparáljuk. A logikai vektor csak TRUE, illetve FALSE értékeket tartalmazhat. numerikus &lt;- c(1,2,3,4,5) karakter &lt;- c(&quot;kutya&quot;,&quot;macska&quot;,&quot;ló&quot;) logikai &lt;- c(TRUE, TRUE, FALSE) A létrehozott vektorokkal különböz mveleteket végezhetünk el, például összeadhatjuk numerikus vektorainkat. Ebben az esetben az els vektor els eleme a második vektor els eleméhez adódik. c(1:4) + c(10,20,30,40) #&gt; [1] 11 22 33 44 A karaktervektorokat össze is fzhetjük egymással. Példánkban egy új objektumot is létrehoztunk, ezért a jobb fels ablakban, az environment fülön láthatjuk, hogy a létrejött karakter_kombinalt objektum egy négy elem (hosszúságú) karaktervektor (chr [1:4]), melynek elemei a \"kutya\",\"macska\",\"ló\",\"nyúl\". Az objektumként tárolt vektorok tartalmát az adott sort lefuttatva írathatjuk ki a console ablakba. Ugyanezt megtehetjük print() függvény segítségével is, ahol a függvény arrgumentumában () az adott objektum nevét kell szerepeltetnünk. karakter1 &lt;- c(&quot;kutya&quot;,&quot;macska&quot;,&quot;ló&quot;) karakter2 &lt;-c(&quot;nyúl&quot;) karakter_kombinalt &lt;-c(karakter1, karakter2) karakter_kombinalt #&gt; [1] &quot;kutya&quot; &quot;macska&quot; &quot;ló&quot; &quot;nyúl&quot; Ha egy vektorról szeretnénk megtudni, hogy milyen típusú azt a typeof() vagy a class() paranccsal tehetjük meg, ahol ()-ben az adott objektumként tárolt vektor nevét kell megadnunk: typeof(karakter1). A vektor hosszúságát (benne tárolt elemek száma vektorok esetén) a lenght() függvénnyel tudhatjuk meg. typeof(karakter1) #&gt; [1] &quot;character&quot; length(karakter1) #&gt; [1] 3 13.1.7 Faktorok A faktorok a kategórikus adatok tárolására szolgálnak. Faktor típusú változó a factor() függvénnyel hozható létre. A faktor szintjeit (igen, semleges, nem), a levels() függvénnyel kaphatjuk meg míg az adatok címkéit (tehát a kapott válaszok számát), a labels() paranccsal érhetjük el. survey_response &lt;- factor(c(&quot;igen&quot;, &quot;semleges&quot;, &quot;nem&quot;, &quot;semleges&quot;, &quot;nem&quot;, &quot;nem&quot;, &quot;igen&quot;), ordered = TRUE) levels(survey_response) #&gt; [1] &quot;igen&quot; &quot;nem&quot; &quot;semleges&quot; labels(survey_response) #&gt; [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; 13.1.8 Data frame Az adattábla (data frame) a statisztikai és adatelemzési folyamatok egyik leggyakrabban használt adattárolási formája. Egy data frame többféle típusú adatot tartalmazhat. A data frame-k különféle oszlopokból állhatnak, amelyek különféle típusú adatokat tartalmazhatnak, de egy oszlop csak egy típusú adatból állhat. Az itt bemutatott data frame 7 megfigyelést és 4 féle változót tartalmaz (id, country, pop, continent). #&gt; id orszag nepesseg kontinens #&gt; 1 1 Thailand 68.7 Asia #&gt; 2 2 Norway 5.2 Europe #&gt; 3 3 North Korea 24.0 Asia #&gt; 4 4 Canada 47.8 North America #&gt; 5 5 Slovenia 2.0 Europe #&gt; 6 6 France 63.6 Europe #&gt; 7 7 Venezuela 31.6 South America A data frame-be rendezett adatokhoz különböz módon férhetünk hozzá, például a data frame nevének majd []-ben a kívánt sor megadásával, kiírathatjuk a console ablakba annak tetszleges sorát ás oszlopát: orszag_adatok[1, 1]. Az R több különböz módot kínál a data frame sorainak és oszlopainak eléréséhez. A [ általános használata: data_frame[sor, oszlop]. Egy másik megoldás a $ haszálata: data_frame$oszlop. orszag_adatok[1, 4] #&gt; [1] Asia #&gt; Levels: Asia Europe North America South America orszag_adatok$orszag #&gt; [1] &quot;Thailand&quot; &quot;Norway&quot; &quot;North Korea&quot; &quot;Canada&quot; #&gt; [5] &quot;Slovenia&quot; &quot;France&quot; &quot;Venezuela&quot; 13.2 Munka saját adatokkal Saját adatainkat legegyszerbben a munkakönyvtárból (working directory) hívhatjuk be. A munkakönyvtár egy olyan mappa a számítógépünkön, amely közvetlenül kapcsolatban van az éppen megnyitott R scripttel vagy projekttel. Amennyiben nem határozzuk meg , hogy adatokat honnan szeretnénk behívni, akkor azokat mindig innen fogja megpróbálni betölteni az R. A getwd() parancs segítségével bármikor megtekinthetjük az aktuális munkakönyvtárunk helyét a számítógépünkön. Amennyiben szeretnénk beállítani egy új helyet a munkakönyvtárunknak akkor azt megtehetjük a setwd() paranccsal (pl.: setwd(C:/ User /Documents) illetve a menü rendszeren keresztül is van rá lehetségünk Session -&gt; Set Working Directory -&gt; Choose Direcotry. Az R-ben praktikus úgynevezett projektalapú munkával dolgozni. Létrehozhatunk egy új projektet a menü rendszerben File -&gt; New Project itt meg kell határoznunk a projekt fájl helyét. A projekt alapú munka elnye, hogy a a munkakönyvtár mindig ugyanabban a mappában található, ahol az R projekt fájl is, amely megkönnyíti a saját adatokkal való munkát. Az egyes fájl formátumokat különböz parancsokkal tudjuk beolvasni. Egy txt esetében használhatjuk a read.txt() parancsot , ehhez a funkcióhoz nem kell csomagot betöltenünk, mivel az R alapverziója tartalmazza. Akárhányszor adatokat olvasunk be meg kell, hogy határozzuk az objektum nevét, amely tartalmazni fogja az adott fájl adatait (pl.: proba&lt;-read.txt (proba.txt)). A csv (comma separated values) formátumú fájlok esetében használhatjuk a read.csv parancsot, amelyet szintén tartalmaz az R alapverziója, illetve használhatjuk a read_csv parancsot is, amelyet a readr csomag tartalmaz (pl.: proba &lt;- read.csv(proba.csv)). Utóbbi használata ajánlott magyar nyelv szöveget tartalmazó adatok esetében, mivel tapasztalataink szerint ez a parancs kezeli legjobban a különböz kódolási problémákat. Amennyiben Excel táblázatokkal dolgozunk érdemes azokat csv formátumban elmenteni és azután betölteni, viszont van lehetségünk a tidyverse csomag read_excel() parancsának segítségével is Excel fájlokat betölteni. A read_excel parancs mködik, mind az xlsx és az xls formátumú fájlok esetében is. Mivel az excel fájlok több munkalappal is rendelkeznek ezért a read_excel használatokar azt is meghatározhatjuk, hogy melyik munkalapot szeretnénk betölteni, amennyiben nem határozzuk meg az els lapot használja alpértelmezett módon (pl.: proba &lt;- read_excel(proba.xlsx, Sheet = 2). 13.3 Vizualizáció library(ggplot2) library(gapminder) library(plotly) Az elemzéseinkhez használt data frame adatai alapján a ggplot2 csomag segítségével lehetségünk van különböz vizualizációk készítésére is. A ggplot2 használata során különböz témákat alkalmazhatunk, melyek részletes leírása megtalálható: https://ggplot2.tidyverse.org/reference/ggtheme.html Abban az esetben, ha nem választunk témát, a ggplot2 a következ ábrán is látható alaptémát használja. Ha például a szürke helyett fehér hátteret szeretnénk, alkalmazhatjuk a theme_minmal()parancsot. Szintén gyakran alkalmazott ábra alap a thema_bw(), ami az elztl az ábra keretezésében különbözik. Ha fehér alapon, de a beosztások vonalait feketén szeretnénk megjeleníteni, alkalmazhatjuk a theme_linedraw() függvényt, a theme_void() segítségével pedig egy fehér alapon, beosztásoktól mentes alapot kapunk, a theme_dark() pedig sötét hátteret eredményez. A theme_classic() segítségével az x és y tengelyt jeleníthetjük meg fehér alapon. Egy ábra készítésének alapja mindig a használni kívánt adatkészlet beolvasása, illetve az ábrázolni kívánt változót vagy változók megadása. Ezt követi a megfelel alakzat kiválasztása, attól függen például, hogy eloszlást, változást, adatok közötti kapcsolatot, vagy eltéréseket akarunk ábrázolni. A geom az a geometriai objektum, a mit a diagram az adatok megjelenítésére használ. Agglpot2 több mint 40 féle alakzat alkalmazására ad lehetséget, ezek közül néhány gyakoribbat mutatunk be az alábbiakban. Az alakzatokról részletes leírása található például az alábbi linken: https://r4ds.had.co.nz/data-visualisation.html A következkben a gapminder csomagban található adatok segítségével szemléltetjük az adatok vizualizálásának alapjait. Elször egyszer alapbeállítások mellett egy histogram típusú vizualizációt készítünk. ggplot( data = gapminder, mapping = aes(x = gdpPercap) ) + geom_histogram() Lehetségünk van arra, hogy az alakzat színét megváltoztassuk. A használható színek és színkódok megtalálhatóak a ggplot2 leírásában: https://ggplot2-book.org/scale-colour.html ggplot( data = gapminder, mapping = aes(x = gdpPercap) ) + geom_histogram(fill = &quot;yellow&quot;, colour = &quot;green&quot;) Meghatározhatjuk külön-külön a histogram x és y tengelyén ábrázolni kívánt adatokat és választhatjuk azok pontszer ábrázolását is. ggplot( data = gapminder, mapping = aes( x = gdpPercap, y = lifeExp ) ) + geom_point() Ahogy az elzekben, itt is megváltoztathatjuk az ábra színét. ggplot( data = gapminder, mapping = aes( x = gdpPercap, y = lifeExp ) ) + geom_point(colour = &quot;blue&quot;) Az fenti script kibvítésével az egyes kontinensek adatait különböz színnel ábrázolhatjuk, az x és y tengelyt elnevezhetjük, a histogramnak címet és alcímet adhatunk, illetve az adataink forrását is feltüntethetjük az alábbi módon: ggplot( data = gapminder, mapping = aes( x = gdpPercap, y = lifeExp, color = continent ) ) + geom_point() + labs( x = &quot;GDP per capita (log $)&quot;, y = &quot;Life expectancy&quot;, title = &quot;Connection between GDP and Life expectancy&quot;, subtitle = &quot;Points are country-years&quot;, caption = &quot;Source: Gapminder dataset&quot; ) Az ábrán található feliratok méretének, bettípusának és betszínének megválasztásra is lehetségünk van. ggplot( data = gapminder, mapping = aes( x = gdpPercap, y = lifeExp, color = continent ) ) + geom_point() + labs( x = &quot;GDP per capita (log $)&quot;, y = &quot;Life expectancy&quot;, title = &quot;Connection between GDP and Life expectancy&quot;, subtitle = &quot;Points are country-years&quot;, caption = &quot;Source: Gapminder dataset&quot; ) + theme(plot.title = element_text( size = 12, colour = &quot;red&quot; )) Készíthetünk oszlopdiagramot is, amit a ggplot2 diamonds adatkészletén személtetünk ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut)) Itt is lehetségünk van arra, hogy a diagram színét megváltoztassuk. ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut), fill = &quot;darkgreen&quot;) De arra is lehetségünk van, hogy az egyes oszlopok eltér színek legyenek. ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, fill = cut)) Arra is van lehetségünk, hogy egyszerre több változót is ábrázoljunk. ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, fill = clarity)) Arra ggplot2 segítségével arra is lehetségünk van, hogy csv-bl beolvasott adatainkat vizualizáljuk. plot_cap_1 &lt;- read.csv(&quot;data/plot_cap_1.csv&quot;, head = TRUE, sep = &quot;;&quot;) ggplot(plot_cap_1, aes(Year, fill = Subtopic)) + scale_x_discrete(limits = c(1957, 1958, 1959, 1960, 1961, 1962, 1963)) + geom_bar(position = &quot;dodge&quot;) + labs( x = NULL, y = NULL, title = &quot;A Magyar Közlönyben kihirdetett agrárpolitikai jogszabályok&quot;, subtitle = &quot;N=445&quot; ) + coord_flip() + # az ábra tipusa theme_minimal() + theme(plot.title = element_text(size = 12)) A csv-bl belolvasott adatainkból kördiagramot is készíthetünk pie &lt;- read.csv(&quot;data/pie.csv&quot;, head = TRUE, sep = &quot;;&quot;) ggplot(pie, aes(x = &quot;&quot;, y = value, fill = Type)) + geom_bar(stat = &quot;identity&quot;, width = 1) + coord_polar(&quot;y&quot;, start = 0) + scale_fill_brewer(palette = &quot;GnBu&quot;) + labs( title = &quot;A Magyar Közlönyben megjelent jogszabályok típusai&quot;, subtitle = &quot;N = 445&quot; ) + theme_void() Továbbá minden ábránkat, amelyet a ggplot segítségével létrehozunk lehetségünk van interaktívvá tenni a plotly csomag ggplotly parancsának segítségével. Ehhez egyszeren csak az ábrát egy objektumba kell, hogy létrehozzuk. ggplotabra &lt;- ggplot( data = gapminder, mapping = aes(x = gdpPercap) ) + geom_histogram() Majd ennek az obejktumnak a nevét helyezzük be a ggplotly parancsba, és futtassuk azt. ggplotly(ggplotabra) "],["irodalomjegyzék.html", "Irodalomjegyzék", " Irodalomjegyzék "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
