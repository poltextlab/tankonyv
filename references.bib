
@article{grimmer2013text,
	title = {Text as data: {The} promise and pitfalls of automatic content analysis methods for political texts},
	volume = {21},
	number = {3},
	journal = {Political analysis},
	author = {Grimmer, Justin and Stewart, Brandon M},
	year = {2013},
	note = {tex.ids= grimmerTextDataPromise2013
publisher: Cambridge University Press},
	pages = {267--297},
	file = {Grimmer and Stewart - 2013 - Text as Data The Promise and Pitfalls of Automati.pdf:C\:\\Users\\Akos\\Zotero\\storage\\L7J3KL2Z\\Grimmer and Stewart - 2013 - Text as Data The Promise and Pitfalls of Automati.pdf:application/pdf}
}

@article{wickham2014tidy,
	title = {Tidy data},
	volume = {59},
	number = {10},
	journal = {Journal of statistical software},
	author = {Wickham, Hadley},
	year = {2014},
	pages = {1--23}
}

@book{wickham2016r,
	title = {R for data science: import, tidy, transform, visualize, and model data},
	publisher = {" O'Reilly Media, Inc."},
	author = {Wickham, Hadley and Grolemund, Garrett},
	year = {2016}
}

@article{jacobiQuantitativeAnalysisLarge2016,
	title = {Quantitative analysis of large amounts of journalistic texts using topic modelling},
	volume = {4},
	issn = {2167-0811},
	number = {1},
	journal = {Digital Journalism},
	author = {Jacobi, Carina and Van Atteveldt, Wouter and Welbers, Kasper},
	year = {2016},
	note = {Publisher: Taylor \& Francis},
	pages = {89--106}
}

@article{bleiLatentDirichletAllocation2003,
	title = {Latent dirichlet allocation},
	volume = {3},
	number = {Jan},
	journal = {Journal of machine Learning research},
	author = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
	year = {2003},
	pages = {993--1022}
}

@incollection{burtejinCsoportositasKlaszterezes2016,
	address = {Budapest},
	title = {Csoportosítás ({Klaszterezés})},
	booktitle = {Kvantitatív szövegelemzés és szövegbányászat a politikatudományban},
	publisher = {L'Harmattan},
	author = {Burtejin, Zorgit},
	editor = {Sebők, Miklós},
	year = {2016},
	pages = {85--101}
}

@book{tanBevezetesAzAdatbanyaszatba2011,
	title = {Bevezetés az adatbányászatba},
	publisher = {Panem Kft.},
	author = {Tan, Pang-Ning and Steinbach, Michael and Kumar, Vipin},
	year = {2011}
}

@book{tikkSzovegbanyaszat2007,
	address = {Budapest},
	title = {Szövegbányászat},
	publisher = {Typotext},
	author = {Tikk, Domonkos},
	year = {2007}
}

@article{young2012affective,
	title = {Affective news: {The} automated coding of sentiment in political texts},
	volume = {29},
	number = {2},
	journal = {Political Communication},
	author = {Young, Lori and Soroka, Stuart},
	year = {2012},
	note = {Publisher: Taylor \& Francis},
	pages = {205--231}
}

@article{griffithsGibbsSamplingGenerative2002,
	title = {Gibbs sampling in the generative model of latent dirichlet allocation},
	author = {Griffiths, Tom},
	year = {2002},
	note = {tex.ids= griffithsGibbsSamplingGenerative2002a
publisher: Citeseer}
}

@inproceedings{phanLearningClassifyShort2008,
	title = {Learning to classify short and sparse text \& web with hidden topics from large-scale data collections},
	author = {Phan, Xuan-Hieu and Nguyen, Le-Minh and Horiguchi, Susumu},
	year = {2008},
	pages = {91--100}
}

@article{mateEffectCentralBank2021,
	title = {The effect of central bank communication on sovereign bond yields: {The} case of {Hungary}},
	volume = {16},
	issn = {1932-6203},
	shorttitle = {The effect of central bank communication on sovereign bond yields},
	url = {https://dx.plos.org/10.1371/journal.pone.0245515},
	doi = {10.1371/journal.pone.0245515},
	language = {en},
	number = {2},
	urldate = {2021-02-15},
	journal = {PLOS ONE},
	author = {Máté, Ákos and Sebők, Miklós and Barczikay, Tamás},
	editor = {Nath, Hiranya K.},
	month = feb,
	year = {2021},
	note = {tex.ids= mateEffectCentralBank2021a
ISBN: 1932-6203
publisher: Public Library of Science San Francisco, CA USA},
	pages = {e0245515}
}

@article{loughranWhenLiabilityNot2011,
	title = {When is a liability not a liability? {Textual} analysis, dictionaries, and 10-{Ks}},
	volume = {66},
	number = {1},
	journal = {The Journal of Finance},
	author = {Loughran, Tim and McDonald, Bill},
	year = {2011},
	note = {tex.publisher: Wiley Online Library},
	pages = {35--65}
}

@article{laver2000estimating,
	title = {Estimating policy positions from political texts},
	journal = {American Journal of Political Science},
	author = {Laver, Michael and Garry, John},
	year = {2000},
	note = {Publisher: JSTOR},
	pages = {619--634}
}

@book{silge2017text,
	title = {Text mining with {R}: {A} tidy approach},
	publisher = {" O'Reilly Media, Inc."},
	author = {Silge, Julia and Robinson, David},
	year = {2017}
}

@book{russelMestersegesIntelligencia2005,
	title = {Mesterséges {Intelligencia}},
	publisher = {Panem Kft.},
	author = {Russel, Stuart and Norvig, Peter},
	year = {2005}
}

@inproceedings{arun2010finding,
	title = {On finding the natural number of topics with latent dirichlet allocation: {Some} observations},
	booktitle = {Pacific-{Asia} conference on knowledge discovery and data mining},
	author = {Arun, Rajkumar and Suresh, Venkatasubramaniyan and Madhavan, CE Veni and Murthy, MN Narasimha},
	year = {2010},
	note = {tex.organization: Springer},
	pages = {391--402},
	file = {Arun et al. - 2010 - On finding the natural number of topics with laten.pdf:C\:\\Users\\Akos\\Zotero\\storage\\YNUETBVV\\Arun et al. - 2010 - On finding the natural number of topics with laten.pdf:application/pdf}
}

@article{cao2009density,
	title = {A density-based method for adaptive {LDA} model selection},
	volume = {72},
	number = {7-9},
	journal = {Neurocomputing},
	author = {Cao, Juan and Xia, Tian and Li, Jintao and Zhang, Yongdong and Tang, Sheng},
	year = {2009},
	note = {Publisher: Elsevier},
	pages = {1775--1781}
}

@article{deveaud2014accurate,
	title = {Accurate and effective latent concept modeling for ad hoc information retrieval},
	volume = {17},
	number = {1},
	journal = {Document numérique},
	author = {Deveaud, Romain and SanJuan, Eric and Bellot, Patrice},
	year = {2014},
	note = {Publisher: Lavoisier},
	pages = {61--84}
}

@article{griffithsFindingScientificTopics2004,
	title = {Finding scientific topics},
	volume = {101},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.0307752101},
	doi = {10.1073/pnas.0307752101},
	language = {en},
	number = {Supplement 1},
	urldate = {2021-02-23},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Griffiths, T. L. and Steyvers, M.},
	month = apr,
	year = {2004},
	pages = {5228--5235}
}

@article{roberts2014structural,
	title = {Structural topic models for open-ended survey responses},
	volume = {58},
	number = {4},
	journal = {American Journal of Political Science},
	author = {Roberts, Margaret E and Stewart, Brandon M and Tingley, Dustin and Lucas, Christopher and Leder-Luis, Jetson and Gadarian, Shana Kushner and Albertson, Bethany and Rand, David G},
	year = {2014},
	note = {Publisher: Wiley Online Library},
	pages = {1064--1082}
}

@article{mikolov2013efficient,
	title = {Efficient estimation of word representations in vector space},
	journal = {arXiv preprint arXiv:1301.3781},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	year = {2013}
}

@article{spirlingword,
	title = {Word embeddings},
	url = {https://polmeth.mit.edu/sites/default/files/documents/Pedro_Rodriguez.pdf},
	journal = {Journal of Politics},
	author = {Spirling, Arthur and Rodriguez, Pedro L},
	year = {2021}
}

@inproceedings{pennington2014glove,
	title = {Glove: {Global} vectors for word representation},
	booktitle = {Proceedings of the 2014 conference on empirical methods in natural language processing ({EMNLP})},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
	year = {2014},
	pages = {1532--1543}
}

@inproceedings{mikolov2018advances,
	title = {Advances in pre-training distributed word representations},
	booktitle = {Proceedings of the international conference on language resources and evaluation ({LREC} 2018)},
	author = {Mikolov, Tomas and Grave, Edouard and Bojanowski, Piotr and Puhrsch, Christian and Joulin, Armand},
	year = {2018}
}

@article{laver2003extracting,
	title = {Extracting policy positions from political texts using words as data},
	journal = {American political science review},
	author = {Laver, Michael and Benoit, Kenneth and Garry, John},
	year = {2003},
	note = {Publisher: JSTOR},
	pages = {311--331}
}

@article{kusnerWordEmbeddingsDocument2015,
	title = {From {Word} {Embeddings} {To} {Document} {Distances}},
	abstract = {We present the Word Mover’s Distance (WMD), a novel distance function between text documents. Our work is based on recent results in word embeddings that learn semantically meaningful representations for words from local cooccurrences in sentences. The WMD distance measures the dissimilarity between two text documents as the minimum amount of distance that the embedded words of one document need to “travel” to reach the embedded words of another document. We show that this distance metric can be cast as an instance of the Earth Mover’s Distance, a well studied transportation problem for which several highly efﬁcient solvers have been developed. Our metric has no hyperparameters and is straight-forward to implement. Further, we demonstrate on eight real world document classiﬁcation data sets, in comparison with seven stateof-the-art baselines, that the WMD metric leads to unprecedented low k-nearest neighbor document classiﬁcation error rates.},
	language = {en},
	journal = {Proceedings of the 32nd International Conference on Machine Learning},
	author = {Kusner, Matt J and Sun, Yu and Kolkin, Nicholas I and Weinberger, Kilian Q},
	year = {2015},
	file = {Kusner et al. - From Word Embeddings To Document Distances.pdf:C\:\\Users\\Akos\\Zotero\\storage\\GPUJ4AHK\\Kusner et al. - From Word Embeddings To Document Distances.pdf:application/pdf}
}

@article{barReflectiveViewText2011,
	title = {A {Reflective} {View} on {Text} {Similarity}},
	abstract = {While the concept of similarity is well grounded in psychology, text similarity is less well-deﬁned. Thus, we analyze text similarity with respect to its deﬁnition and the datasets used for evaluation. We formalize text similarity based on the geometric model of conceptual spaces along three dimensions inherent to texts: structure, style, and content. We empirically ground these dimensions in a set of annotation studies, and categorize applications according to these dimensions. Furthermore, we analyze the characteristics of the existing evaluation datasets, and use those datasets to assess the performance of common text similarity measures.},
	language = {en},
	journal = {Proceedings of Recent Advances in Natural Language Processing},
	author = {Bar, Daniel and Zesch, Torsten and Gurevych, Iryna},
	year = {2011},
	pages = {515--520},
	file = {Bar et al. - A Reflective View on Text Similarity.pdf:C\:\\Users\\Akos\\Zotero\\storage\\W7B8JIRI\\Bar et al. - A Reflective View on Text Similarity.pdf:application/pdf}
}

@article{laddUnderstandingUsingCommon2020,
	title = {Understanding and {Using} {Common} {Similarity} {Measures} for {Text} {Analysis}},
	volume = {9},
	url = {https://doi.org/10.46430/phen0089},
	journal = {The Programming Historian},
	author = {Ladd, John R.},
	year = {2020}
}

@article{wangMeasurementTextSimilarity2020,
	title = {Measurement of {Text} {Similarity}: {A} {Survey}},
	volume = {11},
	issn = {2078-2489},
	shorttitle = {Measurement of {Text} {Similarity}},
	url = {https://www.mdpi.com/2078-2489/11/9/421},
	doi = {10.3390/info11090421},
	abstract = {Text similarity measurement is the basis of natural language processing tasks, which play an important role in information retrieval, automatic question answering, machine translation, dialogue systems, and document matching. This paper systematically combs the research status of similarity measurement, analyzes the advantages and disadvantages of current methods, develops a more comprehensive classiﬁcation description system of text similarity measurement algorithms, and summarizes the future development direction. With the aim of providing reference for related research and application, the text similarity measurement method is described by two aspects: text distance and text representation. The text distance can be divided into length distance, distribution distance, and semantic distance; text representation is divided into string-based, corpus-based, single-semantic text, multi-semantic text, and graph-structure-based representation. Finally, the development of text similarity is also summarized in the discussion section.},
	language = {en},
	number = {9},
	urldate = {2021-03-20},
	journal = {Information},
	author = {Wang, Jiapeng and Dong, Yihong},
	month = aug,
	year = {2020},
	pages = {421},
	file = {Wang and Dong - 2020 - Measurement of Text Similarity A Survey.pdf:C\:\\Users\\Akos\\Zotero\\storage\\CE9VIPPK\\Wang and Dong - 2020 - Measurement of Text Similarity A Survey.pdf:application/pdf}
}

@article{siegTextSimilaritiesEstimate2018,
	title = {Text {Similarities} : {Estimate} the degree of similarity between two texts},
	url = {https://medium.com/@adriensieg/text-similarities-da019229c894},
	journal = {Medium},
	author = {Sieg, Adrien},
	year = {2018}
}

@article{hjorthComputersCodersVoters2015,
	title = {Computers, coders, and voters: {Comparing} automated methods for estimating party positions},
	volume = {2},
	issn = {2053-1680},
	number = {2},
	journal = {Research \& Politics},
	author = {Hjorth, Frederik and Klemmensen, Robert and Hobolt, Sara and Hansen, Martin Ejnar and Kurrild-Klitgaard, Peter},
	year = {2015},
	note = {Publisher: SAGE Publications Sage UK: London, England},
	pages = {2053168015580476}
}

@article{slapinScalingModelEstimating2008,
	title = {A scaling model for estimating time‐series party positions from texts},
	volume = {52},
	issn = {0092-5853},
	number = {3},
	journal = {American Journal of Political Science},
	author = {Slapin, Jonathan B and Proksch, Sven‐Oliver},
	year = {2008},
	note = {Publisher: Wiley Online Library},
	pages = {705--722}
}

@article{welbersTextAnalysis2017,
	title = {Text analysis in {R}},
	volume = {11},
	issn = {1931-2458},
	number = {4},
	journal = {Communication Methods and Measures},
	author = {Welbers, Kasper and Van Atteveldt, Wouter and Benoit, Kenneth},
	year = {2017},
	note = {Publisher: Taylor \& Francis},
	pages = {245--265}
}

@book{schutzeIntroductionInformationRetrieval2008,
	title = {Introduction to information retrieval},
	volume = {39},
	publisher = {Cambridge University Press Cambridge},
	author = {Schütze, Hinrich and Manning, Christopher D and Raghavan, Prabhakar},
	year = {2008}
}

@book{kwartlerTextMiningPractice2017,
	title = {Text mining in practice with {R}},
	isbn = {1-119-28201-2},
	publisher = {John Wiley \& Sons},
	author = {Kwartler, Ted},
	year = {2017}
}

@incollection{vinczeBeszedEsNyelvelemzo2019,
	address = {Szeged},
	title = {Beszéd és nyelvelemző szoftverek},
	booktitle = {Beszéd és nyelvelemző szoftverek a versenyképességért és az esélyegyenlőségért {HunCLARIN} korpuszok és nyelvtechnológiai eszközök a bölcsészet és társadalomtudományokban},
	author = {Vincze, Veronika},
	year = {2019},
	pages = {7--22}
}

@article{uvegesNamedEntityRecognition2019,
	title = {Named {Entity} {Recognition} in the {Miskolc} {Legal} {Corpus}},
	author = {Üveges, István},
	year = {2019},
	note = {ISBN: 963315393X
Publisher: Szegedi Tudományegyetem, Informatikai Intézet}
}

@inproceedings{szarvasMultilingualNamedEntity2006,
	title = {A multilingual named entity recognition system using boosting and c4. 5 decision tree learning algorithms},
	booktitle = {International {Conference} on {Discovery} {Science}},
	publisher = {Springer},
	author = {Szarvas, György and Farkas, Richárd and Kocsor, András},
	year = {2006},
	pages = {267--278}
}

@inproceedings{zsibritaMagyarlancToolMorphological2013,
	title = {magyarlanc: {A} tool for morphological and dependency parsing of hungarian},
	booktitle = {Proceedings of the {International} {Conference} {Recent} {Advances} in {Natural} {Language} {Processing} {RANLP} 2013},
	author = {Zsibrita, János and Vincze, Veronika and Farkas, Richárd},
	year = {2013},
	pages = {763--771}
}

@inproceedings{strakaTokenizingPosTagging2017,
	title = {Tokenizing, pos tagging, lemmatizing and parsing ud 2.0 with udpipe},
	booktitle = {Proceedings of the {CoNLL} 2017 {Shared} {Task}: {Multilingual} {Parsing} from {Raw} {Text} to {Universal} {Dependencies}},
	author = {Straka, Milan and Straková, Jana},
	year = {2017},
	pages = {88--99}
}

@article{bradyChallengeBigData2019,
	title = {The {Challenge} of {Big} {Data} and {Data} {Science}},
	volume = {22},
	issn = {1094-2939, 1545-1577},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-polisci-090216-023229},
	doi = {10.1146/annurev-polisci-090216-023229},
	abstract = {Big data and data science are transforming the world in ways that spawn new concerns for social scientists, such as the impacts of the internet on citizens and the media, the repercussions of smart cities, the possibilities of cyber-warfare and cyber-terrorism, the implications of precision medicine, and the consequences of artificial intelligence and automation. Along with these changes in society, powerful new data science methods support research using administrative, internet, textual, and sensor-audio-video data. Burgeoning data and innovative methods facilitate answering previously hard-to-tackle questions about society by offering new ways to form concepts from data, to do descriptive inference, to make causal inferences, and to generate predictions. They also pose challenges as social scientists must grasp the meaning of concepts and predictions generated by convoluted algorithms, weigh the relative value of prediction versus causal inference, and cope with ethical challenges as their methods, such as algorithms for mobilizing voters or determining bail, are adopted by policy makers.},
	language = {en},
	number = {1},
	urldate = {2021-04-15},
	journal = {Annual Review of Political Science},
	author = {Brady, Henry E.},
	year = {2019},
	pages = {297--323},
	file = {Brady - 2019 - The Challenge of Big Data and Data Science.pdf:C\:\\Users\\Akos\\Zotero\\storage\\7HFHFQ3A\\Brady - 2019 - The Challenge of Big Data and Data Science.pdf:application/pdf}
}

@inproceedings{baccianellaSentiwordnetEnhancedLexical2010,
	title = {Sentiwordnet 3.0: an enhanced lexical resource for sentiment analysis and opinion mining.},
	volume = {10},
	booktitle = {Lrec},
	author = {Baccianella, Stefano and Esuli, Andrea and Sebastiani, Fabrizio},
	year = {2010},
	note = {Issue: 2010},
	pages = {2200--2204}
}

@article{liuSentimentAnalysisSubjectivity2010,
	title = {Sentiment analysis and subjectivity.},
	volume = {2},
	number = {2010},
	journal = {Handbook of natural language processing},
	author = {Liu, Bing},
	year = {2010},
	pages = {627--666}
}

@book{sebokKvantitativSzovegelemzesEs2016,
	title = {Kvantitatív szövegelemzés és szövegbányászat a politikatudományban},
	isbn = {963-414-229-X},
	publisher = {L'Harmattan Kiadó},
	author = {Sebők, Miklós},
	year = {2016}
}

@incollection{balazsFogalmiAlapokEs2016,
	title = {Fogalmi alapok és a szózsák módszer},
	booktitle = {Kvantitatív szövegelemzés és szövegbányászat a politikatudományban},
	author = {Balázs, Ágnes},
	year = {2016},
	note = {Publisher: L’Harmattan Kiadó}
}

@article{sebokMulticlassClassificationNewspaper2021,
	title = {The {Multiclass} {Classification} of {Newspaper} {Articles} with {Machine} {Learning}: {The} {Hybrid} {Binary} {Snowball} {Approach}},
	volume = {29},
	issn = {1047-1987, 1476-4989},
	shorttitle = {The {Multiclass} {Classification} of {Newspaper} {Articles} with {Machine} {Learning}},
	url = {https://www.cambridge.org/core/product/identifier/S1047198720000273/type/journal_article},
	doi = {10.1017/pan.2020.27},
	abstract = {In this article, we present a machine learning-based solution for matching the performance of the gold standard of double-blind human coding when it comes to content analysis in comparative politics. We combine a quantitative text analysis approach with supervised learning and limited human resources in order to classify the front-page articles of a leading Hungarian daily newspaper based on their full text. Our goal was to assign items in our dataset to one of policy topics based on the codebook of the Comparative Agendas Project. The classification of the imbalanced classes of topics was handled by a hybrid binary snowball workflow. This relies on limited human resources as well as supervised learning; it simplifies the multiclass problem to one of binary choice; and it is based on a snowball approach as we augment the training set with machine-classified observations a er each successful round and also between corpora. Our results show that our approach provided better precision results (of over \% for most topic codes) than what is customary for human coders and most computer-assisted coding projects. Nevertheless, this high precision came at the expense of a relatively low, below \%, share of labeled articles.},
	language = {en},
	number = {2},
	urldate = {2021-04-19},
	journal = {Political Analysis},
	author = {Sebők, Miklós and Kacsuk, Zoltán},
	month = apr,
	year = {2021},
	pages = {236--249},
	file = {Sebők and Kacsuk - 2021 - The Multiclass Classification of Newspaper Article.pdf:C\:\\Users\\Akos\\Zotero\\storage\\JGKRBMHB\\Sebők and Kacsuk - 2021 - The Multiclass Classification of Newspaper Article.pdf:application/pdf}
}

@article{sebokViscosityRevisitedPower2021,
	title = {Viscosity {Revisited}: {The} {Power} of {Legislatures} in {New} and {Old} {Democracies} - {A} {Comparative} {Text} {Reuse} {Analysis}},
	author = {Sebők, Miklós and Sciarini, Pascal and Jaquet, Julien and Berki, Tamás and Bolonyai, Flóra},
	year = {2021}
}

@article{niwattanakulUsingJaccardCoefficient2013,
	title = {Using of {Jaccard} {Coefficient} for {Keywords} {Similarity}},
	abstract = {Presently, information retrieval can be accomplished simply and rapidly with the use of search engines. This allows users to specify the search criteria as well as specific keywords to obtain the required results. Additionally, an index of search engines has to be updated on most recent information as it is constantly changed over time. Particularly, information retrieval results as documents are typically too extensive, which affect on accessibility of the required results for searchers. Consequently, a similarity measurement between keywords and index terms is essentially performed to facilitate searchers in accessing the required results promptly. Thus, this paper proposed the similarity measurement method between words by deploying Jaccard Coefficient. Technically, we developed a measure of similarity Jaccard with Prolog programming language to compare similarity between sets of data. Furthermore, the performance of this proposed similarity measurement method was accomplished by employing precision, recall, and F-measure. Precisely, the test results demonstrated the awareness of advantage and disadvantages of the measurement which were adapted and applied to a search for meaning by using Jaccard similarity coefficient.},
	language = {en},
	journal = {Hong Kong},
	author = {Niwattanakul, Suphakit and Singthongchai, Jatsada and Naenudorn, Ekkachai and Wanapu, Supachanun},
	year = {2013},
	pages = {5},
	file = {Niwattanakul et al. - 2013 - Using of Jaccard Coefficient for Keywords Similari.pdf:C\:\\Users\\Akos\\Zotero\\storage\\9U7MQTFS\\Niwattanakul et al. - 2013 - Using of Jaccard Coefficient for Keywords Similari.pdf:application/pdf}
}

@article{benoit2018,
    title = {quanteda: An R package for the quantitative analysis of textual data},
    journal = {Journal of Open Source Software},
    author = {Kenneth Benoit and Kohei Watanabe and Haiyan Wang and Paul Nulty and Adam Obeng and Stefan Müller and Akitaka Matsuo},
    doi = {10.21105/joss.00774},
    url = {https://quanteda.io},
    volume = {3},
    number = {30},
    pages = {774},
    year = {2018},
  }

   @Manual{text2vecpackage,
    title = {text2vec: Modern Text Mining Framework for R},
    author = {Dmitriy Selivanov and Manuel Bickel and Qing Wang},
    year = {2020},
    note = {R package version 0.6},
    url = {https://CRAN.R-project.org/package=text2vec},
  }