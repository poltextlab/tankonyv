# Szövegskálázás {#scaling}

```{r include=FALSE}

source("_common.R")

```

## Fogalmi alapok

A politikatudomány egyik izgalmas kérdése a szereplők ideológiai skálákon való elhelyezése. Ezt jellemzően pártprogramok vagy különböző ügyekkel kapcsolatos álláspontpontok alapján szokták meghatározni, de a politikusok beszédei is alkalmasak arra, hogy meghatározzuk a beszélő ideológiai hovatartozását. A szövegbányászat területén jellemzően a wordfish és a wordscores módszert alkalmazzák erre a feladatra. Míg előbbi a felügyelet nélküli módszerek sorába tartozik, utóbbi felügyelt módszerek közé. A wordscores a szótári módszerekhez hasonlóan a szövegeket a bennük található szavak alapján helyezi el a politikai térben oly módon, hogy az ún. referenciadokumentumok szövegét használja tanító halmazként. A wordscores kiindulópontja, hogy pozíció pontszámokat kell rendelni referencia szövegekhez. A modell számításba veszi a szövegek szavainak súlyozott gyakoriságát és a pozíciópontszám, valamint a szógyakoriság alapján becsüli meg a korpuszban lévő többi dokumentum pozícióját [@laver2003extracting]. 


A felügyelet nélküli `wordfish` módszer a skálázás során nem a referencia dokumentumokra támaszkodik, hanem olyan kifejezéseket keres a szövegben, amelyek megkülönböztetik egymástól a politikai spektrum különböző pontjain elhelyezkedő beszélőket. Az IRT-n (*item response theory*) alapuló módszer azt feltételezi, hogy a politikusok egy kevés dimenziós politikai térben mozognak, amely tér leírható az *i* politikus $\theta_1$ paraméterével. Egy politikus (vagy párt) ebben a térben elfoglalt helyzete pedig befolyásolja a szavak szövegekben történő használatát. A módszer erőssége, hogy kevés erőforrás-befektetéssel megbízható becsléseket ad, ha a szövegek valóban az ideológiák mentén különböznek, tehát ha a szereplők erősen ideológiai tartalamú diskurzust folytatnak. Alkalmazásakor azonban tudnunk kell: a módszer nem képes kezelni, hogy a szövegek között nem csak ideológiai különbség lehet. Mivel a modell nem felügyelt, ezért nehéz garantálni, hogy valóban megbízhatóan azonosítja a szereplők elhelyezkedését a politikai térben, így az eredményeket mindenképpen körültekintően kell validálni [@slapinScalingModelEstimating2008;@hjorthComputersCodersVoters2015;@grimmer2013text].

```{r}
library(readr)
library(dplyr)
library(stringr)
library(ggplot2)
library(ggrepel)
library(quanteda)
library(quanteda.textmodels)
library(HunMineR)
```

A skálázási algoritmusokat egy kis korpuszon mutatjuk be. A minta dokumentumok a 2014–2018-as parlamenti ciklusban az Országgyűlésben frakcióvezető politikusok egy-egy véletlenszerűen kiválasztott napirend előtti felszólalásai. Ebben a ciklusban összesen 11 frakcióvezetője volt a két kormánypárti és öt ellenzéki frakciónak. [^skala-1] A dokumentumokon először elvégeztük a szokásos előkészítési lépéseket. 

[^skala-1]: a mintába nem került be Rogán Antal, akinek csak egy darab napirend előtti felszólalása volt.

```{r}
parl_beszedek <- HunMineR::data_parlspeakers_small

beszedek_tiszta <- parl_beszedek %>% 
    mutate(
    text = str_remove_all(string = text, pattern = "[:cntrl:]"),
    text = str_remove_all(string = text, pattern = "[:punct:]"),
    text = str_remove_all(string = text, pattern = "[:digit:]"),
    text = str_to_lower(text),
    text = str_trim(text),
    text = str_squish(text)
  )
```

A wordfish és wordscores algoritmus is ugyanazt a kiinduló corpus és dfm objektumot használja, amit a szokásos módon a `quanteda` csomag `corpus()` függvényével hozunk létre. 

```{r}
beszedek_corpus <- corpus(beszedek_tiszta)

summary(beszedek_corpus)
```

A leíró statisztikai táblázatban látszik, hogy a beszédek hosszúsága nem egységes, a leghosszabb `r max(summary(beszedek_corpus)$Tokens)`, a legrövidebb pedig `r min(summary(beszedek_corpus)$Tokens)` szavas. Az átlagos dokumentum hossz az `r round(mean(summary(beszedek_corpus)$Tokens))` szó. A korpusz szemléltető célú, alaposabb elemzéshez hosszabb és/vagy több dokumentummal érdemes dolgoznunk.

A korpusz létrehozása után elkészítjük a dfm mátrixot, amelyből eltávolítjuk a magyar stopszvakat a `quanteda` beépített szótára segítségével.

```{r}
beszedek_dfm <- beszedek_corpus %>% 
  tokens() %>% 
  tokens_remove(stopwords("hungarian")) %>% 
  dfm()
```

## Wordfish

A wordfish felügyelet nélküli skálázást a `quanteda.textmodels` csomagban implementált `textmodel_wordfish()` függvény fogja végezni. A megadott `dir = c(1, 2)` paraméterrel a két dokumentum relatív $\theta$ értékét tudjuk rögzíteni, mégpedig úgy hogy $\theta_{dir1} < \theta_{dir2}$. Alapbeállításként az algoritmus az első és az utolsó dokumentumot teszi be ide. A lenti példánál mi a pártpozíciók alapján a Jobbikos Vona Gábor és az LMP-s Schiffer András egy-egy beszédét használtuk. A `summary()` használható az illesztett modellel, és a dokumentumonkénti $\theta$ koefficienst tudjuk így megnézni.

```{r}
beszedek_wf <- quanteda.textmodels::textmodel_wordfish(beszedek_dfm, dir = c(2, 1))

summary(beszedek_wf)
```

Amennyiben szeretnénk a szavak szintjén is megnézni a $\beta$ (a szavakhoz társított súly, ami a relatív fontosságát mutatja) és $\psi$ (a szó rögzített hatást (*word fixed effects*), ami az eltérő szófrekvencia kezeléséért felelős) koefficienseket, akkor a `beszedek_wf` objektumban tárolt értékeket egy data frame-be tudjuk bemásolni. A dokumentumok hosszát és a szófrekvenciát figyelembe véve, a negatív $\beta$ értékű szavakat gyakrabban használják a negatív $\theta$ koefficienssel rendelkező politikusok.

```{r}
szavak_wf <- data.frame(
  word = beszedek_wf$features, 
  beta = beszedek_wf$beta, 
  psi = beszedek_wf$psi
  )

szavak_wf %>% 
  arrange(beta) %>% 
  head(n = 15)
```

Ez a pozitív értékekre is igaz.

```{r}
szavak_wf %>% 
  arrange(desc(beta)) %>% 
  head(n = 15)
```

Az eredményeinket mind a szavak, mind a dokumentumok szintjén tudjuk vizualizálni. Elsőként a klasszikus „Eiffel-torony” ábrát reprodukáljuk, ami a szavak gyakoriságának és a skálára gyakorolt befolyásának az illusztrálására szolgál. Ehhez a már elkészült `szavak_wf` data framet-et és a `ggplot2` csomagot fogjuk használni. Mivel a korpuszunk nagyon kicsi, ezért csak `r nrow(szavak_wf)` kifejezést fogunk ábrázolni. Ennek ellenére a lényeg kirajzolódik a lenti ábrán is.[^skala-2]

[^skala-2]: A `quanteda.textplots` csomag több megoldást is kínál az ábrák elkészítésére. Mivel ezek a megoldások kifejezetten a quanteda elemzések ábrázolására készültek, ezért rövid egysoros függvényekkel tudunk gyorsan ábrákat készíteni. A hátrányuk, hogy kevésbé tudjuk „személyre szabni” az ábráinkat, mint a `ggplot2` példák esetében. A `quanteda.textplots` megoldásokat ezen a linken demonstrálják a csomag készítői: [https://quanteda.io/articles/pkgdown/examples/plotting.html](https://quanteda.io/articles/pkgdown/examples/plotting.html).

Kihasználhatjuk, hogy a `ggplot` ábra definiálása közben a felhasznált bemeneti data frame-et különböző szempontok alapján lehet szűrni. Így ábrázolni tudjuk a gyakran használt, ám semleges szavakat (magas $\psi$, alacsony $\beta$), illetve a ritkább, de meghatározóbb szavakat (magas $\beta$, alacsony $\psi$).

```{r eiffel, fig.cap="A wordfish 'Eiffel-torony'"}
ggplot(szavak_wf, aes(x = beta, y = psi)) +
  geom_point(color = "grey") +
  geom_text_repel(
    data = filter(szavak_wf, beta > 4.5 | beta < -5 | psi > 0),
    aes(beta, psi, label = word),
    alpha = 0.7
    ) +
  labs(
    x = expression(beta),
    y = expression(psi)
    )  
```

A dokumentumok szintjén is érdemes megvizsgálni az eredményeket. Ehhez a dokumentum szintű paramétereket fogjuk egy data frame-be gyűjteni: a $\theta$ ideológiai pozíciót, illetve a beszélő nevét. A vizualizáció kedvéért a párttagságot is hozzáadjuk. A data frame összerakása után az alsó és a felső határát is kiszámoljuk a konfidencia intervallumnak és azt is ábrázoljuk (ld. \@ref(fig:eiffel). ábra).

```{r pozicio, fig.cap="A beszédek egymáshoz viszonyított pozíciója"}
dokumentumok_wf <- data.frame(
  speaker = beszedek_wf$x@docvars$felszolalo,
  part = beszedek_wf$x@docvars$part,
  theta = beszedek_wf$theta,
  theta_se = beszedek_wf$se.theta
) %>% 
  mutate(
    lower = theta - 1.96 * theta_se,
    upper = theta + 1.96 * theta_se
  )

ggplot(dokumentumok_wf, aes(theta, reorder(speaker, theta))) +
  geom_point() +
  geom_errorbarh(aes(xmin = lower, xmax = upper), height = 0) +
  labs(
    y = NULL,
    x = expression(theta)
  ) 
```

A párt metaadattal összehasonlíthatjuk az egy párthoz tartozó frakcióvezetők értékeit a `facet_wrap()` használatával. Figyeljünk arra, hogy az `y` tengelyen szabadon változhasson az egyes rész ábrák között, a `scales = "free"` opcióval  (ld. \@ref(fig:pozicio). ábra).

```{r partbanpoz, fig.cap="Párton belüli pozíciók"}
ggplot(dokumentumok_wf, aes(theta, reorder(speaker, theta))) +
  geom_point() +
  geom_errorbarh(aes(xmin = lower, xmax = upper), height = 0) +
  labs(
    y = NULL,
    x = expression(theta)
  ) +
  facet_wrap(~part, ncol = 1, scales = "free_y")
```

## Wordscores

A modell illesztést a wordfish-hez hasonlóan a `quanteda.textmodels` csomagban található `textmodel_wordscores()` függvény végzi. A kiinduló dfm ugyanaz, mint amit a fejezet elején elkészítettünk, a `beszedek_dfm`.

A referencia pontokat dokumentumváltozóként hozzáadjuk a dfm-hez (a `refrencia_pont` oszlopot, ami `NA` értéket kap alapértelmezetten). A kiválasztott referencia dokumentumoknál pedig egyenként hozzáadjuk az értékeket. Erre több megoldás is van, az egyszerűbb út, hogy az egyik és a másik végletet a `-1; 1` intervallummal jelöljük. Ennek a lehetséges alternatívája, hogy egy külső, már validált forrást használunk. Pártok esetén ilyen lehet a Chapel Hill szakértői kérdőívének a pontszámai, a Manifesto projekt által kódolt jobb-bal (*rile*) dimenzió. A lenti példánál mi maradunk az egyszerűbb bináris kódolásnál (ld. \@ref(fig:partbanpoz). ábra). A wordfish eredményt alapul véve a két referencia pont Gulyás Gergely és Szél Bernadett beszédei lesznek.[^skala-3] Ezek a 3. és a 10. dokumentumok.

[^skala-3]: Azért nem Vona Gábor beszédét választottuk, mert az gyaníthatóan egy kiugró érték, ami nem reprezentálja megfelelően a sokaságot.

```{r}
docvars(beszedek_dfm, "referencia_pont") <- NA
docvars(beszedek_dfm, "referencia_pont")[3] <- -1
docvars(beszedek_dfm, "referencia_pont")[10] <- 1

docvars(beszedek_dfm)
```

A lenti wordscores-modell specifikáció követi a @laver2003extracting tanulmányban leírtakat.

```{r}
beszedek_ws <- textmodel_wordscores(
  x = beszedek_dfm,
  y = docvars(beszedek_dfm, "referencia_pont"),
  scale = "linear",
  smooth = 0
  )

summary(beszedek_ws, 10)
```

Az illesztett wordscores modellünkkel ezek után már meg tudjuk becsülni a korpuszban lévő többi dokumentum pozícióját. Ehhez a `predict()` függvény megoldását használjuk. A kiegészítő opciókkal a konfidencia intervallum alsó és felső határát is meg tudjuk becsülni, ami jól jön akkor, ha szeretnénk ábrázolni az eredményt.

```{r}
beszedek_ws_pred <- predict(
  beszedek_ws, 
  newdata = beszedek_dfm,
  interval = "confidence")

beszedek_ws_pred <- as.data.frame(beszedek_ws_pred$fit)

beszedek_ws_pred
```

A kapott modellünket a wordfish-hez hasonlóan tudjuk ábrázolni, miután a `beszedek_ws_pred` objektumból adattáblát csinálunk és a `ggplot2`-vel elkészítjük a vizualizációt. A `dokumentumok_ws` két részből áll össze. Először a wordscores modell objektumunkból a frakcióvezetők neveit és pártjaikat emeljük ki (kicsit körülményes a dolog, mert egy komplexebb objektumban tárolja őket a `quanteda`, de az `str()` függvény tud segíteni ilyen esetekben). A dokumentumok becsült pontszámait pedig a `beszedek_ws_pred` objektumból készített data frame hozzácsatolásával adjuk hozzá a már elkészült data frame-hez. Ehhez a `dplyr` csomag `bind_cols` függvényét használjuk. Fontos, hogy itt teljesen biztosnak kell lennünk abban, hogy a sorok a két data frame esetében ugyanarra a dokumentumra vonatkoznak.

```{r}
dokumentumok_ws <- data.frame(
  speaker = beszedek_ws$x@docvars$felszolalo,
  part = beszedek_ws$x@docvars$part
)

dokumentumok_ws <- bind_cols(dokumentumok_ws, beszedek_ws_pred)

dokumentumok_ws
```

A 9.4-es ábrán a párton belüli bontást illusztráljuk a `facet_wrap()` segítségével.

```{r, fig.cap="A párton belüli wordscores-alapú skála"}
ggplot(dokumentumok_ws, aes(fit, reorder(speaker, fit))) +
  geom_point() +
  geom_errorbarh(aes(xmin = lwr, xmax = upr), height = 0) +
  labs(
    y = NULL,
    x = "wordscores"
  ) +
  facet_wrap(~part, ncol = 1, scales = "free_y")
```
